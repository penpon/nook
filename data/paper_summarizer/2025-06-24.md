# arXiv 論文要約 (2025-06-24)

## [OmniGen2: Exploration to Advanced Multimodal Generation](http://arxiv.org/abs/2506.18871v1)

**アブストラクト**:
本研究では、OmniGen2という多用途かつオープンソースの生成モデルを提案します。これは、テキストから画像への変換（text-to-image）、画像編集、インコンテキスト生成（in-context generation）など、多様な生成タスクに対応できる統一的なソリューションです。従来のOmniGen v1と異なり、OmniGen2はテキストと画像の各モダリティ（modalities）に対して、異なるデコーディング経路（decoding pathways）を採用し、パラメータを共有しない設計と、分離された画像トークナイザー（image tokenizer）を利用しています。この設計により、既存のマルチモーダル理解モデル（multimodal understanding models）を基盤としながらも、VAE（Variational Autoencoder）の入力を再適応させる必要がなく、元のテキスト生成能力を維持しつつ拡張が可能となっています。

また、OmniGen2の学習を支援するために、画像編集やインコンテキスト生成のデータを含む包括的なデータ構築パイプラインを開発しました。さらに、画像生成タスクに特化した反映（reflection）メカニズムを導入し、それに基づく反映データセット（reflection dataset）も作成しています。パラメータ数は比較的小規模ながらも、テキストから画像への生成や画像編集といった複数のタスクベンチマークで競争力のある結果を達成しています。

さらに、インコンテキスト生成、すなわちサブジェクト駆動型タスク（subject-driven tasks）の評価を目的として、新たなベンチマーク「OmniContext」を提案します。OmniGen2は、オープンソースモデルの中で一貫性（consistency）において最先端の性能を示しています。

今後の研究を支援するために、モデル、学習コード、データセット、データ構築パイプラインを公開予定です。

プロジェクトページ：https://vectorspacelab.github.io/OmniGen2  
GitHubリポジトリ：https://github.com/VectorSpaceLab/OmniGen2

**要約**:
1. 研究の目的と背景  
本研究は、多様なマルチモーダル生成タスク（テキストから画像への変換、画像編集、インコンテキスト生成など）に対応できる統一的な生成モデルを開発することを目的としています。従来のモデルは特定のタスクに特化していることが多く、柔軟性や拡張性に課題がありました。

2. 提案手法の概要  
提案するOmniGen2は、オープンソースの多用途生成モデルであり、テキストと画像の各モダリティに対して異なるデコーディング経路を採用し、パラメータを共有しない設計をしています。また、分離された画像トークナイザーを利用し、既存のマルチモーダル理解モデルを基盤としながらも、VAE（Variational Autoencoder）の入力再適応を必要とせず、元のテキスト生成能力を維持しつつ拡張可能です。さらに、画像編集やインコンテキスト生成のためのデータ構築パイプラインや、画像生成に特化した反映（reflection）メカニズムとそのデータセットも開発しています。

3. 主な結果と貢献  
OmniGen2は、比較的小規模なパラメータ数ながら、複数の生成タスクにおいて競争力のある性能を示し、特にテキストから画像生成や画像編集、インコンテキスト生成において優れた結果を達成しています。また、新たに提案したベンチマーク「OmniContext」により、サブジェクト駆動型のインコンテキスト生成タスクの評価も可能となっています。さらに、オープンソースとしてモデル、学習コード、データセット、パイプラインを公開予定です。

4. 将来の研究への示唆  
今後は、OmniGen2のさらなる性能向上や多様なマルチモーダルタスクへの適応、また新たなベンチマークの開発や応用範囲の拡大が期待されます。オープンソース化により、コミュニティによる研究や改良も促進され、マルチモーダル生成技術の発展に寄与することが見込まれます。

---

## [LongWriter-Zero: Mastering Ultra-Long Text Generation via Reinforcement Learning](http://arxiv.org/abs/2506.18841v1)

**アブストラクト**:
大規模言語モデル（Large Language Models, LLMs）による超長文生成は広く求められるシナリオですが、その実現は依然として大きな課題です。というのも、モデルの最大生成長制限や、シーケンス長の増加に伴う全体的な品質低下が障壁となっているからです。従来のアプローチの一例としてLongWriterがありますが、これらは一般に「教師あり学習（Supervised Fine-Tuning, SFT）」を用いて、合成された長文出力に対して微調整を行う方法に依存しています。しかし、この戦略は合成データに大きく依存しており、その作成は困難かつコストが高く、また一貫性や整合性に欠けることが多く、過度に人工的で構造的に単調になりがちです。

本研究では、全く新しいアプローチとして、アノテーション済みデータや合成データに頼らず、強化学習（Reinforcement Learning, RL）を活用して、超長かつ高品質な文章生成能力の出現を促す方法を提案します。具体的には、ゼロから始めて、報酬モデル（Reward Model）を用いたインセンティブ設計により、長さの制御や文章の質、構造的なフォーマットの改善を図ります。

実験評価の結果、Qwen2.5-32Bを基盤としたLongWriter-Zeroモデルは、従来のSFT手法を一貫して上回り、長文生成タスクにおいて最先端の性能を達成しました。具体的には、WritingBenchやArena-Writeといった評価指標で全てのメトリクスにおいて最高水準を記録し、DeepSeek R1やQwen3-235Bといった100Bパラメータ超のモデルをも凌駕しています。

なお、本研究で使用したデータやモデルのチェックポイントは、以下のURLにてオープンソースとして公開しています。
https://huggingface.co/THU-KEG/LongWriter-Zero-32B

**要約**:
1. 研究の目的と背景  
大規模言語モデル（LLMs）による超長文生成は、多くの応用シナリオで求められる一方、モデルの最大生成長制限やシーケンス長の増加に伴う品質低下が課題となっている。従来の手法は教師あり学習（Supervised Fine-Tuning, SFT）に依存しており、合成データの作成コストや一貫性の問題があるため、より効果的な超長文生成方法の開発が求められていた。

2. 提案手法の概要  
本研究では、アノテーション済みや合成データに頼らず、強化学習（Reinforcement Learning, RL）を用いた新たなアプローチを提案。報酬モデル（Reward Model）を活用し、長さや文章の質、構造的フォーマットの改善をインセンティブとして設計し、ゼロから超長文生成能力を向上させる。

3. 主な結果と貢献  
実験により、Qwen2.5-32Bを基盤としたLongWriter-Zeroモデルが、従来のSFT手法を上回り、長文生成の評価指標（WritingBenchやArena-Write）で最先端の性能を達成。さらに、DeepSeek R1やQwen3-235B（100Bパラメータ超）と比較しても優れた結果を示し、超長文生成の新たな基準を確立した。

4. 将来の研究への示唆  
本研究の成果は、強化学習を用いた超長文生成の有効性を示しており、今後はより多様なタスクやモデル規模への適用、報酬設計の最適化、生成品質のさらなる向上に向けた研究が期待される。また、公開されたモデルやデータを活用し、実用的な長文生成システムの開発が進む可能性がある。

---

## [Phantom-Data : Towards a General Subject-Consistent Video Generation Dataset](http://arxiv.org/abs/2506.18851v1)

**アブストラクト**:
Subject-to-video生成は近年大きな進展を遂げています。しかし、既存のモデルは依然としてテキスト指示に忠実に従うことにおいて重要な課題に直面しています。この制約は一般に「コピペ問題（copy-paste problem）」と呼ばれ、広く用いられているペア内（in-pair）トレーニングの枠組みに起因します。このアプローチは、ターゲット動画と同じシーンからリファレンス画像をサンプリングすることで、被写体のアイデンティティと背景や文脈的属性を本質的に絡めてしまうという問題を抱えています。

この課題に対処するために、我々は「Phantom-Data（ファントムデータ）」と名付けた、【汎用的なクロスペア（cross-pair）被写体対動画の一貫性（subject-to-video consistency）データセット】を提案します。本データセットは、多様なカテゴリーにわたる約100万のアイデンティティの一貫性を持つペアを含んでいます。

我々のデータセットは、以下の三段階のパイプラインによって構築されました。第一に、一般的かつ入力に整合した被写体検出モジュールを用いて対象を抽出します。第二に、5300万以上の動画と30億以上の画像から大規模なクロスコンテキスト（cross-context）被写体検索を行います。第三に、事前知識（prior）に基づくアイデンティティ検証を行い、文脈の変動下でも視覚的な一貫性を確保します。

包括的な実験の結果、Phantom-Dataを用いた学習は、プロンプト（prompt）の整合性や視覚的品質を大幅に向上させるとともに、アイデンティティの一貫性も従来のペア内（in-pair）ベースの手法と遜色ないレベルで維持できることを示しました。

**要約**:
1. 研究の目的と背景  
本研究は、Subject-to-video（被写体から動画生成）における重要な課題である「コピペ問題（copy-paste problem）」に対処し、テキスト指示に忠実かつ一貫性のある動画生成を実現することを目的としています。従来のモデルは、ペア内（in-pair）トレーニングに依存しており、被写体のアイデンティティと背景・文脈属性が混ざりやすいという問題がありました。  

2. 提案手法の概要  
この課題に対して、「Phantom-Data（ファントムデータ）」と呼ばれるクロスペア（cross-pair）被写体対動画の一貫性を重視した新しいデータセットを提案します。データセットは、約100万のアイデンティティを含む多様なカテゴリーのペアを収録しています。構築は三段階のパイプラインで行われ、まず対象の被写体を検出し、次に5300万以上の動画と30億以上の画像からクロスコンテキストの被写体検索を行い、最後に事前知識に基づくアイデンティティ検証を実施して、文脈変動下でも視覚的な一貫性を確保します。  

3. 主な結果と貢献  
実験の結果、Phantom-Dataを用いた学習により、プロンプトの整合性や視覚的品質が大幅に向上し、従来のペア内トレーニングに比べてアイデンティティの一貫性も保持できることを示しました。これにより、より信頼性の高い動画生成が可能となり、モデルの汎用性と品質向上に寄与しました。  

4. 将来の研究への示唆  
本研究は、クロスペアデータセットの有効性を示したものであり、今後はより多様なシーンや長時間の動画生成、リアルタイム性の向上など、実用性を高める方向への展開が期待されます。また、アイデンティティの検証や一貫性維持の手法の改良も重要な研究課題となるでしょう。

---

## [Vision as a Dialect: Unifying Visual Understanding and Generation via Text-Aligned Representations](http://arxiv.org/abs/2506.18898v1)

**アブストラクト**:
本論文は、視覚理解と生成を共通の離散的セマンティック表現（semantic representation）内で統合しようとするマルチモーダル（multimodal）フレームワークを提案します。その中心となるのは、「Text-Aligned Tokenizer（TA-Tok）」であり、これは大規模言語モデル（Large Language Model, LLM）の語彙（vocabulary）から投影されたテキストに整合したコードブック（codebook）を用いて、画像を離散トークンに変換します。視覚情報とテキスト情報を拡張された語彙を持つ統一空間に統合することで、我々のマルチモーダルLLM「Tar」は、モダリティ固有の設計を必要とせずに、クロスモーダル（cross-modal）の入力と出力を共有インターフェースを通じて実現します。

さらに、効率性と視覚的詳細のバランスを取るためのスケール適応型エンコーディング（encoding）とデコーディング（decoding）を提案し、高忠実度（high-fidelity）な視覚出力を生成するための生成型デトークナイザー（generative de-tokenizer）も導入しています。多様なデコーディングニーズに対応するために、二つの補完的なデトークナイザーを採用しています。一つは高速な自己回帰モデル（autoregressive model）、もう一つは拡散モデル（diffusion-based model）です。

モダリティ融合（modality fusion）を強化するために、先進的な事前学習タスク（pre-training tasks）を検討し、視覚理解と生成の両面で性能向上を実証しています。実験結果は、Tarが既存のマルチモーダルLLM手法と比較して、収束速度の向上や学習効率の向上を達成していることを示しています。コード、モデル、データは以下のURLで公開しています：https://tar.csuhan.com

**要約**:
1. 研究の目的と背景  
本研究は、視覚理解と生成を統一的に扱うマルチモーダル（multimodal）フレームワークの構築を目的としています。従来の手法では、視覚情報とテキスト情報を別々に処理する必要がありましたが、これらを共通の離散的セマンティック表現（semantic representation）内で統合することで、より効率的かつ柔軟なマルチモーダル処理を実現しようとしています。

2. 提案手法の概要  
提案手法の中心は、「Text-Aligned Tokenizer（TA-Tok）」と呼ばれる画像の離散トークン変換技術です。これは、大規模言語モデル（Large Language Model, LLM）の語彙から投影されたテキストに整合したコードブック（codebook）を用いて、画像を離散トークンに変換します。これにより、視覚情報とテキスト情報を拡張された語彙を持つ統一空間に融合させ、モダリティ固有の設計を必要とせずにクロスモーダル（cross-modal）の入力と出力を共有インターフェースで処理可能にしています。また、視覚出力の高忠実度を実現するために、スケール適応型エンコーディングとデコーディングを導入し、二つの補完的なデトークナイザー（自己回帰モデルと拡散モデル）を採用しています。さらに、先進的な事前学習タスクを用いてモダリティ融合を強化しています。

3. 主な結果と貢献  
実験により、提案手法「Tar」は従来のマルチモーダルLLMと比較して、収束速度や学習効率の向上を実証しました。画像理解と生成の両面で高い性能を達成し、クロスモーダルなタスクにおいても柔軟かつ高忠実な出力を実現しています。これにより、視覚とテキストの統合処理における新たなアプローチを提示し、マルチモーダルAIの発展に寄与しています。

4. 将来の研究への示唆  
今後は、提案手法のさらなる拡張や多様なマルチモーダルタスクへの適用、リアルタイム処理への最適化などが考えられます。また、より大規模なデータセットや多様なモダリティの統合を通じて、汎用性と性能の向上を目指すことが示唆されます。

---

## [ReasonFlux-PRM: Trajectory-Aware PRMs for Long Chain-of-Thought Reasoning in LLMs](http://arxiv.org/abs/2506.18896v1)

**アブストラクト**:
最近、Process Reward Models（PRMs）は、大規模言語モデル（LLMs）の中間推論過程を監督するための強力なフレームワークとして登場しています。従来のPRMsは主にモデルの最終出力レスポンスに基づいて訓練されており、特にDeepseek-R1のような最先端推論モデルによって生成される軌跡応答（trajectory-response）型の出力において、中間の思考過程（thinking trajectories）を堅牢に評価することが難しいという課題がありました。

本研究では、「ReasonFlux-PRM」と名付けた新たな軌跡（trajectory）に対応したPRMを提案します。これは、推論の軌跡（reasoning traces）を評価することに特化した設計であり、ステップごとの監督（step-level supervision）と軌跡全体の監督（trajectory-level supervision）を組み合わせることで、構造化されたチェーン・オブ・サトゥル（chain-of-thought）データに沿った細粒度の報酬割り当てを可能にします。

また、ReasonFlux-PRMは、オフライン（offline）とオンライン（online）の両方の設定に対応できるように適応しており、具体的には以下の用途に利用されます。
1. より高品質なモデル蒸留（distillation）データの選択を通じた、小型モデルの下流監督微調整（supervised fine-tuning）
2. 強化学習（Reinforcement Learning）中の方針最適化（policy optimization）において、密度の高い（dense）プロセスレベルの報酬を提供
3. 報酬誘導によるN最良（Best-of-N）テスト時のスケーリングを可能にする

実験では、AIME、MATH500、GPQA-Diamondといった難易度の高い下流ベンチマークにおいて、ReasonFlux-PRM-7Bが従来の強力なPRMs（例：Qwen2.5-Math-PRM-72B）や人間がキュレーションしたベースラインよりも高品質なデータを選択できることを示しました。さらに、提案モデルのReasonFlux-PRM-7Bは、一貫した性能向上を実現し、監督微調整で平均12.1％の改善、強化学習で4.5％の改善、テスト時スケーリングで6.3％の向上を達成しています。

また、リソース制約のある環境やエッジデバイス向けに、軽量版のReasonFlux-PRM-1.5Bも公開しています。

プロジェクトの詳細は以下のURLからご覧いただけます：https://github.com/Gen-Verse/ReasonFlux

**要約**:
1. 研究の目的と背景  
本研究は、大規模言語モデル（LLMs）の推論過程における中間思考過程（thinking trajectories）をより正確に評価・監督するためのフレームワークの開発を目的としています。従来のProcess Reward Models（PRMs）は最終出力に基づいて訓練されており、特に複雑な推論過程を伴う応答の評価に課題がありました。

2. 提案手法の概要  
「ReasonFlux-PRM」は、推論の軌跡（reasoning traces）に特化したPRMであり、ステップごとの監督と軌跡全体の監督を組み合わせることで、細粒度の報酬割り当てを実現します。これにより、チェーン・オブ・サトゥル（chain-of-thought）データに沿った評価が可能となり、オフライン・オンライン両方の設定に適応します。具体的には、モデル蒸留や方針最適化、N最良選択のスケーリングなどに利用されます。

3. 主な結果と貢献  
実験では、ReasonFlux-PRM-7Bが高難度のベンチマーク（AIME、MATH500、GPQA-Diamond）において、従来のPRMsや人間キュレーションのベースラインを上回る高品質なデータ選択を実現しました。監督微調整では平均12.1％の性能向上、強化学習では4.5％、テスト時のスケーリングでは6.3％の改善を達成しています。さらに、リソース制約環境向けに軽量版のReasonFlux-PRM-1.5Bも公開しています。

4. 将来の研究への示唆  
本手法は推論過程の評価精度向上に寄与しており、今後はより多様な推論タスクやモデル規模への適用、リアルタイム評価の拡張、また他の強化学習や微調整手法との連携など、多方面での発展が期待されます。

---

