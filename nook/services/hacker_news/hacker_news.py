"""Hacker Newsã®è¨˜äº‹ã‚’åé›†ã™ã‚‹ã‚µãƒ¼ãƒ“ã‚¹ã€‚"""

import asyncio
import json
import os
import re
from dataclasses import dataclass
from datetime import UTC, date, datetime
from typing import Any
from urllib.parse import urlparse

from bs4 import BeautifulSoup

from nook.common.base_service import BaseService
from nook.common.daily_snapshot import group_records_by_date, store_daily_snapshots
from nook.common.date_utils import (
    is_within_target_dates,
    normalize_datetime_to_local,
    target_dates_set,
)
from nook.common.decorators import handle_errors
from nook.common.dedup import DedupTracker
from nook.common.logging_utils import (
    log_article_counts,
    log_no_new_articles,
    log_processing_start,
    log_storage_complete,
    log_summarization_progress,
    log_summarization_start,
    log_summary_candidates,
)


@dataclass
class Story:
    """
    Hacker Newsè¨˜äº‹æƒ…å ±ã€‚

    Parameters
    ----------
    title : str
        ã‚¿ã‚¤ãƒˆãƒ«ã€‚
    score : int
        ã‚¹ã‚³ã‚¢ã€‚
    url : str | None
        URLã€‚
    text : str | None
        æœ¬æ–‡ã€‚
    """

    title: str
    score: int
    url: str | None = None
    text: str | None = None
    summary: str = ""
    created_at: datetime | None = None


# ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°æ¡ä»¶ã®å®šæ•°
SCORE_THRESHOLD = 20  # æœ€å°ã‚¹ã‚³ã‚¢
MIN_TEXT_LENGTH = 100  # æœ€å°ãƒ†ã‚­ã‚¹ãƒˆé•·
MAX_TEXT_LENGTH = 10000  # æœ€å¤§ãƒ†ã‚­ã‚¹ãƒˆé•·
FETCH_LIMIT: int | None = (
    None  # ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°å‰ã«å–å¾—ã™ã‚‹è¨˜äº‹æ•°ï¼ˆNoneã®å ´åˆã¯åˆ¶é™ãªã—ï¼‰
)
MAX_STORY_LIMIT = 15  # ä¿å­˜ã™ã‚‹è¨˜äº‹æ•°ã®ä¸Šé™


class HackerNewsRetriever(BaseService):
    """
    Hacker Newsã®è¨˜äº‹ã‚’åé›†ã™ã‚‹ã‚¯ãƒ©ã‚¹ã€‚

    Parameters
    ----------
    storage_dir : str, default="data"
        ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®ãƒ‘ã‚¹ã€‚
    """

    def __init__(self, storage_dir: str = "data"):
        """
        HackerNewsRetrieverã‚’åˆæœŸåŒ–ã—ã¾ã™ã€‚

        Parameters
        ----------
        storage_dir : str, default="data"
            ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®ãƒ‘ã‚¹ã€‚
        """
        super().__init__("hacker_news")
        self.base_url = "https://hacker-news.firebaseio.com/v0"
        self.http_client = None  # setup_http_clientã§åˆæœŸåŒ–
        self.blocked_domains = self._load_blocked_domains()

    async def collect(
        self,
        limit: int = MAX_STORY_LIMIT,
        *,
        target_dates: list[date] | None = None,
    ) -> list[tuple[str, str]]:
        """
        Hacker Newsã®è¨˜äº‹ã‚’åé›†ã—ã¦ä¿å­˜ã—ã¾ã™ã€‚

        Parameters
        ----------
        limit : int, default=15
            å–å¾—ã™ã‚‹è¨˜äº‹æ•°ã€‚
        target_dates : list[date] | None
            ä¿å­˜å¯¾è±¡ã¨ã™ã‚‹æ—¥ä»˜ã€‚None ã®å ´åˆã¯å½“æ—¥ã‚’å¯¾è±¡ã¨ã—ã¾ã™ã€‚

        Returns
        -------
        list[tuple[str, str]]
            ä¿å­˜ã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ã®ãƒªã‚¹ãƒˆ [(json_path, md_path), ...]
        """
        limit = min(limit, MAX_STORY_LIMIT)
        effective_target_dates = target_dates or target_dates_set(1)

        # å¯¾è±¡æ—¥ä»˜ã®ãƒ­ã‚°å‡ºåŠ› - æœ€å¤ã®æ—¥ä»˜ã‹ã‚‰å‡¦ç†ã™ã‚‹ã“ã¨ã‚’æ˜ç¤º
        date_str = min(effective_target_dates).strftime("%Y-%m-%d")
        log_processing_start(self.logger, date_str)

        # HTTPã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã®åˆæœŸåŒ–ã‚’ç¢ºèª
        if self.http_client is None:
            await self.setup_http_client()

        dedup_tracker = await self._load_existing_titles()

        stories = await self._get_top_stories(
            limit, dedup_tracker, effective_target_dates
        )
        saved_files = await self._store_summaries(stories, effective_target_dates)

        # å‡¦ç†å®Œäº†ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸
        if saved_files:
            self.logger.info(f"\nğŸ’¾ {len(saved_files)}æ—¥åˆ†ã®ãƒ‡ãƒ¼ã‚¿ã‚’ä¿å­˜å®Œäº†")
            for json_path, md_path in saved_files:
                log_storage_complete(self.logger, json_path, md_path)
        else:
            log_no_new_articles(self.logger)

        return saved_files

    # åŒæœŸç‰ˆã®äº’æ›æ€§ã®ãŸã‚ã®ãƒ©ãƒƒãƒ‘ãƒ¼
    def run(self, limit: int = MAX_STORY_LIMIT) -> None:
        """åŒæœŸçš„ã«å®Ÿè¡Œã™ã‚‹ãŸã‚ã®ãƒ©ãƒƒãƒ‘ãƒ¼"""
        asyncio.run(self.collect(limit))

    async def _load_existing_titles(self) -> DedupTracker:
        tracker = DedupTracker()
        today = datetime.now().strftime("%Y-%m-%d")
        filename_json = f"{today}.json"
        try:
            if await self.storage.exists(filename_json):
                data = await self.load_json(filename_json)
                if data:
                    for item in data:
                        title = item.get("title")
                        if title:
                            tracker.add(title)
            else:
                content = self.storage.load_markdown("hacker_news", datetime.now())
                if content:
                    for match in re.finditer(r"^## \[(.+?)\]", content, re.MULTILINE):
                        tracker.add(match.group(1))
                    for match in re.finditer(r"^## (?!\[)(.+)$", content, re.MULTILINE):
                        tracker.add(match.group(1).strip())
        except Exception as exc:
            self.logger.debug(f"æ—¢å­˜Hacker Newsã‚¿ã‚¤ãƒˆãƒ«ã®èª­ã¿è¾¼ã¿ã«å¤±æ•—ã—ã¾ã—ãŸ: {exc}")
        return tracker

    @handle_errors(retries=3)
    async def _get_top_stories(
        self,
        limit: int,
        dedup_tracker: DedupTracker,
        target_dates: list[date],
    ) -> list[Story]:
        """
        ãƒˆãƒƒãƒ—è¨˜äº‹ã‚’å–å¾—ã—ã¾ã™ã€‚

        Parameters
        ----------
        limit : int
            å–å¾—ã™ã‚‹è¨˜äº‹æ•°ã€‚

        Returns
        -------
        List[Story]
            å–å¾—ã—ãŸè¨˜äº‹ã®ãƒªã‚¹ãƒˆã€‚
        """
        # 1. topstoriesã‹ã‚‰å¤šã‚ã«è¨˜äº‹IDã‚’å–å¾—ï¼ˆ100ä»¶ï¼‰
        response = await self.http_client.get(f"{self.base_url}/topstories.json")
        story_ids = response.json()
        if FETCH_LIMIT is not None:
            story_ids = story_ids[:FETCH_LIMIT]

        # 2. ä¸¦è¡Œã—ã¦ã‚¹ãƒˆãƒ¼ãƒªãƒ¼ã‚’å–å¾—ï¼ˆæ—¢å­˜ã®å‡¦ç†ï¼‰
        tasks = []
        for story_id in story_ids:
            tasks.append(self._fetch_story(story_id))

        story_results = await asyncio.gather(*tasks, return_exceptions=True)

        # 3. æœ‰åŠ¹ãªã‚¹ãƒˆãƒ¼ãƒªãƒ¼ã‚’åé›†
        all_stories = []
        for result in story_results:
            if isinstance(result, Story):
                all_stories.append(result)
            elif isinstance(result, Exception):
                self.logger.error(f"Error fetching story: {result}")

        # 4. ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°å‡¦ç†ã‚’è¿½åŠ 
        filtered_stories = []
        for story in all_stories:
            if story.created_at:
                story_date = normalize_datetime_to_local(story.created_at).date()

                # ã‚¹ã‚³ã‚¢ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
                if story.score < SCORE_THRESHOLD:
                    continue

                # ãƒ†ã‚­ã‚¹ãƒˆé•·ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
                text_content = story.text or ""
                text_length = len(text_content)

                if text_length < MIN_TEXT_LENGTH or text_length > MAX_TEXT_LENGTH:
                    continue

                if not is_within_target_dates(story.created_at, target_dates):
                    continue

                filtered_stories.append(story)

        # 5. ã‚¹ã‚³ã‚¢ã§é™é †ã‚½ãƒ¼ãƒˆ
        filtered_stories.sort(key=lambda story: story.score, reverse=True)

        unique_stories: list[Story] = []
        for story in filtered_stories:
            is_dup, normalized = dedup_tracker.is_duplicate(story.title)
            if is_dup:
                original = dedup_tracker.get_original_title(normalized) or story.title
                self.logger.debug(
                    "é‡è¤‡è¨˜äº‹ã‚’ã‚¹ã‚­ãƒƒãƒ—: '%s' (åˆå‡º: '%s')",
                    story.title,
                    original,
                )
                continue

            dedup_tracker.add(story.title)
            unique_stories.append(story)

        # 6. å„æ—¥ç‹¬ç«‹ã§æœ€å¤§15ä»¶ãšã¤é¸æŠ
        # æ—¥ä»˜åˆ¥ã«è¨˜äº‹ã‚’ã‚°ãƒ«ãƒ¼ãƒ—åŒ–
        stories_by_date = {}
        for story in unique_stories:
            if story.created_at:
                story_date = normalize_datetime_to_local(story.created_at).date()
                if story_date not in stories_by_date:
                    stories_by_date[story_date] = []
                stories_by_date[story_date].append(story)

        # å„æ—¥ç‹¬ç«‹ã§ä¸Šä½15ä»¶ã‚’é¸æŠã—ã¦çµåˆ
        selected_stories = []
        for target_date in sorted(target_dates):
            if target_date in stories_by_date:
                date_stories = sorted(
                    stories_by_date[target_date], key=lambda s: s.score, reverse=True
                )
                selected_stories.extend(date_stories[:limit])

        # 7. ãƒ­ã‚°ã«çµ±è¨ˆæƒ…å ±ã‚’å‡ºåŠ›ï¼ˆqiitaå½¢å¼ã«åˆã‚ã›ã‚‹ï¼‰
        existing_count = 0  # æ—¢å­˜è¨˜äº‹æ•°ï¼ˆç°¡ç•¥åŒ–ï¼‰
        new_count = len(selected_stories)  # æ–°è¦è¨˜äº‹æ•°

        # è¨˜äº‹æƒ…å ±ã‚’è¡¨ç¤º
        log_article_counts(self.logger, existing_count, new_count)

        if selected_stories:
            log_summary_candidates(self.logger, selected_stories, "score")

        # 8. è¦ç´„ã‚’ä¸¦è¡Œã—ã¦ç”Ÿæˆ
        await self._summarize_stories(selected_stories)

        return selected_stories

    def _load_blocked_domains(self) -> dict[str, Any]:
        """ãƒ–ãƒ­ãƒƒã‚¯ãƒ‰ãƒ¡ã‚¤ãƒ³ãƒªã‚¹ãƒˆã‚’èª­ã¿è¾¼ã¿ã¾ã™ã€‚"""
        try:
            current_dir = os.path.dirname(os.path.abspath(__file__))
            blocked_domains_path = os.path.join(current_dir, "blocked_domains.json")

            with open(blocked_domains_path, encoding="utf-8") as f:
                blocked_data = json.load(f)

            self.logger.info(
                f"ãƒ–ãƒ­ãƒƒã‚¯ãƒ‰ãƒ¡ã‚¤ãƒ³ãƒªã‚¹ãƒˆã‚’èª­ã¿è¾¼ã¿ã¾ã—ãŸ: {len(blocked_data.get('blocked_domains', []))}ä»¶"
            )
            return blocked_data
        except Exception as e:
            self.logger.warning(f"ãƒ–ãƒ­ãƒƒã‚¯ãƒ‰ãƒ¡ã‚¤ãƒ³ãƒªã‚¹ãƒˆã®èª­ã¿è¾¼ã¿ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")
            return {"blocked_domains": [], "reasons": {}}

    def _is_blocked_domain(self, url: str) -> bool:
        """æŒ‡å®šã•ã‚ŒãŸURLãŒãƒ–ãƒ­ãƒƒã‚¯ã•ã‚ŒãŸãƒ‰ãƒ¡ã‚¤ãƒ³ã‹ã©ã†ã‹ã‚’ãƒã‚§ãƒƒã‚¯ã—ã¾ã™ã€‚"""
        if not url:
            return False

        try:
            parsed_url = urlparse(url)
            domain = parsed_url.netloc.lower()

            # www.ã‚’é™¤å»ã—ã¦æ¯”è¼ƒ
            if domain.startswith("www."):
                domain = domain[4:]

            blocked_domains = self.blocked_domains.get("blocked_domains", [])
            return domain in [d.lower() for d in blocked_domains]
        except Exception:
            return False

    async def _log_fetch_summary(self, stories: list[Story]) -> None:
        """ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ãƒ•ã‚§ãƒƒãƒã®è¦ç´„ãƒ­ã‚°ã‚’å‡ºåŠ›ã—ã¾ã™ã€‚"""
        success_count = 0
        blocked_count = 0
        error_count = 0

        for story in stories:
            if not story.text:
                error_count += 1
            elif "ãƒ–ãƒ­ãƒƒã‚¯ã•ã‚Œã¦ã„ã¾ã™" in story.text:
                blocked_count += 1
            elif (
                "ã‚¢ã‚¯ã‚»ã‚¹åˆ¶é™ã«ã‚ˆã‚Š" in story.text
                or "è¨˜äº‹ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ" in story.text
                or "è¨˜äº‹ã®å†…å®¹ã‚’å–å¾—ã§ãã¾ã›ã‚“ã§ã—ãŸ" in story.text
            ):
                error_count += 1
            else:
                success_count += 1

        self.logger.info(
            f"Content fetch summary: {success_count} succeeded, {blocked_count} blocked, {error_count} failed"
        )

    def _is_http1_required_domain(self, url: str) -> bool:
        """æŒ‡å®šã•ã‚ŒãŸURLãŒHTTP/1.1ã‚’å¿…è¦ã¨ã™ã‚‹ãƒ‰ãƒ¡ã‚¤ãƒ³ã‹ã©ã†ã‹ã‚’ãƒã‚§ãƒƒã‚¯ã—ã¾ã™ã€‚"""
        if not url:
            return False

        try:
            parsed_url = urlparse(url)
            domain = parsed_url.netloc.lower()

            # www.ã‚’é™¤å»ã—ã¦æ¯”è¼ƒ
            if domain.startswith("www."):
                domain = domain[4:]

            http1_required_domains = self.blocked_domains.get(
                "http1_required_domains", []
            )
            return domain in [d.lower() for d in http1_required_domains]
        except Exception:
            return False

    async def _fetch_story(self, story_id: int) -> Story | None:
        """å€‹åˆ¥ã®ã‚¹ãƒˆãƒ¼ãƒªãƒ¼ã‚’å–å¾—"""
        try:
            response = await self.http_client.get(
                f"{self.base_url}/item/{story_id}.json"
            )
            item = response.json()

            if "title" not in item:
                return None

            story = Story(
                title=item.get("title", ""),
                score=item.get("score", 0),
                url=item.get("url"),
                text=item.get("text"),
            )

            timestamp = item.get("time")
            if timestamp is not None:
                try:
                    story.created_at = datetime.fromtimestamp(
                        int(timestamp), tz=UTC
                    )
                except Exception:
                    story.created_at = None
            if story.created_at is None:
                story.created_at = datetime.now(UTC)

            # URLãŒã‚ã‚‹å ´åˆã¯è¨˜äº‹ã®å†…å®¹ã‚’å–å¾—
            if story.url and not story.text:
                await self._fetch_story_content(story)

            return story
        except Exception as e:
            self.logger.error(f"Error fetching story {story_id}: {e}")
            return None

    async def _fetch_story_content(self, story: Story) -> None:
        """è¨˜äº‹ã®å†…å®¹ã‚’å–å¾—"""
        # ãƒ–ãƒ­ãƒƒã‚¯ã•ã‚ŒãŸãƒ‰ãƒ¡ã‚¤ãƒ³ã‚’ãƒã‚§ãƒƒã‚¯
        if self._is_blocked_domain(story.url):
            domain = urlparse(story.url).netloc.lower()
            if domain.startswith("www."):
                domain = domain[4:]

            reason = self.blocked_domains.get("reasons", {}).get(domain, "ã‚¢ã‚¯ã‚»ã‚¹åˆ¶é™")
            story.text = f"ã“ã®ã‚µã‚¤ãƒˆï¼ˆ{domain}ï¼‰ã¯{reason}ã®ãŸã‚ãƒ–ãƒ­ãƒƒã‚¯ã•ã‚Œã¦ã„ã¾ã™ã€‚"
            self.logger.debug(
                f"ãƒ–ãƒ­ãƒƒã‚¯ã•ã‚ŒãŸãƒ‰ãƒ¡ã‚¤ãƒ³ã‚’ã‚¹ã‚­ãƒƒãƒ—: {story.url} - {reason}"
            )
            return

        # HTTP/1.1ãŒå¿…è¦ãªãƒ‰ãƒ¡ã‚¤ãƒ³ã‚’ãƒã‚§ãƒƒã‚¯
        force_http1 = self._is_http1_required_domain(story.url)
        if force_http1:
            self.logger.info(f"Using HTTP/1.1 for {story.url} (required domain)")

        try:
            response = await self.http_client.get(story.url, force_http1=force_http1)

            if response.status_code == 200:
                soup = BeautifulSoup(response.text, "html.parser")

                # ãƒ¡ã‚¿ãƒ‡ã‚£ã‚¹ã‚¯ãƒªãƒ—ã‚·ãƒ§ãƒ³ã‚’å–å¾—
                meta_desc = soup.find("meta", attrs={"name": "description"})
                if not meta_desc:
                    # Open Graphã®descriptionã‚‚è©¦ã™
                    meta_desc = soup.find("meta", attrs={"property": "og:description"})

                if meta_desc and meta_desc.get("content"):
                    story.text = meta_desc.get("content")
                else:
                    # æœ¬æ–‡ã®æœ€åˆã®æ®µè½ã‚’å–å¾—ï¼ˆã‚ˆã‚Šå¤šãã®æ®µè½ã‚’è©¦ã™ï¼‰
                    paragraphs = soup.find_all("p")
                    if paragraphs:
                        # æœ€åˆã®3ã¤ã®æ®µè½ã‚’çµåˆï¼ˆçŸ­ã™ãã‚‹æ®µè½ã¯é™¤å¤–ï¼‰
                        meaningful_paragraphs = [
                            p.get_text().strip()
                            for p in paragraphs[:5]
                            if len(p.get_text().strip()) > 50
                        ]
                        if meaningful_paragraphs:
                            story.text = " ".join(meaningful_paragraphs[:3])
                        else:
                            # æ„å‘³ã®ã‚ã‚‹æ®µè½ãŒãªã„å ´åˆã¯æœ€åˆã®æ®µè½ã‚’ä½¿ç”¨
                            story.text = paragraphs[0].get_text().strip()

                    # æœ¬æ–‡ãŒå–å¾—ã§ããªã„å ´åˆã¯ã€articleè¦ç´ ã‚’æ¢ã™
                    if not story.text:
                        article = soup.find("article")
                        if article:
                            story.text = article.get_text()[:500]
        except Exception as e:
            # HTTPã‚¨ãƒ©ãƒ¼ã«å¿œã˜ã¦ãƒ­ã‚°ãƒ¬ãƒ™ãƒ«ã‚’èª¿æ•´
            error_str = str(e)
            error_type = type(e).__name__

            if "401" in error_str or "403" in error_str or "Forbidden" in error_str:
                # 401/403ã‚¨ãƒ©ãƒ¼ã¯æƒ³å®šå†…ã®ãŸã‚ã€debugãƒ¬ãƒ™ãƒ«
                self.logger.debug(f"Access denied for {story.url}")
                story.text = "ã‚¢ã‚¯ã‚»ã‚¹åˆ¶é™ã«ã‚ˆã‚Šè¨˜äº‹ã®å†…å®¹ã‚’å–å¾—ã§ãã¾ã›ã‚“ã§ã—ãŸã€‚"
            elif "404" in error_str or "Not Found" in error_str:
                # 404ã‚¨ãƒ©ãƒ¼ã¯infoãƒ¬ãƒ™ãƒ«
                self.logger.info(f"Content not found for {story.url}")
                story.text = "è¨˜äº‹ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚"
            elif "SSL" in error_str or "handshake" in error_str.lower():
                # SSLã‚¨ãƒ©ãƒ¼ã¯warningãƒ¬ãƒ™ãƒ«
                self.logger.warning(f"SSL/TLS error for {story.url}")
                story.text = "è¨˜äº‹ã®å†…å®¹ã‚’å–å¾—ã§ãã¾ã›ã‚“ã§ã—ãŸã€‚"
            elif "timeout" in error_str.lower():
                # ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆã‚¨ãƒ©ãƒ¼ã¯warningãƒ¬ãƒ™ãƒ«
                self.logger.warning(f"Timeout error for {story.url}")
                story.text = "è¨˜äº‹ã®å†…å®¹ã‚’å–å¾—ã§ãã¾ã›ã‚“ã§ã—ãŸã€‚"
            else:
                # ãã®ä»–ã®äºˆæœŸã—ãªã„ã‚¨ãƒ©ãƒ¼ã¯warningãƒ¬ãƒ™ãƒ«ã§ç°¡æ½”ã«
                self.logger.warning(f"Error fetching {story.url}: {error_type}")
                story.text = "è¨˜äº‹ã®å†…å®¹ã‚’å–å¾—ã§ãã¾ã›ã‚“ã§ã—ãŸã€‚"

    async def _summarize_stories(self, stories: list[Story]) -> None:
        """è¤‡æ•°ã®ã‚¹ãƒˆãƒ¼ãƒªãƒ¼ã‚’é€æ¬¡è¦ç´„ï¼ˆãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ é€²æ—è¡¨ç¤ºï¼‰"""
        if not stories:
            return

        # è¦ç´„ç”Ÿæˆé–‹å§‹ã‚’è¡¨ç¤º
        log_summarization_start(self.logger)

        # é€æ¬¡è¦ç´„ã—ã¦ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ã§é€²æ—ã‚’è¡¨ç¤º
        for idx, story in enumerate(stories, 1):
            await self._summarize_story(story)
            log_summarization_progress(self.logger, idx, len(stories), story.title)

        # è¦ç´„å®Œäº†å¾Œã«ã‚¨ãƒ©ãƒ¼ãƒ‰ãƒ¡ã‚¤ãƒ³ã‚’ãƒ–ãƒ­ãƒƒã‚¯ãƒªã‚¹ãƒˆã«è¿½è¨˜
        await self._update_blocked_domains_from_errors(stories)

    async def _update_blocked_domains_from_errors(self, stories: list[Story]) -> None:
        """ã‚¨ãƒ©ãƒ¼ã«ãªã£ãŸãƒ‰ãƒ¡ã‚¤ãƒ³ã‚’ãƒ–ãƒ­ãƒƒã‚¯ãƒ‰ãƒ¡ã‚¤ãƒ³ãƒªã‚¹ãƒˆã«è¿½è¨˜ã—ã¾ã™ã€‚"""
        error_domains = {}

        for story in stories:
            if not story.url:
                continue

            # ã‚¨ãƒ©ãƒ¼çŠ¶æ…‹ã®ãƒã‚§ãƒƒã‚¯ - ã‚ˆã‚Šåºƒç¯„å›²ã«
            is_error = (
                not story.text
                or story.text == "è¨˜äº‹ã®å†…å®¹ã‚’å–å¾—ã§ãã¾ã›ã‚“ã§ã—ãŸã€‚"
                or "Function get failed after 3 attempts" in story.text
                or "RetryException" in story.text
                or "HTTP error" in story.text
                or "Request error" in story.text
                or "APIException" in story.text
            )

            if is_error:
                self.logger.debug(f"Error detected for domain: {story.url}")

                try:
                    parsed_url = urlparse(story.url)
                    domain = parsed_url.netloc.lower()

                    # www.ã‚’é™¤å»ã—ã¦æ­£è¦åŒ–
                    if domain.startswith("www."):
                        domain = domain[4:]

                    # æ—¢å­˜ã®ãƒ–ãƒ­ãƒƒã‚¯ãƒ‰ãƒ¡ã‚¤ãƒ³ã¯é™¤å¤–
                    if domain in [
                        d.lower()
                        for d in self.blocked_domains.get("blocked_domains", [])
                    ]:
                        continue

                    # ã‚¨ãƒ©ãƒ¼ç†ç”±ã‚’ç‰¹å®š
                    if "522" in story.text or "Server error" in story.text:
                        reason = "522 - Server error"
                    elif "429" in story.text:
                        reason = "429 - Too Many Requests"
                    elif "403" in story.text:
                        reason = "403 - Access denied"
                    elif "404" in story.text:
                        reason = "404 - Not found"
                    elif "timeout" in story.text.lower():
                        reason = "Timeout error"
                    elif "SSL" in story.text or "handshake" in story.text:
                        reason = "SSL/TLS error"
                    elif "Request error" in story.text:
                        reason = "Request error"
                    else:
                        reason = "Connection error"

                    error_domains[domain] = reason
                    self.logger.info(f"æ¤œå‡ºã•ã‚ŒãŸã‚¨ãƒ©ãƒ¼ãƒ‰ãƒ¡ã‚¤ãƒ³: {domain} ({reason})")

                except Exception as e:
                    self.logger.debug(f"Failed to parse domain from {story.url}: {e}")

        # ãƒ–ãƒ­ãƒƒã‚¯ãƒ‰ãƒ¡ã‚¤ãƒ³ãƒªã‚¹ãƒˆã‚’æ›´æ–°
        if error_domains:
            await self._add_to_blocked_domains(error_domains)

    async def _add_to_blocked_domains(self, new_domains: dict[str, str]) -> None:
        """æ–°ã—ã„ãƒ‰ãƒ¡ã‚¤ãƒ³ã‚’ãƒ–ãƒ­ãƒƒã‚¯ãƒ‰ãƒ¡ã‚¤ãƒ³ãƒªã‚¹ãƒˆã«è¿½åŠ ã—ã¾ã™ã€‚"""
        try:
            current_dir = os.path.dirname(os.path.abspath(__file__))
            blocked_domains_path = os.path.join(current_dir, "blocked_domains.json")

            # ç¾åœ¨ã®ãƒªã‚¹ãƒˆã‚’èª­ã¿è¾¼ã¿
            if os.path.exists(blocked_domains_path):
                with open(blocked_domains_path, encoding="utf-8") as f:
                    blocked_data = json.load(f)
            else:
                blocked_data = {"blocked_domains": [], "reasons": {}}

            # æ–°ã—ã„ãƒ‰ãƒ¡ã‚¤ãƒ³ã‚’è¿½åŠ 
            added_count = 0
            for domain, reason in new_domains.items():
                if domain not in [
                    d.lower() for d in blocked_data.get("blocked_domains", [])
                ]:
                    blocked_data.setdefault("blocked_domains", []).append(domain)
                    blocked_data.setdefault("reasons", {})[domain] = reason
                    added_count += 1
                    self.logger.info(f"ãƒ–ãƒ­ãƒƒã‚¯ãƒ‰ãƒ¡ã‚¤ãƒ³ã‚’è¿½åŠ : {domain} ({reason})")

            # ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜
            if added_count > 0:
                with open(blocked_domains_path, "w", encoding="utf-8") as f:
                    json.dump(blocked_data, f, indent=4, ensure_ascii=False)

                # ãƒ¡ãƒ¢ãƒªä¸Šã®ãƒªã‚¹ãƒˆã‚‚æ›´æ–°
                self.blocked_domains = blocked_data

                self.logger.info(f"ãƒ–ãƒ­ãƒƒã‚¯ãƒ‰ãƒ¡ã‚¤ãƒ³ãƒªã‚¹ãƒˆã‚’æ›´æ–°: {added_count}ä»¶è¿½åŠ ")

        except Exception as e:
            self.logger.error(f"ãƒ–ãƒ­ãƒƒã‚¯ãƒ‰ãƒ¡ã‚¤ãƒ³ãƒªã‚¹ãƒˆã®æ›´æ–°ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")

    async def _summarize_story(self, story: Story) -> None:
        """
        Hacker Newsè¨˜äº‹ã‚’è¦ç´„ã—ã¾ã™ã€‚

        Parameters
        ----------
        story : Story
            è¦ç´„ã™ã‚‹è¨˜äº‹ã€‚
        """
        if not story.text:
            story.summary = "æœ¬æ–‡æƒ…å ±ãŒãªã„ãŸã‚è¦ç´„ã§ãã¾ã›ã‚“ã€‚"
            return

        prompt = f"""
        ä»¥ä¸‹ã®Hacker Newsè¨˜äº‹ã‚’è¦ç´„ã—ã¦ãã ã•ã„ã€‚

        ã‚¿ã‚¤ãƒˆãƒ«: {story.title}
        æœ¬æ–‡: {story.text}
        ã‚¹ã‚³ã‚¢: {story.score}

        è¦ç´„ã¯ä»¥ä¸‹ã®å½¢å¼ã§è¡Œã„ã€æ—¥æœ¬èªã§å›ç­”ã—ã¦ãã ã•ã„:
        1. è¨˜äº‹ã®ä¸»ãªå†…å®¹ï¼ˆ1-2æ–‡ï¼‰
        2. é‡è¦ãªãƒã‚¤ãƒ³ãƒˆï¼ˆç®‡æ¡æ›¸ã3-5ç‚¹ï¼‰
        3. ã“ã®è¨˜äº‹ãŒæ³¨ç›®ã‚’é›†ã‚ãŸç†ç”±
        """

        system_instruction = """
        ã‚ãªãŸã¯Hacker Newsè¨˜äº‹ã®è¦ç´„ã‚’è¡Œã†ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚
        ä¸ãˆã‚‰ã‚ŒãŸè¨˜äº‹ã‚’åˆ†æã—ã€ç°¡æ½”ã§æƒ…å ±é‡ã®å¤šã„è¦ç´„ã‚’ä½œæˆã—ã¦ãã ã•ã„ã€‚
        æŠ€è¡“çš„ãªå†…å®¹ã¯æ­£ç¢ºã«ã€ä¸€èˆ¬çš„ãªå†…å®¹ã¯åˆ†ã‹ã‚Šã‚„ã™ãè¦ç´„ã—ã¦ãã ã•ã„ã€‚
        å›ç­”ã¯å¿…ãšæ—¥æœ¬èªã§è¡Œã£ã¦ãã ã•ã„ã€‚å°‚é–€ç”¨èªã¯é©åˆ‡ã«ç¿»è¨³ã—ã€å¿…è¦ã«å¿œã˜ã¦è‹±èªã®å°‚é–€ç”¨èªã‚’æ‹¬å¼§å†…ã«æ®‹ã—ã¦ãã ã•ã„ã€‚
        """

        try:
            summary = await self.gpt_client.generate_async(
                prompt=prompt,
                system_instruction=system_instruction,
                temperature=0.3,
                max_tokens=1000,
            )
            story.summary = summary
            await self.rate_limit()  # APIå‘¼ã³å‡ºã—å¾Œã®ãƒ¬ãƒ¼ãƒˆåˆ¶é™
        except Exception as e:
            story.summary = f"è¦ç´„ã®ç”Ÿæˆä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}"

    async def _store_summaries(
        self, stories: list[Story], target_dates: list[date]
    ) -> list[tuple[str, str]]:
        """è¨˜äº‹æƒ…å ±ã‚’æ—¥ä»˜åˆ¥ã«ä¿å­˜ã—ã¾ã™ã€‚"""
        if not stories:
            self.logger.info("ä¿å­˜ã™ã‚‹è¨˜äº‹ãŒã‚ã‚Šã¾ã›ã‚“")
            return []

        default_date = max(target_dates) if target_dates else datetime.now().date()
        records = self._serialize_stories(stories)
        records_by_date = group_records_by_date(records, default_date=default_date)

        saved_files = await store_daily_snapshots(
            records_by_date,
            load_existing=self._load_existing_stories,
            save_json=self.save_json,
            save_markdown=self.save_markdown,
            render_markdown=self._render_markdown,
            key=lambda item: item.get("title", ""),
            sort_key=self._story_sort_key,
            limit=MAX_STORY_LIMIT,
            logger=None,  # äºŒé‡è¡¨ç¤ºã‚’é˜²ããŸã‚Noneã‚’è¨­å®š
        )

        return saved_files

    def _serialize_stories(self, stories: list[Story]) -> list[dict[str, Any]]:
        records: list[dict[str, Any]] = []
        for story in stories:
            created = story.created_at or datetime.now(UTC)
            records.append(
                {
                    "title": story.title,
                    "score": story.score,
                    "url": story.url,
                    "text": story.text,
                    "summary": story.summary,
                    "published_at": created.isoformat(),
                }
            )
        return records

    async def _load_existing_stories(
        self, target_date: datetime
    ) -> list[dict[str, Any]]:
        date_str = target_date.strftime("%Y-%m-%d")
        filename_json = f"{date_str}.json"
        existing_json = await self.load_json(filename_json)
        if existing_json:
            return existing_json

        markdown_content = await self.storage.load(f"{date_str}.md")
        if not markdown_content:
            return []

        return self._parse_markdown(markdown_content)

    def _story_sort_key(self, item: dict[str, Any]) -> tuple[int, datetime]:
        score = int(item.get("score", 0) or 0)
        published_raw = item.get("published_at")
        if published_raw:
            try:
                published = datetime.fromisoformat(published_raw)
            except ValueError:
                published = datetime.min.replace(tzinfo=UTC)
        else:
            published = datetime.min.replace(tzinfo=UTC)
        return (score, published)

    def _render_markdown(self, records: list[dict[str, Any]], today: datetime) -> str:
        content = f"# Hacker News ãƒˆãƒƒãƒ—è¨˜äº‹ ({today.strftime('%Y-%m-%d')})\n\n"

        for record in records:
            title = record.get("title", "")
            url = record.get("url")
            title_link = f"[{title}]({url})" if url else title
            content += f"## {title_link}\n\n"
            content += f"ã‚¹ã‚³ã‚¢: {record.get('score', 0)}\n\n"

            summary = record.get("summary")
            text = record.get("text")
            if summary:
                content += f"**è¦ç´„**:\n{summary}\n\n"
            elif text:
                trimmed = text[:500]
                ellipsis = "..." if len(text) > 500 else ""
                content += f"{trimmed}{ellipsis}\n\n"

            content += "---\n\n"

        return content

    def _parse_markdown(self, content: str) -> list[dict[str, Any]]:
        records: list[dict[str, Any]] = []
        pattern = re.compile(
            r"##\s+(?:\[(?P<title>.+?)\]\((?P<url>[^\)]+)\)|(?P<title_only>.+?))\n\n"
            r"ã‚¹ã‚³ã‚¢:\s*(?P<score>\d+)\n\n"
            r"(?:(\*\*è¦ç´„\*\*:\n(?P<summary>.*?))|(?P<text>.+?))?---",
            re.DOTALL,
        )

        for match in pattern.finditer(content + "---"):
            title = match.group("title") or match.group("title_only") or ""
            url = match.group("url")
            score = int(match.group("score") or 0)
            summary = (match.group("summary") or "").strip()
            text = (match.group("text") or "").strip()

            records.append(
                {
                    "title": title.strip(),
                    "url": url.strip() if url else None,
                    "score": score,
                    "summary": summary,
                    "text": text,
                }
            )

        return records
