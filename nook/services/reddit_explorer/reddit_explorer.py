"""Redditã®äººæ°—æŠ•ç¨¿ã‚’åé›†ãƒ»è¦ç´„ã™ã‚‹ã‚µãƒ¼ãƒ“ã‚¹ã€‚"""

import asyncio
import os
import re
from dataclasses import dataclass, field
from datetime import date, datetime, timedelta, timezone
from pathlib import Path
from typing import Literal

import asyncpraw
import tomli

from nook.common.base_service import BaseService
from nook.common.daily_snapshot import group_records_by_date, store_daily_snapshots
from nook.common.date_utils import is_within_target_dates, target_dates_set
from nook.common.dedup import DedupTracker
from nook.common.logging_utils import (
    log_article_counts,
    log_no_new_articles,
    log_processing_start,
    log_storage_complete,
    log_summarization_progress,
    log_summarization_start,
    log_summary_candidates,
)


@dataclass
class RedditPost:
    """
    RedditæŠ•ç¨¿æƒ…å ±ã€‚

    Parameters
    ----------
    type : Literal["image", "gallery", "video", "poll", "crosspost", "text", "link"]
        æŠ•ç¨¿ã‚¿ã‚¤ãƒ—ã€‚
    id : str
        æŠ•ç¨¿IDã€‚
    title : str
        ã‚¿ã‚¤ãƒˆãƒ«ã€‚
    url : str | None
        URLã€‚
    upvotes : int
        ã‚¢ãƒƒãƒ—ãƒœãƒ¼ãƒˆæ•°ã€‚
    text : str
        æœ¬æ–‡ã€‚
    permalink : str
        æŠ•ç¨¿ã¸ã®ãƒ‘ãƒ¼ãƒãƒªãƒ³ã‚¯ã€‚
    thumbnail : str
        ã‚µãƒ ãƒã‚¤ãƒ«URLã€‚
    """

    type: Literal["image", "gallery", "video", "poll", "crosspost", "text", "link"]
    id: str
    title: str
    url: str | None
    upvotes: int
    text: str
    permalink: str = ""
    comments: list[dict[str, str | int]] = field(default_factory=list)
    summary: str = field(init=False)
    thumbnail: str = "self"
    popularity_score: float = field(default=0.0)
    created_at: datetime | None = None


class RedditExplorer(BaseService):
    """
    Redditã®äººæ°—æŠ•ç¨¿ã‚’åé›†ãƒ»è¦ç´„ã™ã‚‹ã‚¯ãƒ©ã‚¹ã€‚

    Parameters
    ----------
    client_id : str, optional
        Reddit APIã®ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆIDã€‚æŒ‡å®šã—ãªã„å ´åˆã¯ç’°å¢ƒå¤‰æ•°ã‹ã‚‰å–å¾—ã€‚
    client_secret : str, optional
        Reddit APIã®ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã‚·ãƒ¼ã‚¯ãƒ¬ãƒƒãƒˆã€‚æŒ‡å®šã—ãªã„å ´åˆã¯ç’°å¢ƒå¤‰æ•°ã‹ã‚‰å–å¾—ã€‚
    user_agent : str, optional
        Reddit APIã®ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã€‚æŒ‡å®šã—ãªã„å ´åˆã¯ç’°å¢ƒå¤‰æ•°ã‹ã‚‰å–å¾—ã€‚
    storage_dir : str, default="data"
        ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®ãƒ‘ã‚¹ã€‚
    """

    def __init__(
        self,
        client_id: str | None = None,
        client_secret: str | None = None,
        user_agent: str | None = None,
        storage_dir: str = "data",
    ):
        """
        RedditExplorerã‚’åˆæœŸåŒ–ã—ã¾ã™ã€‚

        Parameters
        ----------
        client_id : str, optional
            Reddit APIã®ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆIDã€‚æŒ‡å®šã—ãªã„å ´åˆã¯ç’°å¢ƒå¤‰æ•°ã‹ã‚‰å–å¾—ã€‚
        client_secret : str, optional
            Reddit APIã®ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã‚·ãƒ¼ã‚¯ãƒ¬ãƒƒãƒˆã€‚æŒ‡å®šã—ãªã„å ´åˆã¯ç’°å¢ƒå¤‰æ•°ã‹ã‚‰å–å¾—ã€‚
        user_agent : str, optional
            Reddit APIã®ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã€‚æŒ‡å®šã—ãªã„å ´åˆã¯ç’°å¢ƒå¤‰æ•°ã‹ã‚‰å–å¾—ã€‚
        storage_dir : str, default="data"
            ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®ãƒ‘ã‚¹ã€‚
        """
        super().__init__("reddit_explorer")

        self.client_id = client_id or os.environ.get("REDDIT_CLIENT_ID")
        self.client_secret = client_secret or os.environ.get("REDDIT_CLIENT_SECRET")
        self.user_agent = user_agent or os.environ.get("REDDIT_USER_AGENT")

        if not all([self.client_id, self.client_secret, self.user_agent]):
            raise ValueError(
                "Reddit API credentials must be provided or set as environment variables"
            )

        # asyncprawã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã¯ä½¿ç”¨æ™‚ã«ä½œæˆ
        self.reddit = None

        self.SUMMARY_LIMIT = 15

        self.http_client = None  # setup_http_clientã§åˆæœŸåŒ–

        # ã‚µãƒ–ãƒ¬ãƒ‡ã‚£ãƒƒãƒˆã®è¨­å®šã‚’èª­ã¿è¾¼ã‚€
        script_dir = Path(__file__).parent
        with open(script_dir / "subreddits.toml", "rb") as f:
            self.subreddits_config = tomli.load(f)

    def run(self, limit: int | None = None) -> None:
        """
        Redditã®äººæ°—æŠ•ç¨¿ã‚’åé›†ãƒ»è¦ç´„ã—ã¦ä¿å­˜ã—ã¾ã™ã€‚

        Parameters
        ----------
        limit : Optional[int], default=None
            å„ã‚µãƒ–ãƒ¬ãƒ‡ã‚£ãƒƒãƒˆã‹ã‚‰å–å¾—ã™ã‚‹æŠ•ç¨¿æ•°ã€‚Noneã®å ´åˆã¯åˆ¶é™ãªã—ã€‚
        """
        asyncio.run(self.collect(limit))

    async def collect(
        self,
        limit: int | None = None,
        *,
        target_dates: list[date] | None = None,
    ) -> list[tuple[str, str]]:
        """
        Redditã®äººæ°—æŠ•ç¨¿ã‚’åé›†ãƒ»è¦ç´„ã—ã¦ä¿å­˜ã—ã¾ã™ï¼ˆéåŒæœŸç‰ˆï¼‰ã€‚

        Parameters
        ----------
        limit : Optional[int], default=None
            å„ã‚µãƒ–ãƒ¬ãƒ‡ã‚£ãƒƒãƒˆã‹ã‚‰å–å¾—ã™ã‚‹æŠ•ç¨¿æ•°ã€‚Noneã®å ´åˆã¯åˆ¶é™ãªã—ã€‚

        Returns
        -------
        list[tuple[str, str]]
            ä¿å­˜ã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ã®ãƒªã‚¹ãƒˆ [(json_path, md_path), ...]
        """
        effective_target_dates = target_dates or target_dates_set(1)

        # HTTPã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã®åˆæœŸåŒ–ã‚’ç¢ºèª
        if self.http_client is None:
            await self.setup_http_client()

        candidate_posts: list[tuple[str, str, RedditPost]] = []
        dedup_tracker = await self._load_existing_titles()

        # ãƒ‡ãƒãƒƒã‚°ï¼šé‡è¤‡ãƒˆãƒ©ãƒƒã‚«ãƒ¼ã®çŠ¶æ…‹ã‚’è¡¨ç¤º
        self.logger.info(f"ğŸ” æ—¢å­˜ã‚¿ã‚¤ãƒˆãƒ«æ•°: {dedup_tracker.count()}ä»¶")

        self.logger.info("\nğŸ“¡ ã‚µãƒ–ãƒ¬ãƒ‡ã‚£ãƒƒãƒˆå–å¾—ä¸­...")

        # Redditã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã‚’ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆãƒãƒãƒ¼ã‚¸ãƒ£ãƒ¼ã§ä½¿ç”¨
        async with asyncpraw.Reddit(
            client_id=self.client_id,
            client_secret=self.client_secret,
            user_agent=self.user_agent,
        ) as reddit:
            self.reddit = reddit

            try:
                # å„ã‚«ãƒ†ã‚´ãƒªã®ã‚µãƒ–ãƒ¬ãƒ‡ã‚£ãƒƒãƒˆã‹ã‚‰æŠ•ç¨¿ã‚’å–å¾—
                for category, subreddits in self.subreddits_config.items():
                    for subreddit_name in subreddits:
                        try:
                            posts, total_found = await self._retrieve_hot_posts(
                                subreddit_name,
                                limit,
                                dedup_tracker,
                                effective_target_dates,
                            )

                            # æœ¬æ¥ã®ä»¶æ•°ã¨å®Ÿéš›ã®å–å¾—ä»¶æ•°ã‚’è¡¨ç¤º
                            if total_found > 0:
                                self.logger.info(
                                    f"   â€¢ r/{subreddit_name}: {len(posts)}ä»¶å–å¾— (æœ¬æ¥{total_found}ä»¶)"
                                )
                            else:
                                self.logger.info(f"   â€¢ r/{subreddit_name}: 0ä»¶å–å¾—")

                            for post in posts:
                                candidate_posts.append((category, subreddit_name, post))

                        except Exception as e:
                            self.logger.error(
                                f"ã‚µãƒ–ãƒ¬ãƒ‡ã‚£ãƒƒãƒˆ r/{subreddit_name} ã®å‡¦ç†ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}"
                            )

                self.logger.info(f"åˆè¨ˆ {len(candidate_posts)} ä»¶ã®æŠ•ç¨¿å€™è£œã‚’å–å¾—ã—ã¾ã—ãŸ")

                # æ—¥ä»˜ã”ã¨ã«ã‚°ãƒ«ãƒ¼ãƒ—åŒ–ã—ã¦å„æ—¥ç‹¬ç«‹ã§å‡¦ç†
                posts_by_date = {}
                for category, subreddit_name, post in candidate_posts:
                    if post.created_at:
                        # JSTã‚¿ã‚¤ãƒ ã‚¾ãƒ¼ãƒ³ã«å¤‰æ›ã—ã¦æ—¥ä»˜ã‚’å–å¾—
                        jst_time = post.created_at.astimezone(timezone(timedelta(hours=9)))
                        post_date = jst_time.date()
                        if post_date not in posts_by_date:
                            posts_by_date[post_date] = []
                        posts_by_date[post_date].append((category, subreddit_name, post))

                # å„æ—¥ç‹¬ç«‹ã§å‡¦ç†
                saved_files: list[tuple[str, str]] = []
                for target_date in sorted(effective_target_dates):
                    date_str = target_date.strftime("%Y-%m-%d")
                    date_posts = posts_by_date.get(target_date, [])

                    # æ—¥ä»˜æƒ…å ±ã‚’å…ˆé ­ã«è¡¨ç¤º
                    log_processing_start(self.logger, date_str)

                    if not date_posts:
                        log_no_new_articles(self.logger)
                        continue

                    # æ—¢å­˜è¨˜äº‹æ•°ã‚’å–å¾—
                    try:
                        existing_posts = await self._load_existing_posts(target_date)
                        existing_count = len(existing_posts)
                    except Exception:
                        existing_count = 0
                    new_count = len(date_posts)
                    log_article_counts(self.logger, existing_count, new_count)

                    # ä¸Šä½15ä»¶ã‚’é¸æŠ
                    if len(date_posts) <= self.SUMMARY_LIMIT:
                        selected_posts = date_posts
                    else:

                        def sort_key(item: tuple[str, str, RedditPost]):
                            _, _, post = item
                            created = post.created_at or datetime.min
                            return (post.popularity_score, created)

                        sorted_posts = sorted(date_posts, key=sort_key, reverse=True)
                        selected_posts = sorted_posts[: self.SUMMARY_LIMIT]

                    # è¦ç´„å¯¾è±¡ã‚’å‡ºåŠ›
                    post_candidates = [post for _, _, post in selected_posts]
                    log_summary_candidates(self.logger, post_candidates)

                    # è¦ç´„ç”Ÿæˆ
                    log_summarization_start(self.logger)
                    for idx, (category, subreddit_name, post) in enumerate(selected_posts, 1):
                        post.comments = await self._retrieve_top_comments_of_post(post, limit=5)
                        await self._summarize_reddit_post(post)
                        log_summarization_progress(
                            self.logger, idx, len(selected_posts), post.title
                        )

                    # ä¿å­˜
                    day_saved_files = await self._store_summaries(selected_posts, [target_date])
                    for json_path, md_path in day_saved_files:
                        log_storage_complete(self.logger, json_path, md_path)
                        saved_files.append((json_path, md_path))

                return saved_files

            finally:
                # ã‚°ãƒ­ãƒ¼ãƒãƒ«HTTPã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆãªã®ã§ã‚¯ãƒ­ãƒ¼ã‚ºä¸è¦
                pass
                # asyncprawã®ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆãƒãƒãƒ¼ã‚¸ãƒ£ãƒ¼ãŒè‡ªå‹•çš„ã«ã‚¯ãƒ­ãƒ¼ã‚ºã™ã‚‹

    async def _retrieve_hot_posts(
        self,
        subreddit_name: str,
        limit: int | None,
        dedup_tracker: DedupTracker,
        target_dates: list[date],
    ) -> tuple[list[RedditPost], int]:
        """
        ã‚µãƒ–ãƒ¬ãƒ‡ã‚£ãƒƒãƒˆã®äººæ°—æŠ•ç¨¿ã‚’å–å¾—ã—ã¾ã™ã€‚

        Parameters
        ----------
        subreddit_name : str
            ã‚µãƒ–ãƒ¬ãƒ‡ã‚£ãƒƒãƒˆåã€‚
        limit : Optional[int]
            å–å¾—ã™ã‚‹æŠ•ç¨¿æ•°ã€‚Noneã®å ´åˆã¯åˆ¶é™ãªã—ã€‚
        dedup_tracker : DedupTracker
            ã‚¿ã‚¤ãƒˆãƒ«é‡è¤‡ã‚’è¿½è·¡ã™ã‚‹ãƒˆãƒ©ãƒƒã‚«ãƒ¼ã€‚
        target_dates : list[date]
            ä¿å­˜å¯¾è±¡ã¨ã™ã‚‹æ—¥ä»˜é›†åˆã€‚

        Returns
        -------
        tuple[List[RedditPost], int]
            å–å¾—ã—ãŸæŠ•ç¨¿ã®ãƒªã‚¹ãƒˆã¨ã€æœ¬æ¥å–å¾—ã§ããŸä»¶æ•°ã®ã‚¿ãƒ—ãƒ«ã€‚
        """
        subreddit = await self.reddit.subreddit(subreddit_name)
        posts = []
        total_found = 0

        async for submission in subreddit.hot(limit=limit):
            if submission.stickied:
                continue

            total_found += 1

            # æŠ•ç¨¿ã‚¿ã‚¤ãƒ—ã‚’åˆ¤å®š
            post_type = "text"
            if hasattr(submission, "is_video") and submission.is_video:
                post_type = "video"
            elif hasattr(submission, "is_gallery") and submission.is_gallery:
                post_type = "gallery"
            elif hasattr(submission, "poll_data") and submission.poll_data:
                post_type = "poll"
            elif hasattr(submission, "crosspost_parent") and submission.crosspost_parent:
                post_type = "crosspost"
            elif submission.is_self:
                post_type = "text"
            elif any(submission.url.endswith(ext) for ext in [".jpg", ".jpeg", ".png", ".gif"]):
                post_type = "image"
            else:
                post_type = "link"

            # ã‚¿ã‚¤ãƒˆãƒ«ã¨æœ¬æ–‡ã‚’æ—¥æœ¬èªã«ç¿»è¨³
            title = submission.title

            is_dup, normalized = dedup_tracker.is_duplicate(title)
            if is_dup:
                original = dedup_tracker.get_original_title(normalized) or title
                self.logger.info(
                    "é‡è¤‡RedditæŠ•ç¨¿ã‚’ã‚¹ã‚­ãƒƒãƒ—: '%s' (åˆå‡º: '%s')",
                    title,
                    original,
                )
                continue
            text_ja = (
                await self._translate_to_japanese(submission.selftext)
                if submission.selftext
                else ""
            )

            created_at = (
                datetime.fromtimestamp(submission.created_utc, tz=timezone.utc)
                if hasattr(submission, "created_utc")
                else None
            )

            post = RedditPost(
                type=post_type,
                id=submission.id,
                title=title,
                url=submission.url if not submission.is_self else None,
                upvotes=submission.score,
                text=text_ja,
                permalink=f"https://www.reddit.com{submission.permalink}",
                thumbnail=(submission.thumbnail if hasattr(submission, "thumbnail") else "self"),
                popularity_score=float(submission.score),
                created_at=created_at,
            )

            if not is_within_target_dates(post.created_at, target_dates):
                continue

            posts.append(post)
            dedup_tracker.add(post.title)

        return posts, total_found

    async def _translate_to_japanese(self, text: str) -> str:
        """
        ãƒ†ã‚­ã‚¹ãƒˆã‚’æ—¥æœ¬èªã«ç¿»è¨³ã—ã¾ã™ã€‚

        Parameters
        ----------
        text : str
            ç¿»è¨³ã™ã‚‹ãƒ†ã‚­ã‚¹ãƒˆã€‚

        Returns
        -------
        str
            ç¿»è¨³ã•ã‚ŒãŸãƒ†ã‚­ã‚¹ãƒˆã€‚
        """
        if not text:
            return ""

        try:
            prompt = f"ä»¥ä¸‹ã®è‹±èªã®ãƒ†ã‚­ã‚¹ãƒˆã‚’è‡ªç„¶ãªæ—¥æœ¬èªã«ç¿»è¨³ã—ã¦ãã ã•ã„ã€‚å°‚é–€ç”¨èªã‚„å›ºæœ‰åè©ã¯é©åˆ‡ã«ç¿»è¨³ã—ã€å¿…è¦ã«å¿œã˜ã¦è‹±èªã®åŸèªã‚’æ‹¬å¼§å†…ã«æ®‹ã—ã¦ãã ã•ã„ã€‚\n\n{text}"

            translated_text = self.gpt_client.generate_content(
                prompt=prompt, temperature=0.3, max_tokens=1000
            )

            return translated_text
        except Exception as e:
            self.logger.error(f"Error translating text: {str(e)}")
            return text  # ç¿»è¨³ã«å¤±æ•—ã—ãŸå ´åˆã¯åŸæ–‡ã‚’è¿”ã™

    async def _retrieve_top_comments_of_post(
        self, post: RedditPost, limit: int = 5
    ) -> list[dict[str, str | int]]:
        """
        æŠ•ç¨¿ã®ãƒˆãƒƒãƒ—ã‚³ãƒ¡ãƒ³ãƒˆã‚’å–å¾—ã—ã¾ã™ã€‚

        Parameters
        ----------
        post : RedditPost
            æŠ•ç¨¿æƒ…å ±ã€‚
        limit : int, default=5
            å–å¾—ã™ã‚‹ã‚³ãƒ¡ãƒ³ãƒˆæ•°ã€‚

        Returns
        -------
        List[Dict[str, str | int]]
            å–å¾—ã—ãŸã‚³ãƒ¡ãƒ³ãƒˆã®ãƒªã‚¹ãƒˆã€‚
        """
        submission = await self.reddit.submission(id=post.id)

        # ã‚³ãƒ¡ãƒ³ãƒˆã‚’å–å¾—ã™ã‚‹å‰ã«ã‚½ãƒ¼ãƒˆé †ã‚’è¨­å®š
        await submission.comments.replace_more(limit=0)
        comments_list = submission.comments.list()[:limit]

        comments = []
        for comment in comments_list:
            if hasattr(comment, "body"):
                # ã‚³ãƒ¡ãƒ³ãƒˆã‚’æ—¥æœ¬èªã«ç¿»è¨³
                comment_text_ja = await self._translate_to_japanese(comment.body)

                comments.append(
                    {
                        "text": comment_text_ja,
                        "score": comment.score if hasattr(comment, "score") else 0,
                    }
                )

        return comments

    async def _summarize_reddit_post(self, post: RedditPost) -> None:
        """
        RedditæŠ•ç¨¿ã‚’è¦ç´„ã—ã¾ã™ã€‚

        Parameters
        ----------
        post : RedditPost
            è¦ç´„ã™ã‚‹æŠ•ç¨¿ã€‚
        """
        prompt = f"""
        ä»¥ä¸‹ã®RedditæŠ•ç¨¿ã‚’è¦ç´„ã—ã¦ãã ã•ã„ã€‚

        ã‚¿ã‚¤ãƒˆãƒ«: {post.title}
        æœ¬æ–‡: {post.text if post.text else "(æœ¬æ–‡ãªã—)"}
        URL: {post.url if post.url else "(URLãªã—)"}

        ãƒˆãƒƒãƒ—ã‚³ãƒ¡ãƒ³ãƒˆ:
        {chr(10).join([f"- {comment['text']}" for comment in post.comments])}

        è¦ç´„ã¯ä»¥ä¸‹ã®å½¢å¼ã§è¡Œã„ã€æ—¥æœ¬èªã§å›ç­”ã—ã¦ãã ã•ã„:
        1. æŠ•ç¨¿ã®ä¸»ãªå†…å®¹ï¼ˆ1-2æ–‡ï¼‰
        2. é‡è¦ãªãƒã‚¤ãƒ³ãƒˆï¼ˆç®‡æ¡æ›¸ã3-5ç‚¹ï¼‰
        3. è­°è«–ã®å‚¾å‘ï¼ˆã‚³ãƒ¡ãƒ³ãƒˆã‹ã‚‰ï¼‰
        """

        system_instruction = """
        ã‚ãªãŸã¯RedditæŠ•ç¨¿ã®è¦ç´„ã‚’è¡Œã†ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚
        ä¸ãˆã‚‰ã‚ŒãŸæŠ•ç¨¿ã¨ã‚³ãƒ¡ãƒ³ãƒˆã‚’åˆ†æã—ã€ç°¡æ½”ã§æƒ…å ±é‡ã®å¤šã„è¦ç´„ã‚’ä½œæˆã—ã¦ãã ã•ã„ã€‚
        æŠ€è¡“çš„ãªå†…å®¹ã¯æ­£ç¢ºã«ã€ä¸€èˆ¬çš„ãªå†…å®¹ã¯åˆ†ã‹ã‚Šã‚„ã™ãè¦ç´„ã—ã¦ãã ã•ã„ã€‚
        å›ç­”ã¯å¿…ãšæ—¥æœ¬èªã§è¡Œã£ã¦ãã ã•ã„ã€‚å°‚é–€ç”¨èªã¯é©åˆ‡ã«ç¿»è¨³ã—ã€å¿…è¦ã«å¿œã˜ã¦è‹±èªã®å°‚é–€ç”¨èªã‚’æ‹¬å¼§å†…ã«æ®‹ã—ã¦ãã ã•ã„ã€‚
        """

        try:
            summary = self.gpt_client.generate_content(
                prompt=prompt,
                system_instruction=system_instruction,
                temperature=0.3,
                max_tokens=1000,
            )
            post.summary = summary
        except Exception as e:
            self.logger.error(f"è¦ç´„ã®ç”Ÿæˆä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}")
            post.summary = f"è¦ç´„ã®ç”Ÿæˆä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}"

    async def _store_summaries(
        self,
        posts: list[tuple[str, str, RedditPost]],
        target_dates: list[date],
    ) -> list[tuple[str, str]]:
        if not posts:
            self.logger.info("ä¿å­˜ã™ã‚‹æŠ•ç¨¿ãŒã‚ã‚Šã¾ã›ã‚“")
            return []

        default_date = max(target_dates) if target_dates else datetime.now().date()
        records = self._serialize_posts(posts)
        records_by_date = group_records_by_date(records, default_date=default_date)

        saved_files = await store_daily_snapshots(
            records_by_date,
            load_existing=self._load_existing_posts,
            save_json=self.save_json,
            save_markdown=self.save_markdown,
            render_markdown=self._render_markdown,
            key=lambda item: item.get("id", ""),
            sort_key=self._post_sort_key,
            limit=self.SUMMARY_LIMIT,
            logger=self.logger,
        )

        return saved_files

    def _serialize_posts(self, posts: list[tuple[str, str, RedditPost]]) -> list[dict]:
        records: list[dict] = []
        for category, subreddit, post in posts:
            created_at = post.created_at or datetime.now(timezone.utc)
            records.append(
                {
                    "id": post.id,
                    "category": category,
                    "subreddit": subreddit,
                    "title": post.title,
                    "url": post.url,
                    "permalink": post.permalink,
                    "text": post.text,
                    "upvotes": post.upvotes,
                    "summary": getattr(post, "summary", ""),
                    "type": post.type,
                    "thumbnail": post.thumbnail,
                    "comments": post.comments,
                    "popularity_score": post.popularity_score,
                    "created_at": created_at.isoformat(),
                    "published_at": created_at.isoformat(),
                }
            )
        return records

    async def _load_existing_posts(self, target_date: datetime) -> list[dict]:
        date_str = target_date.strftime("%Y-%m-%d")
        filename_json = f"{date_str}.json"
        existing_json = await self.load_json(filename_json)
        if existing_json:
            if isinstance(existing_json, dict):
                flattened: list[dict] = []
                for subreddit, items in existing_json.items():
                    for item in items:
                        flattened.append({"subreddit": subreddit, **item})
                return flattened
            return existing_json

        markdown = await self.storage.load(f"{date_str}.md")
        if not markdown:
            return []

        return self._parse_markdown(markdown)

    def _post_sort_key(self, item: dict) -> tuple[float, datetime]:
        popularity = float(item.get("popularity_score", 0.0) or 0.0)
        created_raw = item.get("created_at") or item.get("published_at")
        if created_raw:
            try:
                created = datetime.fromisoformat(created_raw)
            except ValueError:
                created = datetime.min.replace(tzinfo=timezone.utc)
        else:
            created = datetime.min.replace(tzinfo=timezone.utc)
        return (popularity, created)

    def _extract_post_id_from_permalink(self, permalink: str) -> str:
        if not permalink:
            return ""

        trimmed = permalink.strip()
        trimmed = trimmed.split("?", 1)[0]
        trimmed = trimmed.rstrip("/")

        parts = trimmed.split("/")
        try:
            comments_index = parts.index("comments")
        except ValueError:
            comments_index = -1

        if comments_index != -1 and comments_index + 1 < len(parts):
            post_id = parts[comments_index + 1]
            return post_id.strip()

        for part in reversed(parts):
            if part:
                return part.strip()

        return ""

    def _render_markdown(self, records: list[dict], today: datetime) -> str:
        content = f"# Reddit äººæ°—æŠ•ç¨¿ ({today.strftime('%Y-%m-%d')})\n\n"
        grouped: dict[str, list[dict]] = {}
        for record in records:
            subreddit = record.get("subreddit", "unknown")
            grouped.setdefault(subreddit, []).append(record)

        for subreddit, posts in grouped.items():
            content += f"## r/{subreddit}\n\n"
            for post in posts:
                content += f"### [{post['title']}]({post.get('permalink')})\n\n"
                url = post.get("url")
                if url and url != post.get("permalink"):
                    content += f"ãƒªãƒ³ã‚¯: {url}\n\n"
                text = post.get("text")
                if text:
                    trimmed = text[:200]
                    ellipsis = "..." if len(text) > 200 else ""
                    content += f"æœ¬æ–‡: {trimmed}{ellipsis}\n\n"
                content += f"ã‚¢ãƒƒãƒ—ãƒœãƒ¼ãƒˆæ•°: {post.get('upvotes', 0)}\n\n"
                content += f"**è¦ç´„**:\n{post.get('summary', '')}\n\n"
                content += "---\n\n"
        return content

    def _parse_markdown(self, markdown: str) -> list[dict]:
        records: list[dict] = []
        subreddit_pattern = re.compile(r"^##\s+r/(?P<subreddit>.+)$", re.MULTILINE)
        post_pattern = re.compile(
            r"### \[(?P<title>.+?)\]\((?P<permalink>[^\)]+)\)\n\n"
            r"(?:ãƒªãƒ³ã‚¯: (?P<link>.+?)\n\n)?"
            r"(?:æœ¬æ–‡: (?P<text>.+?)\n\n)?"
            r"ã‚¢ãƒƒãƒ—ãƒœãƒ¼ãƒˆæ•°: (?P<upvotes>\d+)\n\n"
            r"\*\*è¦ç´„\*\*:\n(?P<summary>.*?)(?:\n\n)?---",
            re.DOTALL,
        )

        sections = list(subreddit_pattern.finditer(markdown))
        for idx, match in enumerate(sections):
            start = match.end()
            end = sections[idx + 1].start() if idx + 1 < len(sections) else len(markdown)
            block = markdown[start:end]
            subreddit = match.group("subreddit").strip()

            for post_match in post_pattern.finditer(block + "---"):
                permalink = post_match.group("permalink")
                record = {
                    "id": self._extract_post_id_from_permalink(permalink),
                    "category": "unknown",
                    "title": post_match.group("title").strip(),
                    "url": (post_match.group("link") or "").strip() or None,
                    "permalink": permalink.strip() if permalink else "",
                    "text": (post_match.group("text") or "").strip(),
                    "upvotes": int(post_match.group("upvotes") or 0),
                    "summary": post_match.group("summary").strip(),
                    "type": "text",
                    "thumbnail": "self",
                    "comments": [],
                    "popularity_score": 0.0,
                    "subreddit": subreddit,
                }
                records.append(record)

        return records

    def _select_top_posts(
        self, posts: list[tuple[str, str, RedditPost]]
    ) -> list[tuple[str, str, RedditPost]]:
        """äººæ°—é †ã«æŠ•ç¨¿ã‚’ä¸¦ã¹æ›¿ãˆã€ä¸Šä½ã®ã¿è¿”ã—ã¾ã™ã€‚"""
        if not posts:
            return []

        if len(posts) <= self.SUMMARY_LIMIT:
            return posts

        def sort_key(item: tuple[str, str, RedditPost]):
            _, _, post = item
            created = post.created_at or datetime.min
            return (post.popularity_score, created)

        sorted_posts = sorted(posts, key=sort_key, reverse=True)
        return sorted_posts[: self.SUMMARY_LIMIT]

    async def _load_existing_titles(self) -> DedupTracker:
        tracker = DedupTracker()
        try:
            content = self.storage.load_markdown("", datetime.now())
            if content:
                for match in re.finditer(r"^### \[(.+?)\]", content, re.MULTILINE):
                    tracker.add(match.group(1))
        except Exception as exc:
            self.logger.debug(f"æ—¢å­˜RedditæŠ•ç¨¿ã‚¿ã‚¤ãƒˆãƒ«ã®èª­ã¿è¾¼ã¿ã«å¤±æ•—ã—ã¾ã—ãŸ: {exc}")
        return tracker
