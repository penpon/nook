"""Zennã®RSSãƒ•ã‚£ãƒ¼ãƒ‰ã‚’ç›£è¦–ãƒ»åé›†ãƒ»è¦ç´„ã™ã‚‹ã‚µãƒ¼ãƒ“ã‚¹ã€‚"""

import asyncio
import re
from dataclasses import dataclass, field
from datetime import date, datetime, timedelta, timezone
from pathlib import Path

import feedparser
import tomli
from bs4 import BeautifulSoup

from nook.common.base_service import BaseService
from nook.common.dedup import DedupTracker
from nook.common.daily_snapshot import group_records_by_date, store_daily_snapshots
from nook.common.date_utils import is_within_target_dates, target_dates_set
from nook.common.feed_utils import parse_entry_datetime


@dataclass
class Article:
    """
    Zennè¨˜äº‹ã®æƒ…å ±ã€‚

    Parameters
    ----------
    feed_name : str
        ãƒ•ã‚£ãƒ¼ãƒ‰åã€‚
    title : str
        ã‚¿ã‚¤ãƒˆãƒ«ã€‚
    url : str
        URLã€‚
    text : str
        æœ¬æ–‡ã€‚
    soup : BeautifulSoup
        BeautifulSoupã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã€‚
    category : str | None
        ã‚«ãƒ†ã‚´ãƒªã€‚
    """

    feed_name: str
    title: str
    url: str
    text: str
    soup: BeautifulSoup
    category: str | None = None
    summary: str = field(default="")
    popularity_score: float = field(default=0.0)
    published_at: datetime | None = None


class ZennExplorer(BaseService):
    """
    Zennã®RSSãƒ•ã‚£ãƒ¼ãƒ‰ã‚’ç›£è¦–ãƒ»åé›†ãƒ»è¦ç´„ã™ã‚‹ã‚¯ãƒ©ã‚¹ã€‚

    Parameters
    ----------
    storage_dir : str, default="data"
        ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®ãƒ‘ã‚¹ã€‚
    """

    SUMMARY_LIMIT = 15

    def __init__(self, storage_dir: str = "data"):
        """
        ZennExplorerã‚’åˆæœŸåŒ–ã—ã¾ã™ã€‚

        Parameters
        ----------
        storage_dir : str, default="data"
            ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®ãƒ‘ã‚¹ã€‚
        """
        super().__init__("zenn_explorer")
        self.http_client = None  # setup_http_clientã§åˆæœŸåŒ–

        # ãƒ•ã‚£ãƒ¼ãƒ‰ã®è¨­å®šã‚’èª­ã¿è¾¼ã‚€
        script_dir = Path(__file__).parent
        with open(script_dir / "feed.toml", "rb") as f:
            self.feed_config = tomli.load(f)

    def run(self, days: int = 1, limit: int | None = None) -> None:
        """
        Zennã®RSSãƒ•ã‚£ãƒ¼ãƒ‰ã‚’ç›£è¦–ãƒ»åé›†ãƒ»è¦ç´„ã—ã¦ä¿å­˜ã—ã¾ã™ã€‚

        Parameters
        ----------
        days : int, default=1
            ä½•æ—¥å‰ã¾ã§ã®è¨˜äº‹ã‚’å–å¾—ã™ã‚‹ã‹ã€‚
        limit : Optional[int], default=None
            å„ãƒ•ã‚£ãƒ¼ãƒ‰ã‹ã‚‰å–å¾—ã™ã‚‹è¨˜äº‹æ•°ã€‚Noneã®å ´åˆã¯åˆ¶é™ãªã—ã€‚
        """
        asyncio.run(self.collect(days, limit))

    async def collect(
        self,
        days: int = 1,
        limit: int | None = None,
        *,
        target_dates: set[date] | None = None,
    ) -> list[tuple[str, str]]:
        """
        Zennã®RSSãƒ•ã‚£ãƒ¼ãƒ‰ã‚’ç›£è¦–ãƒ»åé›†ãƒ»è¦ç´„ã—ã¦ä¿å­˜ã—ã¾ã™ï¼ˆéåŒæœŸç‰ˆï¼‰ã€‚

        Parameters
        ----------
        days : int, default=1
            ä½•æ—¥å‰ã¾ã§ã®è¨˜äº‹ã‚’å–å¾—ã™ã‚‹ã‹ã€‚
        limit : Optional[int], default=None
            å„ãƒ•ã‚£ãƒ¼ãƒ‰ã‹ã‚‰å–å¾—ã™ã‚‹è¨˜äº‹æ•°ã€‚Noneã®å ´åˆã¯åˆ¶é™ãªã—ã€‚

        Returns
        -------
        list[tuple[str, str]]
            ä¿å­˜ã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ã®ãƒªã‚¹ãƒˆ [(json_path, md_path), ...]
        """
        # HTTPã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã®åˆæœŸåŒ–ã‚’ç¢ºèª
        if self.http_client is None:
            await self.setup_http_client()

        candidate_articles: list[Article] = []
        dedup_tracker = (
            self._load_existing_titles()
        )  # ã‚«ãƒ†ã‚´ãƒªæ¨ªæ–­ã®ã‚¿ã‚¤ãƒˆãƒ«é‡è¤‡ãƒã‚§ãƒƒã‚¯ç”¨
        effective_target_dates = target_dates or target_dates_set(days)

        self.logger.info("\nğŸ“¡ ãƒ•ã‚£ãƒ¼ãƒ‰å–å¾—ä¸­...")

        try:
            # å„ã‚«ãƒ†ã‚´ãƒªã®ãƒ•ã‚£ãƒ¼ãƒ‰ã‹ã‚‰è¨˜äº‹ã‚’å–å¾—
            total_entries = 0
            for category, feeds in self.feed_config.items():
                for feed_url in feeds:
                    try:
                        # ãƒ•ã‚£ãƒ¼ãƒ‰ã‚’è§£æ
                        feed = feedparser.parse(feed_url)
                        feed_name = (
                            feed.feed.title
                            if hasattr(feed, "feed") and hasattr(feed.feed, "title")
                            else feed_url
                        )

                        effective_limit = None
                        if limit is not None:
                            effective_limit = limit * max(days, 1)

                        entries = self._filter_entries(
                            feed.entries, effective_target_dates, effective_limit
                        )
                        total_entries += len(entries)
                        self.logger.info(f"   â€¢ {feed_name}: {len(entries)}ä»¶å–å¾—")

                        for entry in entries:
                            # è¨˜äº‹ã‚’å–å¾—
                            article = await self._retrieve_article(
                                entry, feed_name, category
                            )
                            if article:
                                # é‡è¤‡ã‚¿ã‚¤ãƒˆãƒ«ã‚’ã‚¹ã‚­ãƒƒãƒ—ï¼ˆã‚«ãƒ†ã‚´ãƒªæ¨ªæ–­ãƒ»æ­£è¦åŒ–æ¸ˆã¿ï¼‰
                                is_dup, normalized_title = dedup_tracker.is_duplicate(
                                    article.title
                                )
                                if is_dup:
                                    original = dedup_tracker.get_original_title(
                                        normalized_title
                                    )
                                    self.logger.info(
                                        f"é‡è¤‡è¨˜äº‹ã‚’ã‚¹ã‚­ãƒƒãƒ—: '{article.title}' "
                                        f"(æ­£è¦åŒ–å¾Œ: '{normalized_title}', åˆå‡º: '{original}')"
                                    )
                                    continue

                                # æ—¥ä»˜ç¯„å›²ãƒã‚§ãƒƒã‚¯ã‚’é‡è¤‡ãƒˆãƒ©ãƒƒã‚­ãƒ³ã‚°ã®å‰ã«å®Ÿè¡Œ
                                if not is_within_target_dates(
                                    article.published_at, effective_target_dates
                                ):
                                    continue

                                dedup_tracker.add(article.title)
                                candidate_articles.append(article)

                    except Exception as e:
                        self.logger.error(
                            f"ãƒ•ã‚£ãƒ¼ãƒ‰ {feed_url} ã®å‡¦ç†ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}"
                        )

            # æ—¥ä»˜ã”ã¨ã«ã‚°ãƒ«ãƒ¼ãƒ—åŒ–
            articles_by_date = self._group_articles_by_date(candidate_articles)

            # æ—¥ä»˜ã”ã¨ã«ä¸Šä½Nä»¶ã‚’é¸æŠã—ã¦è¦ç´„ï¼ˆå¤ã„æ—¥ä»˜ã‹ã‚‰æ–°ã—ã„æ—¥ä»˜ã¸ï¼‰
            all_selected_articles = []
            for date_str in sorted(articles_by_date.keys()):
                date_articles = articles_by_date[date_str]

                # æ—¥ä»˜æƒ…å ±ã‚’å…ˆé ­ã«è¡¨ç¤º
                self.logger.info(f"\nğŸ“° [{date_str}] ã®è¨˜äº‹ã‚’å‡¦ç†ä¸­...")
                self.logger.info(f"   ğŸ” å€™è£œè¨˜äº‹: {len(date_articles)}ä»¶")

                selected = self._select_top_articles(date_articles)

                self.logger.info(
                    f"   âœ… é¸æŠã•ã‚ŒãŸè¨˜äº‹ ({len(selected)}/{len(date_articles)}):"
                )
                for idx, article in enumerate(selected, 1):
                    self.logger.info(
                        f"      {idx}. ã€Œ{article.title}ã€(ã‚¹ã‚³ã‚¢: {article.popularity_score:.0f})"
                    )

                # è¦ç´„ç”Ÿæˆ
                if selected:
                    self.logger.info(f"\n   ğŸ¤– è¦ç´„ç”Ÿæˆä¸­...")
                    for idx, article in enumerate(selected, 1):
                        await self._summarize_article(article)
                        self.logger.info(
                            f"      âœ“ {idx}/{len(selected)}: ã€Œ{article.title[:50]}...ã€"
                        )

                all_selected_articles.extend(selected)

            # è¦ç´„ã‚’ä¿å­˜
            saved_files: list[tuple[str, str]] = []
            if all_selected_articles:
                saved_files = await self._store_summaries(
                    all_selected_articles, effective_target_dates
                )
                self.logger.info(f"\nğŸ’¾ {len(saved_files)}æ—¥åˆ†ã®ãƒ‡ãƒ¼ã‚¿ã‚’ä¿å­˜å®Œäº†")
            else:
                self.logger.info("\nä¿å­˜ã™ã‚‹è¨˜äº‹ãŒã‚ã‚Šã¾ã›ã‚“")

            return saved_files

        finally:
            # ã‚°ãƒ­ãƒ¼ãƒãƒ«ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆãªã®ã§ã‚¯ãƒ­ãƒ¼ã‚ºä¸è¦
            pass

    def _group_articles_by_date(
        self, articles: list[Article]
    ) -> dict[str, list[Article]]:
        """è¨˜äº‹ã‚’æ—¥ä»˜ã”ã¨ã«ã‚°ãƒ«ãƒ¼ãƒ—åŒ–ã—ã¾ã™ã€‚"""
        by_date: dict[str, list[Article]] = {}
        default_date = datetime.now().strftime("%Y-%m-%d")

        for article in articles:
            date_key = (
                article.published_at.strftime("%Y-%m-%d")
                if article.published_at
                else default_date
            )
            by_date.setdefault(date_key, []).append(article)

        return by_date

    def _load_existing_titles(self) -> DedupTracker:
        tracker = DedupTracker()
        try:
            content = self.storage.load_markdown("", datetime.now())
            if content:
                for match in re.finditer(r"^### \[(.+?)\]", content, re.MULTILINE):
                    tracker.add(match.group(1))
        except Exception as exc:
            self.logger.debug(f"æ—¢å­˜ã‚¿ã‚¤ãƒˆãƒ«ã®èª­ã¿è¾¼ã¿ã«å¤±æ•—ã—ã¾ã—ãŸ: {exc}")
        return tracker

    def _filter_entries(
        self,
        entries: list[dict],
        target_dates: set[date],
        limit: int | None = None,
    ) -> list[dict]:
        """
        æ–°ã—ã„ã‚¨ãƒ³ãƒˆãƒªã‚’ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ã—ã¾ã™ã€‚

        Parameters
        ----------
        entries : List[dict]
            ã‚¨ãƒ³ãƒˆãƒªã®ãƒªã‚¹ãƒˆã€‚
        days : int
            ä½•æ—¥å‰ã¾ã§ã®è¨˜äº‹ã‚’å–å¾—ã™ã‚‹ã‹ã€‚
        limit : Optional[int], default=None
            å–å¾—ã™ã‚‹è¨˜äº‹æ•°ã€‚Noneã®å ´åˆã¯å…¨ã¦å–å¾—ã€‚

        Returns
        -------
        List[dict]
            ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ã•ã‚ŒãŸã‚¨ãƒ³ãƒˆãƒªã®ãƒªã‚¹ãƒˆã€‚
        """
        # æ—¥ä»˜ã§ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
        recent_entries = []

        for entry in entries:
            entry_date = parse_entry_datetime(entry)

            if entry_date:
                if is_within_target_dates(entry_date, target_dates):
                    recent_entries.append(entry)
                else:
                    self.logger.debug(
                        "å¯¾è±¡å¤–æ—¥ä»˜ã®è¨˜äº‹ã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™ã€‚ raw=%s",
                        getattr(entry, "published", getattr(entry, "updated", "")),
                    )

        # limitãŒNoneã®å ´åˆã¯å…¨ã¦ã®ã‚¨ãƒ³ãƒˆãƒªã‚’è¿”ã™
        if limit is None:
            return recent_entries
        # ãã†ã§ãªã‘ã‚Œã°æŒ‡å®šã•ã‚ŒãŸæ•°ã ã‘è¿”ã™
        return recent_entries[:limit]

    async def _retrieve_article(
        self, entry: dict, feed_name: str, category: str
    ) -> Article | None:
        """
        è¨˜äº‹ã‚’å–å¾—ã—ã¾ã™ã€‚

        Parameters
        ----------
        entry : dict
            ã‚¨ãƒ³ãƒˆãƒªæƒ…å ±ã€‚
        feed_name : str
            ãƒ•ã‚£ãƒ¼ãƒ‰åã€‚
        category : str
            ã‚«ãƒ†ã‚´ãƒªã€‚

        Returns
        -------
        Article or None
            å–å¾—ã—ãŸè¨˜äº‹ã€‚å–å¾—ã«å¤±æ•—ã—ãŸå ´åˆã¯Noneã€‚
        """
        try:
            # URLã‚’å–å¾—
            url = entry.link if hasattr(entry, "link") else None
            if not url:
                return None

            # ã‚¿ã‚¤ãƒˆãƒ«ã‚’å–å¾—
            title = entry.title if hasattr(entry, "title") else "ç„¡é¡Œ"

            # è¨˜äº‹ã®å†…å®¹ã‚’å–å¾—
            response = await self.http_client.get(url)
            soup = BeautifulSoup(response.text, "html.parser")

            # æœ¬æ–‡ã‚’æŠ½å‡º
            text = ""

            # ã¾ãšã¯ã‚¨ãƒ³ãƒˆãƒªã®è¦ç´„ã‚’ä½¿ç”¨
            if hasattr(entry, "summary"):
                text = entry.summary

            # æ¬¡ã«è¨˜äº‹ã®æœ¬æ–‡ã‚’æŠ½å‡º
            if not text:
                # ãƒ¡ã‚¿ãƒ‡ã‚£ã‚¹ã‚¯ãƒªãƒ—ã‚·ãƒ§ãƒ³ã‚’å–å¾—
                meta_desc = soup.find("meta", attrs={"name": "description"})
                if meta_desc and meta_desc.get("content"):
                    text = meta_desc.get("content")
                else:
                    # æœ¬æ–‡ã®æœ€åˆã®æ®µè½ã‚’å–å¾—
                    paragraphs = soup.find_all("p")
                    if paragraphs:
                        text = "\n".join([p.get_text() for p in paragraphs[:5]])

            popularity = self._extract_popularity(entry, soup)
            published_at = parse_entry_datetime(entry)

            return Article(
                feed_name=feed_name,
                title=title,
                url=url,
                text=text,
                soup=soup,
                category=category,
                popularity_score=popularity,
                published_at=published_at,
            )

        except Exception as e:
            self.logger.error(
                f"è¨˜äº‹ {entry.get('link', 'ä¸æ˜')} ã®å–å¾—ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}"
            )
            return None

    async def _summarize_article(self, article: Article) -> None:
        """
        è¨˜äº‹ã‚’è¦ç´„ã—ã¾ã™ã€‚

        Parameters
        ----------
        article : Article
            è¦ç´„ã™ã‚‹è¨˜äº‹ã€‚
        """
        prompt = f"""
        ä»¥ä¸‹ã®Zennè¨˜äº‹ã‚’è¦ç´„ã—ã¦ãã ã•ã„ã€‚

        ã‚¿ã‚¤ãƒˆãƒ«: {article.title}
        æœ¬æ–‡: {article.text[:2000]}

        è¦ç´„ã¯ä»¥ä¸‹ã®å½¢å¼ã§è¡Œã„ã€æ—¥æœ¬èªã§å›ç­”ã—ã¦ãã ã•ã„:
        1. è¨˜äº‹ã®ä¸»ãªå†…å®¹ï¼ˆ1-2æ–‡ï¼‰
        2. é‡è¦ãªãƒã‚¤ãƒ³ãƒˆï¼ˆç®‡æ¡æ›¸ã3-5ç‚¹ï¼‰
        3. æŠ€è¡“çš„ãªæ´å¯Ÿ
        """

        system_instruction = """
        ã‚ãªãŸã¯Zennã®æŠ€è¡“è¨˜äº‹ã®è¦ç´„ã‚’è¡Œã†ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚
        ä¸ãˆã‚‰ã‚ŒãŸè¨˜äº‹ã‚’åˆ†æã—ã€ç°¡æ½”ã§æƒ…å ±é‡ã®å¤šã„è¦ç´„ã‚’ä½œæˆã—ã¦ãã ã•ã„ã€‚
        æŠ€è¡“çš„ãªå†…å®¹ã¯æ­£ç¢ºã«ã€ä¸€èˆ¬çš„ãªå†…å®¹ã¯åˆ†ã‹ã‚Šã‚„ã™ãè¦ç´„ã—ã¦ãã ã•ã„ã€‚
        å›ç­”ã¯å¿…ãšæ—¥æœ¬èªã§è¡Œã£ã¦ãã ã•ã„ã€‚
        """

        try:
            summary = self.gpt_client.generate_content(
                prompt=prompt,
                system_instruction=system_instruction,
                temperature=0.3,
                max_tokens=1000,
            )
            article.summary = summary
        except Exception as e:
            self.logger.error(f"è¦ç´„ã®ç”Ÿæˆä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}")
            article.summary = f"è¦ç´„ã®ç”Ÿæˆä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}"

    async def _store_summaries(
        self, articles: list[Article], target_dates: set[date]
    ) -> list[tuple[str, str]]:
        if not articles:
            self.logger.info("ä¿å­˜ã™ã‚‹è¨˜äº‹ãŒã‚ã‚Šã¾ã›ã‚“")
            return []

        default_date = max(target_dates) if target_dates else datetime.now().date()
        incoming_records = self._serialize_articles(articles)
        records_by_date = group_records_by_date(
            incoming_records,
            default_date=default_date,
        )

        saved_files = await store_daily_snapshots(
            records_by_date,
            load_existing=self._load_existing_articles,
            save_json=self.save_json,
            save_markdown=self.save_markdown,
            render_markdown=self._render_markdown,
            key=lambda item: item.get("title", ""),
            sort_key=self._article_sort_key,
            limit=self.SUMMARY_LIMIT,
            logger=None,  # æ—¥ä»˜æƒ…å ±ã®äºŒé‡è¡¨ç¤ºã‚’é˜²ã
        )

        return saved_files

    def _serialize_articles(self, articles: list[Article]) -> list[dict]:
        records: list[dict] = []
        for article in articles:
            category = article.category or "uncategorized"
            records.append(
                {
                    "title": article.title,
                    "url": article.url,
                    "feed_name": article.feed_name,
                    "summary": article.summary,
                    "popularity_score": article.popularity_score,
                    "published_at": (
                        article.published_at.isoformat()
                        if article.published_at
                        else None
                    ),
                    "category": category,
                }
            )
        return records

    async def _load_existing_articles(self, target_date: datetime) -> list[dict]:
        date_str = target_date.strftime("%Y-%m-%d")
        filename_json = f"{date_str}.json"
        filename_md = f"{date_str}.md"

        existing_json = await self.load_json(filename_json)
        if existing_json:
            return existing_json

        markdown = await self.storage.load(filename_md)
        if not markdown:
            return []

        return self._parse_markdown(markdown)

    def _article_sort_key(self, item: dict) -> tuple[float, datetime]:
        popularity = float(item.get("popularity_score", 0.0) or 0.0)
        published_raw = item.get("published_at")
        if published_raw:
            try:
                published = datetime.fromisoformat(published_raw)
            except ValueError:
                published = datetime.min
        else:
            published = datetime.min
        return (popularity, published)

    def _render_markdown(self, records: list[dict], today: datetime) -> str:
        content = f"# Zennè¨˜äº‹ ({today.strftime('%Y-%m-%d')})\n\n"
        grouped: dict[str, list[dict]] = {}
        for record in records:
            category = record.get("category", "uncategorized")
            grouped.setdefault(category, []).append(record)

        for category, articles in grouped.items():
            heading = category.replace("_", " ").capitalize()
            content += f"## {heading}\n\n"

            for article in articles:
                content += f"### [{article['title']}]({article['url']})\n\n"
                content += f"**ãƒ•ã‚£ãƒ¼ãƒ‰**: {article.get('feed_name', '')}\n\n"
                content += f"**è¦ç´„**:\n{article.get('summary', '')}\n\n"
                content += "---\n\n"
        return content

    def _parse_markdown(self, markdown: str) -> list[dict]:
        result: list[dict] = []
        category_pattern = re.compile(r"^##\s+(.+)$", re.MULTILINE)
        article_pattern = re.compile(
            r"### \[(?P<title>.+?)\]\((?P<url>[^\)]+)\)\n\n"
            r"\*\*ãƒ•ã‚£ãƒ¼ãƒ‰\*\*: (?P<feed>.+?)\n\n"
            r"\*\*è¦ç´„\*\*:\n(?P<summary>.*?)(?:\n\n)?---",
            re.DOTALL,
        )

        sections = list(category_pattern.finditer(markdown))
        for idx, match in enumerate(sections):
            start = match.end()
            end = (
                sections[idx + 1].start() if idx + 1 < len(sections) else len(markdown)
            )
            block = markdown[start:end]
            category = match.group(1).strip().lower().replace(" ", "_")

            for article_match in article_pattern.finditer(block + "---"):
                result.append(
                    {
                        "title": article_match.group("title").strip(),
                        "url": article_match.group("url").strip(),
                        "feed_name": article_match.group("feed").strip(),
                        "summary": article_match.group("summary").strip(),
                        "popularity_score": 0.0,
                        "published_at": None,
                        "category": category,
                    }
                )

        return result

    def _extract_popularity(self, entry, soup: BeautifulSoup) -> float:
        """è¨˜äº‹ã®äººæ°—æŒ‡æ¨™ï¼ˆã„ã„ã­æ•°ï¼‰ã‚’æŠ½å‡ºã—ã¾ã™ã€‚"""
        # 1. ãƒ¡ã‚¿ã‚¿ã‚°ï¼ˆæœ€å„ªå…ˆï¼‰
        meta_like = soup.find("meta", attrs={"property": "zenn:likes_count"})
        if meta_like and meta_like.get("content"):
            value = self._safe_parse_int(meta_like.get("content"))
            if value is not None:
                return float(value)

        candidates: list[int] = []

        # 2. dataå±æ€§ã‚’æŒã¤è¦ç´ 
        for element in soup.select("[data-like-count]"):
            value = self._safe_parse_int(element.get("data-like-count"))
            if value is not None:
                candidates.append(value)

        # 3. ãƒœã‚¿ãƒ³ã‚„ã‚¹ãƒ‘ãƒ³ã®ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰æŠ½å‡º
        for selector in ["button", "span", "div"]:
            for element in soup.select(selector):
                text = element.get_text(strip=True)
                if not text:
                    continue
                if "ã„ã„ã­" in text:
                    value = self._safe_parse_int(text)
                    if value is not None:
                        candidates.append(value)

        if candidates:
            return float(max(candidates))

        # 4. ãƒ•ã‚£ãƒ¼ãƒ‰ã‚¨ãƒ³ãƒˆãƒªã«å«ã¾ã‚Œã‚‹æ—¢çŸ¥ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰
        try:
            like_candidate = getattr(entry, "likes", None) or getattr(
                entry, "likes_count", None
            )
            if like_candidate is None and hasattr(entry, "zenn_likes_count"):
                like_candidate = getattr(entry, "zenn_likes_count")
            value = self._safe_parse_int(like_candidate)
            if value is not None:
                return float(value)
        except Exception as exc:
            self.logger.debug(f"ãƒ•ã‚£ãƒ¼ãƒ‰å†…äººæ°—æƒ…å ±ã®æŠ½å‡ºã«å¤±æ•—ã—ã¾ã—ãŸ: {exc}")

        return 0.0

    def _safe_parse_int(self, value) -> int | None:
        """ã•ã¾ã–ã¾ãªå€¤ã‹ã‚‰æ•´æ•°ã‚’æŠ½å‡ºã—ã¾ã™ã€‚"""
        if value is None:
            return None
        if isinstance(value, (int, float)):
            return int(value)
        if isinstance(value, str):
            match = re.search(r"(-?\d+)", value.replace(",", ""))
            if match:
                try:
                    return int(match.group(1))
                except ValueError:
                    return None
        return None

    def _select_top_articles(self, articles: list[Article]) -> list[Article]:
        """äººæ°—ã‚¹ã‚³ã‚¢é †ã«è¨˜äº‹ã‚’ã‚½ãƒ¼ãƒˆã—ã€ä¸Šä½ã®ã¿è¿”ã—ã¾ã™ã€‚"""
        if not articles:
            return []

        if len(articles) <= self.SUMMARY_LIMIT:
            return articles

        def sort_key(article: Article):
            published = article.published_at or datetime.min
            return (article.popularity_score, published)

        sorted_articles = sorted(articles, key=sort_key, reverse=True)
        return sorted_articles[: self.SUMMARY_LIMIT]
