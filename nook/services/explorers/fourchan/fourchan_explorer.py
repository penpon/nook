"""4chanã‹ã‚‰ã®AIé–¢é€£ã‚¹ãƒ¬ãƒƒãƒ‰åé›†ã‚µãƒ¼ãƒ“ã‚¹ã€‚"""

import asyncio
import re
from dataclasses import dataclass, field
from datetime import date, datetime, timezone
from pathlib import Path
from typing import Any

import tomli

from nook.core.clients.gpt_client import GPTClient
from nook.core.logging.logging_utils import (
    log_article_counts,
    log_no_new_articles,
    log_processing_start,
    log_storage_complete,
    log_summarization_progress,
    log_summarization_start,
    log_summary_candidates,
)
from nook.core.storage import LocalStorage
from nook.core.storage.daily_snapshot import (
    group_records_by_date,
    store_daily_snapshots,
)
from nook.core.utils.date_utils import is_within_target_dates, target_dates_set
from nook.core.utils.dedup import DedupTracker
from nook.services.base.base_service import BaseService


@dataclass
class Thread:
    """
    4chanã‚¹ãƒ¬ãƒƒãƒ‰æƒ…å ±ã€‚

    Parameters
    ----------
    thread_id : int
        ã‚¹ãƒ¬ãƒƒãƒ‰IDã€‚
    title : str
        ã‚¹ãƒ¬ãƒƒãƒ‰ã‚¿ã‚¤ãƒˆãƒ«ã€‚
    url : str
        ã‚¹ãƒ¬ãƒƒãƒ‰URLã€‚
    board : str
        ãƒœãƒ¼ãƒ‰åã€‚
    posts : List[Dict[str, Any]]
        æŠ•ç¨¿ãƒªã‚¹ãƒˆã€‚
    timestamp : int
        ä½œæˆã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ã€‚
    """

    thread_id: int
    title: str
    url: str
    board: str
    posts: list[dict[str, Any]]
    timestamp: int
    summary: str = field(default="")
    popularity_score: float = field(default=0.0)


class FourChanExplorer(BaseService):
    """
    4chanã‹ã‚‰AIé–¢é€£ã‚¹ãƒ¬ãƒƒãƒ‰ã‚’åé›†ã™ã‚‹ã‚¯ãƒ©ã‚¹ã€‚

    Parameters
    ----------
    storage_dir : str, default="data"
        ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®ãƒ‘ã‚¹ã€‚
    """

    TOTAL_LIMIT = 15

    def __init__(self, storage_dir: str = "var/data", test_mode: bool = False):
        """
        FourChanExplorerã‚’åˆæœŸåŒ–ã—ã¾ã™ã€‚

        Parameters
        ----------
        storage_dir : str, default="var/data"
            ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®ãƒ‘ã‚¹ã€‚
        test_mode : bool, default=False
            ãƒ†ã‚¹ãƒˆãƒ¢ãƒ¼ãƒ‰ã®å ´åˆã¯é…å»¶ã‚’çŸ­ç¸®ã—ã¾ã™ã€‚
        """
        super().__init__("fourchan_explorer")
        self.http_client = None  # setup_http_clientã§åˆæœŸåŒ–
        self.gpt_client = GPTClient()

        storage_path = Path(storage_dir)
        if storage_path.name != self.service_name:
            storage_path = storage_path / self.service_name
        self.storage = LocalStorage(str(storage_path))

        # å¯¾è±¡ã¨ãªã‚‹ãƒœãƒ¼ãƒ‰ã‚’è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰èª­ã¿è¾¼ã‚€
        self.target_boards = self._load_boards()

        # AIã«é–¢é€£ã™ã‚‹ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰
        self.ai_keywords = [
            "ai",
            "artificial intelligence",
            "machine learning",
            "ml",
            "deep learning",
            "neural network",
            "gpt",
            "llm",
            "chatgpt",
            "claude",
            "gemini",
            "grok",
            "anthropic",
            "openai",
            "stable diffusion",
            "dalle",
            "midjourney",
        ]

        # APIãƒªã‚¯ã‚¨ã‚¹ãƒˆé–“ã®é…å»¶ã‚’ç„¡åŠ¹åŒ–ï¼ˆé€Ÿåº¦å„ªå…ˆï¼‰
        # ãƒ†ã‚¹ãƒˆãƒ¢ãƒ¼ãƒ‰ã§ã¯å…ƒã€…çŸ­ã„é…å»¶ã‚’ä½¿ç”¨
        self.request_delay = 0.1 if test_mode else 0  # ç§’

    def _load_boards(self) -> list[str]:
        """
        å¯¾è±¡ã¨ãªã‚‹ãƒœãƒ¼ãƒ‰ã®è¨­å®šã‚’èª­ã¿è¾¼ã¿ã¾ã™ã€‚

        Returns
        -------
        List[str]
            ãƒœãƒ¼ãƒ‰IDã®ãƒªã‚¹ãƒˆ
        """
        script_dir = Path(__file__).parent
        boards_file = script_dir / "boards.toml"

        # boards.tomlãŒå­˜åœ¨ã—ãªã„å ´åˆã¯ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤ã‚’ä½¿ç”¨
        if not boards_file.exists():
            self.logger.warning(f"è­¦å‘Š: {boards_file} ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã®ãƒœãƒ¼ãƒ‰ã‚’ä½¿ç”¨ã—ã¾ã™ã€‚")
            return ["g", "sci", "biz", "pol"]

        try:
            with open(boards_file, "rb") as f:
                config = tomli.load(f)
                boards_dict = config.get("boards", {})
                # ãƒœãƒ¼ãƒ‰IDã®ãƒªã‚¹ãƒˆã‚’è¿”ã™
                return list(boards_dict.keys())
        except Exception as e:
            self.logger.error(f"ã‚¨ãƒ©ãƒ¼: boards.tomlã®èª­ã¿è¾¼ã¿ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")
            self.logger.info("ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã®ãƒœãƒ¼ãƒ‰ã‚’ä½¿ç”¨ã—ã¾ã™ã€‚")
            return ["g", "sci", "biz", "pol"]

    def run(self, thread_limit: int | None = None) -> None:
        """
        4chanã‹ã‚‰AIé–¢é€£ã‚¹ãƒ¬ãƒƒãƒ‰ã‚’åé›†ã—ã¦ä¿å­˜ã—ã¾ã™ã€‚

        Parameters
        ----------
        thread_limit : Optional[int], default=None
            å„ãƒœãƒ¼ãƒ‰ã‹ã‚‰å–å¾—ã™ã‚‹ã‚¹ãƒ¬ãƒƒãƒ‰æ•°ã€‚Noneã®å ´åˆã¯åˆ¶é™ãªã—ã€‚
        """
        asyncio.run(self.collect(thread_limit))

    async def collect(
        self,
        thread_limit: int | None = None,
        *,
        target_dates: list[date] | None = None,
    ) -> list[tuple[str, str]]:
        """
        4chanã‹ã‚‰AIé–¢é€£ã‚¹ãƒ¬ãƒƒãƒ‰ã‚’åé›†ã—ã¦ä¿å­˜ã—ã¾ã™ï¼ˆéåŒæœŸç‰ˆï¼‰ã€‚

        Parameters
        ----------
        thread_limit : Optional[int], default=None
            å„ãƒœãƒ¼ãƒ‰ã‹ã‚‰å–å¾—ã™ã‚‹ã‚¹ãƒ¬ãƒƒãƒ‰æ•°ã€‚Noneã®å ´åˆã¯åˆ¶é™ãªã—ã€‚

        Returns
        -------
        list[tuple[str, str]]
            ä¿å­˜ã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ã®ãƒªã‚¹ãƒˆ [(json_path, md_path), ...]
        """
        total_limit = self.TOTAL_LIMIT
        effective_target_dates = target_dates or target_dates_set(1)

        # å¯¾è±¡æ—¥ä»˜ã®ãƒ­ã‚°å‡ºåŠ›
        date_str = max(effective_target_dates).strftime("%Y-%m-%d")
        log_processing_start(self.logger, date_str)

        # HTTPã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã®åˆæœŸåŒ–ã‚’ç¢ºèª
        if self.http_client is None:
            await self.setup_http_client()

        candidate_threads: list[Thread] = []
        selected_threads: list[Thread] = []
        dedup_tracker = self._load_existing_titles()

        try:
            # å„ãƒœãƒ¼ãƒ‰ã‹ã‚‰ã‚¹ãƒ¬ãƒƒãƒ‰ã‚’å–å¾—
            for board in self.target_boards:
                try:
                    self.logger.info(f"ãƒœãƒ¼ãƒ‰ /{board}/ ã‹ã‚‰ã®ã‚¹ãƒ¬ãƒƒãƒ‰å–å¾—ã‚’é–‹å§‹ã—ã¾ã™...")
                    threads = await self._retrieve_ai_threads(
                        board,
                        thread_limit,
                        dedup_tracker,
                        effective_target_dates,
                    )
                    self.logger.info(f"ãƒœãƒ¼ãƒ‰ /{board}/ ã‹ã‚‰ {len(threads)} ä»¶ã®ã‚¹ãƒ¬ãƒƒãƒ‰ã‚’å–å¾—ã—ã¾ã—ãŸ")
                    candidate_threads.extend(threads)

                    # APIãƒªã‚¯ã‚¨ã‚¹ãƒˆé–“ã®é…å»¶
                    await asyncio.sleep(self.request_delay)

                except Exception as e:
                    self.logger.error(f"Error processing board /{board}/: {str(e)}")

            self.logger.info(f"åˆè¨ˆ {len(candidate_threads)} ä»¶ã®ã‚¹ãƒ¬ãƒƒãƒ‰å€™è£œã‚’å–å¾—ã—ã¾ã—ãŸ")

            # æ—¥ä»˜ã”ã¨ã«ã‚°ãƒ«ãƒ¼ãƒ—åŒ–ã—ã¦å„æ—¥ç‹¬ç«‹ã§ä¸Šä½15ä»¶ã‚’é¸æŠ
            threads_by_date = {}
            for thread in candidate_threads:
                thread_date = datetime.fromtimestamp(thread.timestamp).date()
                if thread_date not in threads_by_date:
                    threads_by_date[thread_date] = []
                threads_by_date[thread_date].append(thread)

            # å„æ—¥ç‹¬ç«‹ã§ä¸Šä½15ä»¶ã‚’é¸æŠã—ã¦çµåˆ
            selected_threads = []
            for target_date in sorted(effective_target_dates):
                if target_date in threads_by_date:
                    date_threads = threads_by_date[target_date]
                    if len(date_threads) <= total_limit:
                        selected_threads.extend(date_threads)
                    else:

                        def sort_key(thread: Thread):
                            created = datetime.fromtimestamp(thread.timestamp)
                            return (thread.popularity_score, created)

                        sorted_threads = sorted(date_threads, key=sort_key, reverse=True)
                        selected_threads.extend(sorted_threads[:total_limit])

            # æ—¢å­˜/æ–°è¦ã‚¹ãƒ¬ãƒƒãƒ‰æ•°ã‚’ã‚«ã‚¦ãƒ³ãƒˆ
            existing_count = 0  # æ—¢å­˜ã‚¹ãƒ¬ãƒƒãƒ‰æ•°ï¼ˆç°¡ç•¥åŒ–ï¼‰
            new_count = len(selected_threads)  # æ–°è¦ã‚¹ãƒ¬ãƒƒãƒ‰æ•°

            # ã‚¹ãƒ¬ãƒƒãƒ‰æƒ…å ±ã‚’è¡¨ç¤º
            log_article_counts(self.logger, existing_count, new_count)

            if selected_threads:
                log_summary_candidates(self.logger, selected_threads, "popularity_score")

                # è¦ç´„ç”Ÿæˆï¼ˆä¸¦åˆ—åŒ–ï¼‰
                log_summarization_start(self.logger)

                async def summarize_with_progress(idx: int, thread: Thread) -> None:
                    await self._summarize_thread(thread)
                    log_summarization_progress(self.logger, idx, len(selected_threads), thread.title)

                await asyncio.gather(
                    *[summarize_with_progress(idx, thread) for idx, thread in enumerate(selected_threads, 1)]
                )

            # è¦ç´„ã‚’ä¿å­˜
            saved_files: list[tuple[str, str]] = []
            if selected_threads:
                saved_files = await self._store_summaries(selected_threads, effective_target_dates)

                # å‡¦ç†å®Œäº†ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸
                if saved_files:
                    self.logger.info(f"\nğŸ’¾ {len(saved_files)}æ—¥åˆ†ã®ãƒ‡ãƒ¼ã‚¿ã‚’ä¿å­˜å®Œäº†")
                    for json_path, md_path in saved_files:
                        log_storage_complete(self.logger, json_path, md_path)
                else:
                    self.logger.info("\nä¿å­˜ã™ã‚‹ã‚¹ãƒ¬ãƒƒãƒ‰ãŒã‚ã‚Šã¾ã›ã‚“")
            else:
                log_no_new_articles(self.logger)

            return saved_files

        finally:
            # ã‚°ãƒ­ãƒ¼ãƒãƒ«ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆãªã®ã§ã‚¯ãƒ­ãƒ¼ã‚ºä¸è¦
            pass

    async def _retrieve_ai_threads(
        self,
        board: str,
        limit: int | None,
        dedup_tracker: DedupTracker,
        target_dates: list[date],
    ) -> list[Thread]:
        """
        ç‰¹å®šã®ãƒœãƒ¼ãƒ‰ã‹ã‚‰AIé–¢é€£ã‚¹ãƒ¬ãƒƒãƒ‰ã‚’å–å¾—ã—ã¾ã™ã€‚

        Parameters
        ----------
        board : str
            ãƒœãƒ¼ãƒ‰åã€‚
        limit : Optional[int]
            å–å¾—ã™ã‚‹ã‚¹ãƒ¬ãƒƒãƒ‰æ•°ã€‚Noneã®å ´åˆã¯åˆ¶é™ãªã—ã€‚
        target_dates : list[date]
            ä¿å­˜å¯¾è±¡ã¨ã™ã‚‹æ—¥ä»˜é›†åˆã€‚

        Returns
        -------
        List[Thread]
            å–å¾—ã—ãŸã‚¹ãƒ¬ãƒƒãƒ‰ã®ãƒªã‚¹ãƒˆã€‚
        """
        # ã‚«ã‚¿ãƒ­ã‚°ã®å–å¾—ï¼ˆã™ã¹ã¦ã®ã‚¹ãƒ¬ãƒƒãƒ‰ã®ãƒªã‚¹ãƒˆï¼‰
        catalog_url = f"https://a.4cdn.org/{board}/catalog.json"
        response = await self.http_client.get(catalog_url)
        catalog_data = response.json()

        # AIé–¢é€£ã®ã‚¹ãƒ¬ãƒƒãƒ‰ã‚’ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
        ai_threads = []
        for page in catalog_data:
            for thread in page.get("threads", []):
                # ã‚¹ãƒ¬ãƒƒãƒ‰ã®ã‚¿ã‚¤ãƒˆãƒ«ï¼ˆsubjectï¼‰ã¨ã‚³ãƒ¡ãƒ³ãƒˆï¼ˆcomï¼‰ã‚’ç¢ºèª
                subject = thread.get("sub", "").lower()
                comment = thread.get("com", "").lower()

                # HTMLã‚¿ã‚°ã‚’é™¤å»
                if comment:
                    comment = re.sub(r"<[^>]*>", "", comment)

                # AIã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãŒå«ã¾ã‚Œã¦ã„ã‚‹ã‹ãƒã‚§ãƒƒã‚¯
                is_ai_related = any(keyword in subject or keyword in comment for keyword in self.ai_keywords)

                if is_ai_related:
                    thread_id = thread.get("no")
                    timestamp_raw = thread.get("time")
                    thread_created = (
                        datetime.fromtimestamp(int(timestamp_raw), tz=timezone.utc) if timestamp_raw else None
                    )

                    title = thread.get("sub", f"Untitled Thread {thread_id}")

                    is_dup, normalized = dedup_tracker.is_duplicate(title)
                    if is_dup:
                        original = dedup_tracker.get_original_title(normalized) or title
                        self.logger.info(
                            "é‡è¤‡ã‚¹ãƒ¬ãƒƒãƒ‰ã‚’ã‚¹ã‚­ãƒƒãƒ—: '%s' (åˆå‡º: '%s')",
                            title,
                            original,
                        )
                        continue

                    last_modified_raw = thread.get("last_modified")
                    last_modified_dt = (
                        datetime.fromtimestamp(int(last_modified_raw), tz=timezone.utc)
                        if last_modified_raw
                        else thread_created
                    )

                    if last_modified_dt and not is_within_target_dates(last_modified_dt, target_dates):
                        self.logger.debug(
                            "æ›´æ–°æ—¥æ™‚ãŒå¯¾è±¡å¤–ã®ãŸã‚ã‚¹ã‚­ãƒƒãƒ—: /%s/ %s (%s)",
                            board,
                            title,
                            last_modified_dt,
                        )
                        continue

                    # ã‚¹ãƒ¬ãƒƒãƒ‰ã®æŠ•ç¨¿ã‚’å–å¾—
                    thread_data = await self._retrieve_thread_posts(board, thread_id)

                    if not thread_data:
                        self.logger.debug(
                            "æŠ•ç¨¿ãŒå–å¾—ã§ããªã‹ã£ãŸãŸã‚ã‚¹ã‚­ãƒƒãƒ—: /%s/ %s",
                            board,
                            title,
                        )
                        continue

                    latest_post_ts = max(
                        (int(post.get("time")) for post in thread_data if post.get("time") is not None),
                        default=None,
                    )

                    latest_post_at = (
                        datetime.fromtimestamp(latest_post_ts, tz=timezone.utc) if latest_post_ts is not None else None
                    )

                    effective_dt = latest_post_at or thread_created
                    if not effective_dt or not is_within_target_dates(effective_dt, target_dates):
                        self.logger.debug(
                            "æœ€æ–°æŠ•ç¨¿æ—¥æ™‚ãŒå¯¾è±¡å¤–ã®ãŸã‚ã‚¹ã‚­ãƒƒãƒ—: /%s/ %s (%s)",
                            board,
                            title,
                            effective_dt,
                        )
                        continue

                    if effective_dt is not None:
                        if effective_dt.tzinfo is None:
                            effective_dt = effective_dt.replace(tzinfo=timezone.utc)
                        effective_utc = effective_dt.astimezone(timezone.utc)
                        timestamp_value = int(effective_utc.timestamp())
                    else:
                        # ã“ã‚Œã¯355è¡Œç›®ã®æ¤œè¨¼ã«ã‚ˆã‚Šç†è«–çš„ã«ã¯ç™ºç”Ÿã—ãªã„ãŒã€é˜²å¾¡çš„ã«å‡¦ç†
                        self.logger.warning(
                            "ã‚¹ãƒ¬ãƒƒãƒ‰ %s ã®æœ‰åŠ¹æ—¥æ™‚ã¨ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ãŒä¸¡æ–¹ã¨ã‚‚å–å¾—ã§ãã¾ã›ã‚“ã§ã—ãŸ",
                            thread_id,
                        )
                        continue

                    # ã‚¹ãƒ¬ãƒƒãƒ‰ã®URLã‚’æ§‹ç¯‰
                    thread_url = f"https://boards.4chan.org/{board}/thread/{thread_id}"

                    popularity_score = self._calculate_popularity(
                        thread_metadata=thread,
                        posts=thread_data,
                    )

                    dedup_tracker.add(title)

                    ai_threads.append(
                        Thread(
                            thread_id=thread_id,
                            title=title,
                            url=thread_url,
                            board=board,
                            posts=thread_data,
                            timestamp=timestamp_value,
                            popularity_score=popularity_score,
                        )
                    )

                    # æŒ‡å®šã•ã‚ŒãŸæ•°ã®ã‚¹ãƒ¬ãƒƒãƒ‰ã‚’å–å¾—ã—ãŸã‚‰çµ‚äº†
                    if limit is not None and len(ai_threads) >= limit:
                        break

            if limit is not None and len(ai_threads) >= limit:
                break

        return ai_threads

    async def _retrieve_thread_posts(self, board: str, thread_id: int) -> list[dict[str, Any]]:
        """
        ã‚¹ãƒ¬ãƒƒãƒ‰ã®æŠ•ç¨¿ã‚’å–å¾—ã—ã¾ã™ã€‚

        Parameters
        ----------
        board : str
            ãƒœãƒ¼ãƒ‰åã€‚
        thread_id : int
            ã‚¹ãƒ¬ãƒƒãƒ‰IDã€‚

        Returns
        -------
        List[Dict[str, Any]]
            æŠ•ç¨¿ã®ãƒªã‚¹ãƒˆã€‚
        """
        thread_url = f"https://a.4cdn.org/{board}/thread/{thread_id}.json"
        try:
            response = await self.http_client.get(thread_url)
            thread_data = response.json()
            posts = thread_data.get("posts", [])

            # APIãƒªã‚¯ã‚¨ã‚¹ãƒˆé–“ã®é…å»¶
            await asyncio.sleep(self.request_delay)

            return posts
        except Exception as e:
            self.logger.error(f"ã‚¹ãƒ¬ãƒƒãƒ‰ã®å–å¾—ã«å¤±æ•—ã—ã¾ã—ãŸ: {str(e)}")
            return []

    def _load_existing_titles(self) -> DedupTracker:
        tracker = DedupTracker()
        try:
            content = self.storage.load_markdown("", datetime.now())
            if content:
                for match in re.finditer(r"^### \[(.+?)\]", content, re.MULTILINE):
                    tracker.add(match.group(1))
        except Exception as exc:
            self.logger.debug(f"æ—¢å­˜ã‚¹ãƒ¬ãƒƒãƒ‰ã‚¿ã‚¤ãƒˆãƒ«ã®èª­ã¿è¾¼ã¿ã«å¤±æ•—ã—ã¾ã—ãŸ: {exc}")
        return tracker

    def _calculate_popularity(self, thread_metadata: dict[str, Any], posts: list[dict[str, Any]]) -> float:
        replies = thread_metadata.get("replies", 0) or 0
        images = thread_metadata.get("images", 0) or 0
        bumps = thread_metadata.get("bumps", 0) or 0

        recency_bonus = 0.0
        try:
            last_modified = thread_metadata.get("last_modified") or thread_metadata.get("time", 0)
            if last_modified:
                now = datetime.now()
                modified = datetime.fromtimestamp(last_modified)
                hours = (now - modified).total_seconds() / 3600
                recency_bonus = 24 / max(1.0, hours)
        except Exception as exc:
            self.logger.debug("Failed to calculate recency bonus: %s", exc)

        return float(replies + images * 2 + bumps + len(posts) + recency_bonus)

    def _select_top_threads(self, threads: list[Thread], limit: int) -> list[Thread]:
        if not threads:
            return []

        if len(threads) <= limit:
            return threads

        def sort_key(thread: Thread):
            created = datetime.fromtimestamp(thread.timestamp)
            return (thread.popularity_score, created)

        sorted_threads = sorted(threads, key=sort_key, reverse=True)
        return sorted_threads[:limit]

    async def _summarize_thread(self, thread: Thread) -> None:
        """
        ã‚¹ãƒ¬ãƒƒãƒ‰ã‚’è¦ç´„ã—ã¾ã™ï¼ˆä¸¦åˆ—åŒ–å¯¾å¿œã®éåŒæœŸç‰ˆï¼‰ã€‚

        Parameters
        ----------
        thread : Thread
            è¦ç´„ã™ã‚‹ã‚¹ãƒ¬ãƒƒãƒ‰ã€‚
        """
        # ã‚¹ãƒ¬ãƒƒãƒ‰ã®ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’æŠ½å‡ºï¼ˆæœ€åˆã®æŠ•ç¨¿ã¨ã€æœ€ã‚‚åå¿œã®ã‚ã‚‹æŠ•ç¨¿ã‚’å«ã‚€ï¼‰
        thread_content = ""

        # ã‚¹ãƒ¬ãƒƒãƒ‰ã®ã‚¿ã‚¤ãƒˆãƒ«ã‚’è¿½åŠ 
        thread_content += f"Title: {thread.title}\n\n"

        # ã‚ªãƒªã‚¸ãƒŠãƒ«ãƒã‚¹ãƒˆï¼ˆOPï¼‰ã‚’è¿½åŠ 
        if thread.posts and len(thread.posts) > 0:
            op = thread.posts[0]
            op_text = op.get("com", "")
            if op_text:
                # HTMLã‚¿ã‚°ã‚’é™¤å»
                op_text = re.sub(r"<[^>]*>", " ", op_text)
                thread_content += f"OP: {op_text}\n\n"

        # è¿”ä¿¡ã‚’è¿½åŠ ï¼ˆæœ€å¤§5ä»¶ï¼‰
        replies = thread.posts[1:6] if len(thread.posts) > 1 else []
        for i, reply in enumerate(replies):
            reply_text = reply.get("com", "")
            if reply_text:
                # HTMLã‚¿ã‚°ã‚’é™¤å»
                reply_text = re.sub(r"<[^>]*>", " ", reply_text)
                thread_content += f"Reply {i + 1}: {reply_text}\n\n"

        prompt = f"""
        Summarize the following 4chan thread in Japanese.

        Board: /{thread.board}/
        {thread_content}

        Please provide the summary in the following format (in Japanese):
        1. ã‚¹ãƒ¬ãƒƒãƒ‰ã®ä¸»ãªå†…å®¹ï¼ˆ1-2æ–‡ï¼‰
        2. è­°è«–ã®ä¸»è¦ãƒã‚¤ãƒ³ãƒˆï¼ˆç®‡æ¡æ›¸ã3-5ç‚¹ï¼‰
        3. ã‚¹ãƒ¬ãƒƒãƒ‰ã®å…¨ä½“çš„ãªè«–èª¿

        Note: Mitigate offensive content and hate speech, and focus on the main technical discussions.
        """

        system_instruction = """
        You are an assistant that summarizes 4chan threads.
        Objectively analyze the posted content and provide a summary focused on technical discussions and information.
        Neutralize excessive aggression, hate speech, and discriminatory content, and extract only beneficial information.
        IMPORTANT: Always respond in Japanese. Prioritize AI and technology-related information.
        """

        try:
            summary = await self.gpt_client.generate_async(
                prompt=prompt,
                system_instruction=system_instruction,
                temperature=0.3,
                max_tokens=1000,
                service_name=self.service_name,
            )
            thread.summary = summary
        except Exception as e:
            self.logger.error(f"è¦ç´„ã®ç”Ÿæˆä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}")
            thread.summary = f"è¦ç´„ã®ç”Ÿæˆä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}"

    async def _store_summaries(self, threads: list[Thread], target_dates: list[date]) -> list[tuple[str, str]]:
        if not threads:
            self.logger.info("ä¿å­˜ã™ã‚‹ã‚¹ãƒ¬ãƒƒãƒ‰ãŒã‚ã‚Šã¾ã›ã‚“")
            return []

        default_date = max(target_dates) if target_dates else datetime.now().date()
        records = self._serialize_threads(threads)
        records_by_date = group_records_by_date(records, default_date=default_date)

        saved_files = await store_daily_snapshots(
            records_by_date,
            load_existing=self._load_existing_threads,
            save_json=self.save_json,
            save_markdown=self.save_markdown,
            render_markdown=self._render_markdown,
            key=lambda item: item.get("thread_id"),
            sort_key=self._thread_sort_key,
            limit=self.TOTAL_LIMIT,
            logger=self.logger,
        )

        return saved_files

    def _serialize_threads(self, threads: list[Thread]) -> list[dict]:
        records: list[dict] = []
        for thread in threads:
            published = datetime.fromtimestamp(thread.timestamp, tz=timezone.utc)
            records.append(
                {
                    "thread_id": thread.thread_id,
                    "title": thread.title,
                    "url": thread.url,
                    "timestamp": thread.timestamp,
                    "summary": thread.summary,
                    "popularity_score": thread.popularity_score,
                    "board": thread.board,
                    "published_at": published.isoformat(),
                }
            )
        return records

    async def _load_existing_threads(self, target_date: datetime) -> list[dict]:
        date_str = target_date.strftime("%Y-%m-%d")
        filename_json = f"{date_str}.json"
        existing_json = await self.load_json(filename_json)
        if existing_json:
            if isinstance(existing_json, dict):
                flattened: list[dict] = []
                for board, items in existing_json.items():
                    for item in items:
                        flattened.append({"board": board, **item})
                return flattened
            return existing_json

        markdown = await self.storage.load(f"{date_str}.md")
        if not markdown:
            return []

        return self._parse_markdown(markdown)

    def _thread_sort_key(self, item: dict) -> tuple[float, datetime]:
        popularity = float(item.get("popularity_score", 0.0) or 0.0)
        published_raw = item.get("published_at")
        if published_raw:
            try:
                published = datetime.fromisoformat(published_raw)
            except ValueError:
                published = datetime.min.replace(tzinfo=timezone.utc)
        else:
            timestamp = item.get("timestamp")
            if timestamp:
                try:
                    published = datetime.fromtimestamp(int(timestamp), tz=timezone.utc)
                except Exception:
                    published = datetime.min.replace(tzinfo=timezone.utc)
            else:
                published = datetime.min.replace(tzinfo=timezone.utc)
        return (popularity, published)

    def _extract_thread_id_from_url(self, url: str) -> int:
        if not url:
            return 0

        cleaned = url.strip().split("#", 1)[0].split("?", 1)[0].rstrip("/")

        match = re.search(r"/thread/(\d+)", cleaned)
        if match:
            return int(match.group(1))

        for part in reversed(cleaned.split("/")):
            if part.isdigit():
                return int(part)

        return 0

    def _render_markdown(self, records: list[dict], today: datetime) -> str:
        content = f"# 4chan AIé–¢é€£ã‚¹ãƒ¬ãƒƒãƒ‰ ({today.strftime('%Y-%m-%d')})\n\n"
        grouped: dict[str, list[dict]] = {}
        for record in records:
            board = record.get("board", "unknown")
            grouped.setdefault(board, []).append(record)

        for board, threads in grouped.items():
            content += f"## /{board}/\n\n"
            for thread in threads:
                title = thread.get("title") or f"ç„¡é¡Œã‚¹ãƒ¬ãƒƒãƒ‰ #{thread.get('thread_id')}"
                content += f"### [{title}]({thread.get('url')})\n\n"
                published_raw = thread.get("published_at")
                if published_raw:
                    try:
                        published_dt = datetime.fromisoformat(published_raw)
                        timestamp = int(published_dt.timestamp())
                    except ValueError:
                        timestamp = int(thread.get("timestamp", 0) or 0)
                else:
                    timestamp = int(thread.get("timestamp", 0) or 0)
                content += f"ä½œæˆæ—¥æ™‚: <t:{timestamp}:F>\n\n"
                content += f"**è¦ç´„**:\n{thread.get('summary', '')}\n\n"
                content += "---\n\n"
        return content

    def _parse_markdown(self, markdown: str) -> list[dict]:
        records: list[dict] = []
        board_pattern = re.compile(r"^##\s+/([^/]+)/$", re.MULTILINE)
        thread_pattern = re.compile(
            r"### \[(?P<title>.+?)\]\((?P<url>[^\)]+)\)\n\n"
            r"ä½œæˆæ—¥æ™‚: <t:(?P<timestamp>\d+):F>\n\n"
            r"\*\*è¦ç´„\*\*:\n(?P<summary>.*?)(?:\n\n)?---",
            re.DOTALL,
        )

        sections = list(board_pattern.finditer(markdown))
        for idx, match in enumerate(sections):
            start = match.end()
            end = sections[idx + 1].start() if idx + 1 < len(sections) else len(markdown)
            block = markdown[start:end]
            board = match.group(1).strip()

            for thread_match in thread_pattern.finditer(block + "---"):
                title = thread_match.group("title")
                url = thread_match.group("url")
                summary = thread_match.group("summary").strip()
                timestamp = int(thread_match.group("timestamp") or 0)
                thread_id = self._extract_thread_id_from_url(url)
                record = {
                    "thread_id": thread_id,
                    "title": title.strip(),
                    "url": url.strip(),
                    "timestamp": timestamp,
                    "summary": summary,
                    "popularity_score": 0.0,
                    "board": board,
                }

                if timestamp:
                    record["published_at"] = datetime.fromtimestamp(timestamp, tz=timezone.utc).isoformat()

                records.append(record)

        return records
