"""GitHubã®ãƒˆãƒ¬ãƒ³ãƒ‰ãƒªãƒã‚¸ãƒˆãƒªã‚’åé›†ã™ã‚‹ã‚µãƒ¼ãƒ“ã‚¹ã€‚"""

import re
import tomllib
from dataclasses import dataclass
from datetime import UTC, date, datetime, time
from pathlib import Path
from textwrap import dedent
from typing import Any

from bs4 import BeautifulSoup

from nook.common.base_service import BaseService
from nook.common.daily_snapshot import group_records_by_date, store_daily_snapshots
from nook.common.date_utils import target_dates_set
from nook.common.decorators import handle_errors
from nook.common.dedup import DedupTracker
from nook.common.exceptions import APIException
from nook.common.logging_utils import (
    log_article_counts,
    log_processing_start,
    log_summarization_progress,
    log_summarization_start,
    log_summary_candidates,
)


@dataclass
class Repository:
    """
    GitHubãƒªãƒã‚¸ãƒˆãƒªæƒ…å ±ã€‚

    Parameters
    ----------
    name : str
        ãƒªãƒã‚¸ãƒˆãƒªåã€‚
    description : str | None
        èª¬æ˜ã€‚
    link : str
        ãƒªãƒã‚¸ãƒˆãƒªã¸ã®ãƒªãƒ³ã‚¯ã€‚
    stars : int
        ã‚¹ã‚¿ãƒ¼æ•°ã€‚
    """

    name: str
    description: str | None
    link: str
    stars: int


class GithubTrending(BaseService):
    """
    GitHubã®ãƒˆãƒ¬ãƒ³ãƒ‰ãƒªãƒã‚¸ãƒˆãƒªã‚’åé›†ã™ã‚‹ã‚¯ãƒ©ã‚¹ã€‚

    Parameters
    ----------
    storage_dir : str, default="data"
        ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®ãƒ‘ã‚¹ã€‚
    """

    def __init__(self, storage_dir: str = "data"):
        """
        GithubTrendingã‚’åˆæœŸåŒ–ã—ã¾ã™ã€‚

        Parameters
        ----------
        storage_dir : str, default="data"
            ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®ãƒ‘ã‚¹ã€‚
        """
        super().__init__("github_trending")
        self.base_url = "https://github.com/trending"
        self.http_client = None  # setup_http_clientã§åˆæœŸåŒ–

        # è¨€èªã®è¨­å®šã‚’èª­ã¿è¾¼ã‚€
        script_dir = Path(__file__).parent
        with open(script_dir / "languages.toml", "rb") as f:
            self.languages_config = tomllib.load(f)

    async def collect(
        self,
        limit: int = 5,
        *,
        target_dates: list[date] | None = None,
    ) -> list[tuple[str, str]]:
        """
        GitHubã®ãƒˆãƒ¬ãƒ³ãƒ‰ãƒªãƒã‚¸ãƒˆãƒªã‚’åé›†ã—ã¦ä¿å­˜ã—ã¾ã™ã€‚

        Parameters
        ----------
        limit : int, default=5
            å„è¨€èªã‹ã‚‰å–å¾—ã™ã‚‹ãƒªãƒã‚¸ãƒˆãƒªæ•°ã€‚

        Returns
        -------
        list[tuple[str, str]]
            ä¿å­˜ã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ã®ãƒªã‚¹ãƒˆ [(json_path, md_path), ...]
        """
        # HTTPã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã®åˆæœŸåŒ–ã‚’ç¢ºèª
        if self.http_client is None:
            await self.setup_http_client()

        effective_target_dates = target_dates if target_dates is not None else target_dates_set(1)

        # æ—¥ä»˜ã”ã¨ã«å‡¦ç†
        saved_files: list[tuple[str, str]] = []
        for target_date in sorted(effective_target_dates):
            date_str = target_date.strftime("%Y-%m-%d")

            # ãã®æ—¥ã®æ—¢å­˜ãƒªãƒã‚¸ãƒˆãƒªåã‚’å–å¾—
            existing_names_for_date = set()
            try:
                existing_repos = await self._load_existing_repositories_by_date(
                    datetime.combine(target_date, time.min)
                )
                existing_names_for_date = {repo.get("name", "") for repo in existing_repos}
            except Exception as e:
                self.logger.debug(
                    f"æ—¢å­˜ãƒªãƒã‚¸ãƒˆãƒªãƒ•ã‚¡ã‚¤ãƒ« {date_str}.json ã®èª­ã¿è¾¼ã¿ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}"
                )

            # é‡è¤‡ãƒˆãƒ©ãƒƒã‚«ãƒ¼ã‚’åˆæœŸåŒ–
            dedup_tracker = DedupTracker()
            for name in existing_names_for_date:
                dedup_tracker.add(name)

            all_repositories = []

            # è¨€èªæŒ‡å®šãªã—ã®ãƒªãƒã‚¸ãƒˆãƒªã‚’å–å¾—
            repositories = await self._retrieve_repositories("any", limit, dedup_tracker)
            all_repositories.append(("all", repositories))
            await self.rate_limit()  # ãƒ¬ãƒ¼ãƒˆåˆ¶é™ã‚’éµå®ˆ

            # ä¸€èˆ¬çš„ãªè¨€èªã®ãƒªãƒã‚¸ãƒˆãƒªã‚’å–å¾—
            for language in self.languages_config["general"]:
                repositories = await self._retrieve_repositories(language, limit, dedup_tracker)
                all_repositories.append((language, repositories))
                await self.rate_limit()  # ãƒ¬ãƒ¼ãƒˆåˆ¶é™ã‚’éµå®ˆ

            # ç‰¹å®šã®è¨€èªã®ãƒªãƒã‚¸ãƒˆãƒªã‚’å–å¾—
            for language in self.languages_config["specific"]:
                repositories = await self._retrieve_repositories(language, limit, dedup_tracker)
                all_repositories.append((language, repositories))
                await self.rate_limit()  # ãƒ¬ãƒ¼ãƒˆåˆ¶é™ã‚’éµå®ˆ

            # å…¨ãƒªãƒã‚¸ãƒˆãƒªã‚’ãƒ•ãƒ©ãƒƒãƒˆåŒ–
            all_repos_flat = []
            for _, repositories in all_repositories:
                for repo in repositories:
                    all_repos_flat.append(repo)

            # æ—¢å­˜/æ–°è¦ãƒªãƒã‚¸ãƒˆãƒªæ•°ã‚’ã‚«ã‚¦ãƒ³ãƒˆ
            existing_count = len(existing_names_for_date)

            # çœŸã«æ–°è¦ã®ãƒªãƒã‚¸ãƒˆãƒªã‚’ç¢ºèª
            truly_new_repositories = [
                repo for repo in all_repos_flat if repo.name not in existing_names_for_date
            ]

            # æ—¥ä»˜æƒ…å ±ã‚’å…ˆé ­ã«è¡¨ç¤º
            log_processing_start(self.logger, date_str)
            log_article_counts(self.logger, existing_count, len(truly_new_repositories))

            if truly_new_repositories:
                # ä¸Šä½15ä»¶ã‚’é¸æŠã—ã¦è¡¨ç¤º
                selected_repos = sorted(
                    truly_new_repositories, key=lambda x: x.stars, reverse=True
                )[:15]

                log_summary_candidates(self.logger, selected_repos, "stars")

                # è¦ç´„ç”Ÿæˆ
                log_summarization_start(self.logger)

                # è¨€èªã”ã¨ã«å†ã‚°ãƒ«ãƒ¼ãƒ—åŒ–ã—ã¦ç¿»è¨³
                repos_by_language = {}
                for language, repositories in all_repositories:
                    repos_by_language[language] = [
                        repo for repo in repositories if repo in truly_new_repositories
                    ]

                repos_for_translation = [
                    (lang, repos) for lang, repos in repos_by_language.items() if repos
                ]

                translated_repos = await self._translate_repositories(
                    repos_for_translation,
                    progress_callback=lambda idx, total, name: log_summarization_progress(
                        self.logger, idx, total, name
                    ),
                )

                # ä¿å­˜å‡¦ç†
                json_path, md_path = await self._store_summaries_for_date(
                    translated_repos, target_date
                )
                self.logger.info(f"\n   ğŸ’¾ ä¿å­˜å®Œäº†: {json_path}, {md_path}")
                saved_files.append((json_path, md_path))
            else:
                self.logger.info("   â„¹ï¸  æ–°è¦ãƒªãƒã‚¸ãƒˆãƒªãŒã‚ã‚Šã¾ã›ã‚“")

        # å‡¦ç†å®Œäº†ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸
        if saved_files:
            self.logger.info(f"\nğŸ’¾ {len(saved_files)}æ—¥åˆ†ã®ãƒ‡ãƒ¼ã‚¿ã‚’ä¿å­˜å®Œäº†")
            for json_path, md_path in saved_files:
                self.logger.info(f"   ğŸ’¾ ä¿å­˜å®Œäº†: {json_path}, {md_path}")
        else:
            self.logger.info("\nä¿å­˜ã™ã‚‹ãƒªãƒã‚¸ãƒˆãƒªãŒã‚ã‚Šã¾ã›ã‚“")

        return saved_files

    @handle_errors(retries=3)
    async def _retrieve_repositories(
        self, language: str, limit: int, dedup_tracker: DedupTracker
    ) -> list[Repository]:
        """
        ç‰¹å®šã®è¨€èªã®ãƒˆãƒ¬ãƒ³ãƒ‰ãƒªãƒã‚¸ãƒˆãƒªã‚’å–å¾—ã—ã¾ã™ã€‚

        Parameters
        ----------
        language : str
            è¨€èªåï¼ˆç©ºæ–‡å­—åˆ—ã®å ´åˆã¯ã™ã¹ã¦ã®è¨€èªï¼‰ã€‚
        limit : int
            å–å¾—ã™ã‚‹ãƒªãƒã‚¸ãƒˆãƒªæ•°ã€‚

        Returns
        -------
        List[Repository]
            å–å¾—ã—ãŸãƒªãƒã‚¸ãƒˆãƒªã®ãƒªã‚¹ãƒˆã€‚
        """
        url = self.base_url
        if language:
            url += f"/{language}"

        try:
            response = await self.http_client.get(url)
            soup = BeautifulSoup(response.text, "html.parser")

            repositories = []
            repo_elements = soup.select("article.Box-row")

            for repo_element in repo_elements:
                # ãƒªãƒã‚¸ãƒˆãƒªåã‚’å–å¾—
                name_element = repo_element.select_one("h2 a")
                if not name_element:
                    continue

                name = name_element.text.strip().replace("\n", "").replace(" ", "")
                link = f"https://github.com{name_element['href']}"

                is_dup, normalized = dedup_tracker.is_duplicate(name)
                if is_dup:
                    original = dedup_tracker.get_original_title(normalized) or name
                    self.logger.info(
                        "é‡è¤‡ãƒªãƒã‚¸ãƒˆãƒªã‚’ã‚¹ã‚­ãƒƒãƒ—: '%s' (åˆå‡º: '%s')",
                        name,
                        original,
                    )
                    continue

                # èª¬æ˜ã‚’å–å¾—
                description_element = repo_element.select_one("p")
                description = description_element.text.strip() if description_element else None

                # ã‚¹ã‚¿ãƒ¼æ•°ã‚’å–å¾—
                stars_element = repo_element.select_one("a.Link--muted")
                stars_text = stars_element.text.strip() if stars_element else "0"
                stars = (
                    int(stars_text.replace(",", "")) if stars_text.replace(",", "").isdigit() else 0
                )

                repository = Repository(name=name, description=description, link=link, stars=stars)

                repositories.append(repository)
                dedup_tracker.add(name)

                if len(repositories) >= limit:
                    break

            return repositories

        except Exception as e:
            self.logger.error(f"Error retrieving repositories for language {language}: {str(e)}")
            raise APIException(f"Failed to retrieve repositories for {language}") from e

    def _load_existing_repositories(self) -> DedupTracker:
        tracker = DedupTracker()
        try:
            today = datetime.now().strftime("%Y-%m-%d")
            file_path = Path(self.storage.base_dir) / f"{today}.md"
            if file_path.exists():
                content = file_path.read_text(encoding="utf-8")
                for match in re.finditer(r"^### \[(.+?)\]", content, re.MULTILINE):
                    tracker.add(match.group(1))
        except Exception as exc:
            self.logger.debug(f"æ—¢å­˜ãƒªãƒã‚¸ãƒˆãƒªã®èª­ã¿è¾¼ã¿ã«å¤±æ•—ã—ã¾ã—ãŸ: {exc}")
        return tracker

    async def _translate_repositories(
        self,
        repositories_by_language: list[tuple[str, list[Repository]]],
        *,
        progress_callback=None,
    ) -> list[tuple[str, list[Repository]]]:
        """
        ãƒªãƒã‚¸ãƒˆãƒªã®èª¬æ˜ã‚’æ—¥æœ¬èªã«ç¿»è¨³ã—ã¾ã™ã€‚

        Parameters
        ----------
        repositories_by_language : List[tuple[str, List[Repository]]]
            è¨€èªã”ã¨ã®ãƒªãƒã‚¸ãƒˆãƒªãƒªã‚¹ãƒˆã€‚
        progress_callback : callable, optional
            é€²æ—è¡¨ç¤ºç”¨ã®ã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯é–¢æ•°

        Returns
        -------
        List[tuple[str, List[Repository]]]
            ç¿»è¨³ã•ã‚ŒãŸãƒªãƒã‚¸ãƒˆãƒªãƒªã‚¹ãƒˆã€‚
        """
        # é€²æ—ã‚«ã‚¦ãƒ³ã‚¿ãƒ¼ã‚’åˆæœŸåŒ–
        total_repos = sum(len(repos) for _, repos in repositories_by_language)
        current_idx = 0

        try:
            for _language, repositories in repositories_by_language:
                for repo in repositories:
                    if repo.description:
                        prompt = dedent(
                            f"""
                            ä»¥ä¸‹ã®GitHubãƒªãƒã‚¸ãƒˆãƒªã®èª¬æ˜æ–‡ã‚’æ—¥æœ¬èªã§è¦ç´„ã—ã¦ãã ã•ã„ã€‚
                            åˆ¶ç´„:
                            - æ¦‚è¦ã¯åˆè¨ˆã§300æ–‡å­—ä»¥å†…ã‚’ç›®å®‰ã«ã¾ã¨ã‚ã‚‹ã“ã¨ã€‚
                            - ç®‡æ¡æ›¸ãã‚’è¿½åŠ ã—ã€3é …ç›®ã¨ã™ã‚‹ã“ã¨ã€‚
                            - æ–°ã—ã„æƒ…å ±ã‚’æ¨æ¸¬ã›ãšã€åŸæ–‡ã®å†…å®¹ã«åŸºã¥ã„ã¦èª¬æ˜ã™ã‚‹ã“ã¨ã€‚
                            - å‡ºåŠ›å½¢å¼ï¼š
                              æ¦‚è¦: <æ¦‚è¦>
                              ä¸»ãªãƒã‚¤ãƒ³ãƒˆ:
                              - <ãƒã‚¤ãƒ³ãƒˆ1>
                              - <ãƒã‚¤ãƒ³ãƒˆ2>

                            ãƒªãƒã‚¸ãƒˆãƒªå: {repo.name}
                            åŸæ–‡èª¬æ˜: {repo.description}
                            """
                        )
                        try:
                            repo.description = await self.gpt_client.generate_async(
                                prompt=prompt,
                                temperature=0.3,
                                max_tokens=300,
                            )
                            if repo.description:
                                repo.description = repo.description.strip()
                            await self.rate_limit()  # APIå‘¼ã³å‡ºã—å¾Œã®ãƒ¬ãƒ¼ãƒˆåˆ¶é™

                            # é€²æ—è¡¨ç¤º
                            current_idx += 1
                            if progress_callback:
                                progress_callback(current_idx, total_repos, repo.name)

                        except Exception as e:
                            self.logger.error(
                                f"Error translating description for {repo.name}: {str(e)}"
                            )

        except Exception as e:
            self.logger.error(f"Error in translation process: {str(e)}")

        return repositories_by_language

    async def _store_summaries_for_date(
        self,
        repositories_by_language: list[tuple[str, list[Repository]]],
        target_date: date,
    ) -> tuple[str, str]:
        """
        ç‰¹å®šã®æ—¥ä»˜ã®ãƒªãƒã‚¸ãƒˆãƒªæƒ…å ±ã‚’ä¿å­˜ã—ã¾ã™ã€‚

        Parameters
        ----------
        repositories_by_language : List[tuple[str, List[Repository]]]
            è¨€èªã”ã¨ã®ãƒªãƒã‚¸ãƒˆãƒªãƒªã‚¹ãƒˆã€‚
        target_date : date
            å¯¾è±¡æ—¥ä»˜ã€‚

        Returns
        -------
        tuple[str, str]
            ä¿å­˜ã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ (json_path, md_path)
        """
        if not repositories_by_language:
            raise ValueError("ä¿å­˜ã™ã‚‹ãƒªãƒã‚¸ãƒˆãƒªãŒã‚ã‚Šã¾ã›ã‚“")

        records = self._serialize_repositories(repositories_by_language, target_date)
        records_by_date = group_records_by_date(records, default_date=target_date)

        saved_files = await store_daily_snapshots(
            records_by_date,
            load_existing=self._load_existing_repositories_by_date,
            save_json=self.save_json,
            save_markdown=self.save_markdown,
            render_markdown=self._render_markdown,
            key=lambda item: item.get("name", ""),
            sort_key=self._repository_sort_key,
            limit=None,
            logger=None,  # æ—¥ä»˜æƒ…å ±ã®äºŒé‡è¡¨ç¤ºã‚’é˜²ã
        )

        if saved_files and len(saved_files) > 0:
            return saved_files[0]  # æœ€åˆã®ï¼ˆå”¯ä¸€ã®ï¼‰ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ã‚’è¿”ã™
        else:
            raise ValueError("ä¿å­˜ã«å¤±æ•—ã—ã¾ã—ãŸ")

    async def _store_summaries(
        self,
        repositories_by_language: list[tuple[str, list[Repository]]],
        limit_per_language: int | None,
        target_dates: list[date],
    ) -> list[tuple[str, str]]:
        """
        ãƒªãƒã‚¸ãƒˆãƒªæƒ…å ±ã‚’ä¿å­˜ã—ã¾ã™ã€‚

        Parameters
        ----------
        repositories_by_language : List[tuple[str, List[Repository]]]
            è¨€èªã”ã¨ã®ãƒªãƒã‚¸ãƒˆãƒªãƒªã‚¹ãƒˆã€‚
        limit_per_language : int | None
            å„è¨€èªã®æœ€å¤§ä»¶æ•°ã€‚None ã®å ´åˆã¯å…¥åŠ›ãƒ‡ãƒ¼ã‚¿ã®ä»¶æ•°ã‚’åˆ©ç”¨ã€‚

        Returns
        -------
        list[tuple[str, str]]
            ä¿å­˜ã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ã®ãƒªã‚¹ãƒˆ [(json_path, md_path), ...]
        """
        if not repositories_by_language:
            self.logger.info("ä¿å­˜ã™ã‚‹ãƒªãƒã‚¸ãƒˆãƒªãŒã‚ã‚Šã¾ã›ã‚“")
            return []

        default_date = max(target_dates) if target_dates else datetime.now().date()
        records = self._serialize_repositories(repositories_by_language, default_date)
        records_by_date = group_records_by_date(records, default_date=default_date)

        saved_files = await store_daily_snapshots(
            records_by_date,
            load_existing=self._load_existing_repositories_by_date,
            save_json=self.save_json,
            save_markdown=self.save_markdown,
            render_markdown=self._render_markdown,
            key=lambda item: item.get("name", ""),
            sort_key=self._repository_sort_key,
            limit=None,
            logger=self.logger,
        )

        return saved_files

    def _serialize_repositories(
        self,
        repositories_by_language: list[tuple[str, list[Repository]]],
        default_date: date,
    ) -> list[dict[str, Any]]:
        serialized: list[dict[str, Any]] = []
        base_dt = datetime.combine(default_date, time.min, tzinfo=UTC)
        now_iso = base_dt.isoformat()
        for language, repositories in repositories_by_language:
            for repo in repositories:
                serialized.append(
                    {
                        "language": language,
                        "name": repo.name,
                        "description": repo.description,
                        "link": repo.link,
                        "stars": repo.stars,
                        "published_at": now_iso,
                    }
                )
        return serialized

    async def _load_existing_repositories_by_date(
        self, target_date: datetime
    ) -> list[dict[str, Any]]:
        date_str = target_date.strftime("%Y-%m-%d")
        filename_json = f"{date_str}.json"
        existing_json = await self.load_json(filename_json)
        if existing_json:
            if isinstance(existing_json, dict):
                flattened: list[dict[str, Any]] = []
                for language, repos in existing_json.items():
                    for repo in repos:
                        flattened.append({"language": language, **repo})
                return flattened
            return existing_json

        markdown_content = await self.storage.load(f"{date_str}.md")
        if not markdown_content:
            return []

        return self._parse_markdown(markdown_content)

    def _repository_sort_key(self, item: dict[str, Any]) -> tuple[int, datetime]:
        stars = int(item.get("stars", 0) or 0)
        published_raw = item.get("published_at")
        if published_raw:
            try:
                published = datetime.fromisoformat(published_raw)
            except ValueError:
                published = datetime.min.replace(tzinfo=UTC)
        else:
            published = datetime.min.replace(tzinfo=UTC)
        return (stars, published)

    def _render_markdown(self, records: list[dict[str, Any]], today: datetime) -> str:
        content = f"# GitHub ãƒˆãƒ¬ãƒ³ãƒ‰ãƒªãƒã‚¸ãƒˆãƒª ({today.strftime('%Y-%m-%d')})\n\n"

        grouped: dict[str, list[dict[str, Any]]] = {}
        for record in records:
            language = record.get("language", "all")
            grouped.setdefault(language, []).append(record)

        for language, repositories in grouped.items():
            if not repositories:
                continue

            language_display = language if language != "all" else "ã™ã¹ã¦ã®è¨€èª"
            content += f"## {language_display.capitalize()}\n\n"

            for repo in repositories:
                content += f"### [{repo['name']}]({repo.get('link')})\n\n"

                description = repo.get("description")
                if description:
                    content += f"{description}\n\n"

                content += f"â­ ã‚¹ã‚¿ãƒ¼æ•°: {repo.get('stars', 0)}\n\n"
                content += "---\n\n"

        return content

    def _parse_markdown(self, content: str) -> list[dict[str, Any]]:
        records: list[dict[str, Any]] = []
        language_pattern = re.compile(r"^##\s+(.+)$", re.MULTILINE)
        repo_pattern = re.compile(
            r"### \[(?P<name>.+?)\]\((?P<link>[^\)]+)\)\n\n"
            r"(?P<description>.*?)(?:\n\n)?â­ ã‚¹ã‚¿ãƒ¼æ•°: (?P<stars>[0-9,]+)",
            re.DOTALL,
        )

        sections = list(language_pattern.finditer(content))
        for idx, match in enumerate(sections):
            start = match.end()
            end = sections[idx + 1].start() if idx + 1 < len(sections) else len(content)
            section_content = content[start:end]

            language_header = match.group(1).strip()
            language_key = (
                "all" if language_header.lower().startswith("ã™ã¹ã¦") else language_header.lower()
            )

            for repo_match in repo_pattern.finditer(section_content):
                name = repo_match.group("name").strip()
                link = repo_match.group("link").strip()
                description = repo_match.group("description").strip()
                stars_text = repo_match.group("stars")
                stars = int(stars_text.replace(",", "")) if stars_text else 0

                records.append(
                    {
                        "language": language_key,
                        "name": name,
                        "link": link,
                        "description": description,
                        "stars": stars,
                    }
                )

        return records
