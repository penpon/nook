"""arXivè«–æ–‡ã‚’åé›†ãƒ»è¦ç´„ã™ã‚‹ã‚µãƒ¼ãƒ“ã‚¹ã€‚"""

import asyncio
import re
from dataclasses import dataclass, field
from datetime import UTC, date, datetime
from io import BytesIO

import arxiv
import httpx
import pdfplumber
from bs4 import BeautifulSoup

from nook.common.base_service import BaseService
from nook.common.daily_snapshot import group_records_by_date, store_daily_snapshots
from nook.common.date_utils import is_within_target_dates, target_dates_set
from nook.common.decorators import handle_errors
from nook.common.logging_utils import (
    log_article_counts,
    log_multiple_dates_processing,
    log_no_new_articles,
    log_processing_start,
    log_storage_complete,
    log_summarization_progress,
    log_summarization_start,
    log_summary_candidates,
)


def remove_tex_backticks(text: str) -> str:
    r"""
    æ–‡å­—åˆ—ãŒ TeX å½¢å¼ã€ã¤ã¾ã‚Š
      `$\ldots$`
    ã®å ´åˆã€å¤–å´ã®ãƒãƒƒã‚¯ã‚¯ã‚©ãƒ¼ãƒˆ (`) ã ã‘ã‚’å‰Šé™¤ã—ã¦
      $\ldots$
    ã«å¤‰æ›ã—ã¾ã™ã€‚
    ãã‚Œä»¥å¤–ã®å ´åˆã¯ã€æ–‡å­—åˆ—ã‚’å¤‰æ›´ã—ã¾ã›ã‚“ã€‚
    """
    pattern = r"^`(\$.*?\$)`$"
    return re.sub(pattern, r"\1", text)


def remove_outer_markdown_markers(text: str) -> str:
    """
    æ–‡ç« ä¸­ã® "```markdown" ã§å§‹ã¾ã‚‹ãƒ–ãƒ­ãƒƒã‚¯ã«ã¤ã„ã¦ã€
    æœ€ã‚‚é ãã«ã‚ã‚‹ "```" ã‚’é–‰ã˜ãƒãƒ¼ã‚«ãƒ¼ã¨ã—ã¦èªè­˜ã—ã€
    é–‹å§‹ã® "```markdown" ã¨ãã®é–‰ã˜ãƒãƒ¼ã‚«ãƒ¼ "```" ã®ã¿ã‚’å‰Šé™¤ã—ã¾ã™ã€‚
    """
    pattern = r"```markdown(.*)```"
    return re.sub(pattern, lambda m: m.group(1), text, flags=re.DOTALL)


def remove_outer_singlequotes(text: str) -> str:
    """
    æ–‡ç« ä¸­ã® "'''" ã§å§‹ã¾ã‚‹ãƒ–ãƒ­ãƒƒã‚¯ã«ã¤ã„ã¦ã€
    æœ€ã‚‚é ãã«ã‚ã‚‹ "'''" ã‚’é–‰ã˜ãƒãƒ¼ã‚«ãƒ¼ã¨ã—ã¦èªè­˜ã—ã€
    é–‹å§‹ã® "'''" ã¨ãã®é–‰ã˜ãƒãƒ¼ã‚«ãƒ¼ "'''" ã®ã¿ã‚’å‰Šé™¤ã—ã¾ã™ã€‚
    """
    pattern = r"'''(.*)'''"
    return re.sub(pattern, lambda m: m.group(1), text, flags=re.DOTALL)


@dataclass
class PaperInfo:
    """
    arXivè«–æ–‡æƒ…å ±ã€‚

    Parameters
    ----------
    title : str
        è«–æ–‡ã‚¿ã‚¤ãƒˆãƒ«ã€‚
    abstract : str
        è¦ç´„ã€‚
    url : str
        URLã€‚
    contents : str
        è«–æ–‡ã®å†…å®¹ã€‚
    """

    title: str
    abstract: str
    url: str
    contents: str
    summary: str = field(init=False)
    published_at: datetime | None = None


class ArxivSummarizer(BaseService):
    """
    arXivè«–æ–‡ã‚’åé›†ãƒ»è¦ç´„ã™ã‚‹ã‚¯ãƒ©ã‚¹ã€‚

    Parameters
    ----------
    storage_dir : str, default="data"
        ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®ãƒ‘ã‚¹ã€‚
    """

    def __init__(self, storage_dir: str = "data"):
        """
        ArxivSummarizerã‚’åˆæœŸåŒ–ã—ã¾ã™ã€‚

        Parameters
        ----------
        storage_dir : str, default="data"
            ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®ãƒ‘ã‚¹ã€‚
        """
        super().__init__("arxiv_summarizer")
        self.http_client = None  # setup_http_clientã§åˆæœŸåŒ–

    async def collect(
        self,
        limit: int = 5,
        *,
        target_dates: list[date] | None = None,
    ) -> list[tuple[str, str]]:
        """
        arXivè«–æ–‡ã‚’åé›†ãƒ»è¦ç´„ã—ã¦ä¿å­˜ã—ã¾ã™ã€‚

        Parameters
        ----------
        limit : int, default=5
            å–å¾—ã™ã‚‹è«–æ–‡æ•°ã€‚

        Returns
        -------
        list[tuple[str, str]]
            ä¿å­˜ã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ã®ãƒªã‚¹ãƒˆ [(json_path, md_path), ...]
        """
        # HTTPã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã®åˆæœŸåŒ–ã‚’ç¢ºèª
        if self.http_client is None:
            await self.setup_http_client()

        effective_target_dates = target_dates or target_dates_set(1)
        sorted_dates = sorted(effective_target_dates)

        # å¯¾è±¡æœŸé–“ã®ãƒ­ã‚°å‡ºåŠ›
        if not sorted_dates:
            self.logger.info("å¯¾è±¡æ—¥ãŒæŒ‡å®šã•ã‚Œã¦ã„ãªã„ãŸã‚å‡¦ç†ã‚’çµ‚äº†ã—ã¾ã™")
            return []

        if len(sorted_dates) == 1:
            log_processing_start(self.logger, sorted_dates[0].strftime("%Y-%m-%d"))
        else:
            log_multiple_dates_processing(self.logger, sorted_dates)

        # å¯¾è±¡æ—¥ã”ã¨ã«Hugging Faceã‹ã‚‰è«–æ–‡IDã‚’å–å¾—
        collected_ids: list[str] = []
        seen_ids: set[str] = set()
        for snapshot_date in reversed(sorted_dates):
            snapshot_str = snapshot_date.strftime("%Y-%m-%d")
            self.logger.info(f"\nğŸ—“ï¸ {snapshot_str} ã®å€™è£œè«–æ–‡IDã‚’åé›†ä¸­...")

            daily_ids = await self._get_curated_paper_ids(limit, snapshot_date)

            if daily_ids is None:
                self.logger.info("   â„¹ï¸ URLãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ")
                continue

            if not daily_ids:
                self.logger.info("   â„¹ï¸ å–å¾—ã§ãã‚‹IDãŒã‚ã‚Šã¾ã›ã‚“ã§ã—ãŸ")
                continue

            new_ids = [paper_id for paper_id in daily_ids if paper_id not in seen_ids]

            if not new_ids:
                self.logger.info("   â„¹ï¸ ã™ã¹ã¦æ—¢å­˜IDã®ãŸã‚ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã—ãŸ")
                continue

            for paper_id in new_ids:
                seen_ids.add(paper_id)
                collected_ids.append(paper_id)

            self.logger.info(
                "   âœ… æ–°è¦ID %dä»¶ (ç´¯è¨ˆ%dä»¶)",
                len(new_ids),
                len(collected_ids),
            )

        if not collected_ids:
            self.logger.info("\nä¿å­˜ã™ã‚‹è«–æ–‡ãŒã‚ã‚Šã¾ã›ã‚“")
            return []

        # è«–æ–‡æƒ…å ±ã‚’ä¸¦è¡Œã—ã¦å–å¾—
        tasks = []
        for paper_id in collected_ids:
            tasks.append(self._retrieve_paper_info(paper_id))

        paper_results = await asyncio.gather(*tasks, return_exceptions=True)

        papers = []
        for result in paper_results:
            if isinstance(result, PaperInfo):
                if is_within_target_dates(result.published_at, effective_target_dates):
                    papers.append(result)
            elif isinstance(result, Exception):
                self.logger.error(f"Error retrieving paper: {result}")

        # è«–æ–‡æƒ…å ±ã‚’è¡¨ç¤º
        if papers:
            existing_count = 0  # æ—¢å­˜è«–æ–‡æ•°ï¼ˆç°¡ç•¥åŒ–ï¼‰
            new_count = len(papers)  # æ–°è¦è«–æ–‡æ•°

            # è«–æ–‡æƒ…å ±ã‚’è¡¨ç¤º
            log_article_counts(self.logger, existing_count, new_count)
            log_summary_candidates(self.logger, papers, "published_at")

        # è«–æ–‡ã‚’é€æ¬¡è¦ç´„ï¼ˆãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ é€²æ—è¡¨ç¤ºï¼‰
        if papers:
            log_summarization_start(self.logger)
            for idx, paper in enumerate(papers, 1):
                await self._summarize_paper_info(paper)
                log_summarization_progress(self.logger, idx, len(papers), paper.title)

        # è¦ç´„ã‚’ä¿å­˜
        saved_files = await self._store_summaries(papers, limit, effective_target_dates)

        # å‡¦ç†å®Œäº†ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸
        if saved_files:
            self.logger.info(f"\nğŸ’¾ {len(saved_files)}æ—¥åˆ†ã®ãƒ‡ãƒ¼ã‚¿ã‚’ä¿å­˜å®Œäº†")
            for json_path, md_path in saved_files:
                log_storage_complete(self.logger, json_path, md_path)
        else:
            log_no_new_articles(self.logger)

        # å‡¦ç†æ¸ˆã¿ã®è«–æ–‡IDã‚’ä¿å­˜ï¼ˆæ—¥ä»˜ã”ã¨ã«åˆ†ã‘ã¦ä¿å­˜ï¼‰
        await self._save_processed_ids_by_date(collected_ids, effective_target_dates)

        return saved_files

    # åŒæœŸç‰ˆã®äº’æ›æ€§ã®ãŸã‚ã®ãƒ©ãƒƒãƒ‘ãƒ¼
    def run(self, limit: int = 5) -> None:
        """åŒæœŸçš„ã«å®Ÿè¡Œã™ã‚‹ãŸã‚ã®ãƒ©ãƒƒãƒ‘ãƒ¼"""
        asyncio.run(self.collect(limit))

    def _is_valid_body_line(self, line: str, min_length: int = 80):
        """æœ¬æ–‡ã¨ã—ã¦å¦¥å½“ãªè¡Œã‹ã‚’åˆ¤æ–­ã™ã‚‹ãŸã‚ã®ç°¡æ˜“ãƒ’ãƒ¥ãƒ¼ãƒªã‚¹ãƒ†ã‚£ãƒƒã‚¯ã€‚"""
        if "@" in line:
            return False
        for kw in [
            "university",
            "lab",
            "department",
            "institute",
            "corresponding author",
        ]:
            if kw in line.lower():
                return False
        if len(line) < min_length:
            return False
        return False if "." not in line else True

    @handle_errors(retries=3)
    async def _get_curated_paper_ids(self, limit: int, snapshot_date: date) -> list[str] | None:
        """
        Hugging Faceã§ã‚­ãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã•ã‚ŒãŸè«–æ–‡IDã‚’å–å¾—ã—ã¾ã™ã€‚

        Parameters
        ----------
        limit : int
            å–å¾—ã™ã‚‹è«–æ–‡æ•°ã€‚
        snapshot_date : date
            å‚ç…§ã™ã‚‹Hugging Faceãƒšãƒ¼ã‚¸ã®æ—¥ä»˜ã€‚

        Returns
        -------
        List[str] or None
            è«–æ–‡IDã®ãƒªã‚¹ãƒˆã€‚URLãŒå­˜åœ¨ã—ãªã„å ´åˆã¯Noneã‚’è¿”ã™ã€‚
        """
        paper_ids: list[str] = []

        # Upvoteé †ã§ä¸¦ã‚“ã§ã„ã‚‹æ—¥ä»˜ãƒšãƒ¼ã‚¸ã‹ã‚‰è«–æ–‡IDã‚’æŠ½å‡º
        page_url = f"https://huggingface.co/papers/date/{snapshot_date:%Y-%m-%d}"
        try:
            response = await self.http_client.get(page_url)
            response.raise_for_status()

            # ãƒªãƒ€ã‚¤ãƒ¬ã‚¯ãƒˆã‚’æ¤œå‡ºï¼ˆå®Ÿéš›ã®URLã¨ãƒªã‚¯ã‚¨ã‚¹ãƒˆã—ãŸURLãŒç•°ãªã‚‹å ´åˆã¯ãƒªãƒ€ã‚¤ãƒ¬ã‚¯ãƒˆï¼‰
            if str(response.url) != page_url:
                self.logger.info(
                    "Hugging Faceæ—¥ä»˜ãƒšãƒ¼ã‚¸ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ: %s",
                    page_url,
                )
                return None

            soup = BeautifulSoup(response.text, "html.parser")

            for article in soup.select("article"):
                link = article.find("a", href=re.compile(r"^/papers/\d+\.\d+"))
                if not link:
                    continue

                href = link.get("href", "")
                paper_id_match = re.search(r"/papers/(\d+\.\d+)", href)
                if not paper_id_match:
                    continue

                paper_id = paper_id_match.group(1)
                if paper_id in paper_ids:
                    continue

                paper_ids.append(paper_id)
                if len(paper_ids) >= limit:
                    break

            if not paper_ids:
                self.logger.warning(
                    "Hugging Faceæ—¥ä»˜ãƒšãƒ¼ã‚¸ã‹ã‚‰è«–æ–‡IDã‚’å–å¾—ã§ãã¾ã›ã‚“ã§ã—ãŸ: %s",
                    page_url,
                )
        except httpx.HTTPStatusError as exc:
            # 404ã‚¨ãƒ©ãƒ¼ã®å ´åˆã¯URLãŒå­˜åœ¨ã—ãªã„ã“ã¨ã‚’ç¤ºã™
            if exc.response.status_code == 404:
                self.logger.info(
                    "Hugging Faceæ—¥ä»˜ãƒšãƒ¼ã‚¸ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ: %s",
                    page_url,
                )
                return None
            else:
                self.logger.error(
                    "Hugging Faceæ—¥ä»˜ãƒšãƒ¼ã‚¸ã®å–å¾—ã«å¤±æ•—ã—ã¾ã—ãŸ (%s): %s",
                    page_url,
                    exc,
                )
        except Exception as exc:
            self.logger.error(
                "Hugging Faceæ—¥ä»˜ãƒšãƒ¼ã‚¸ã®å–å¾—ã«å¤±æ•—ã—ã¾ã—ãŸ (%s): %s",
                page_url,
                exc,
            )

        # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: æ—§æ¥ã®ãƒˆãƒƒãƒ—ãƒšãƒ¼ã‚¸ã‹ã‚‰å–å¾—
        if not paper_ids:
            fallback_url = "https://huggingface.co/papers"
            response = await self.http_client.get(fallback_url)
            soup = BeautifulSoup(response.text, "html.parser")

            paper_links = soup.select("a[href^='/papers/']")
            for link in paper_links:
                href = link.get("href", "")
                paper_id_match = re.search(r"/papers/(\d+\.\d+)", href)
                if not paper_id_match:
                    continue

                paper_id = paper_id_match.group(1)
                if paper_id in paper_ids:
                    continue

                paper_ids.append(paper_id)
                if len(paper_ids) >= limit:
                    break

            if paper_ids:
                self.logger.warning("ãƒˆãƒƒãƒ—ãƒšãƒ¼ã‚¸ã‹ã‚‰è«–æ–‡IDã‚’å–å¾—ã—ã¾ã—ãŸ (ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯)")

        # æ—¢ã«å‡¦ç†æ¸ˆã¿ã®è«–æ–‡IDã‚’é™¤å¤–ï¼ˆå¯¾è±¡æ—¥ä»˜ã®ãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰å–å¾—ï¼‰
        processed_ids = await self._get_processed_ids(snapshot_date)
        paper_ids = [pid for pid in paper_ids if pid not in processed_ids]

        return paper_ids[:limit] if paper_ids else []

    async def _get_processed_ids(self, target_date: date | None = None) -> list[str]:
        """
        æ—¢ã«å‡¦ç†æ¸ˆã¿ã®è«–æ–‡IDã‚’å–å¾—ã—ã¾ã™ã€‚

        Parameters
        ----------
        target_date : date, optional
            å¯¾è±¡æ—¥ä»˜ã€‚æŒ‡å®šã—ãªã„å ´åˆã¯ä»Šæ—¥ã®æ—¥ä»˜ã‚’ä½¿ç”¨ã€‚

        Returns
        -------
        List[str]
            å‡¦ç†æ¸ˆã¿ã®è«–æ–‡IDã®ãƒªã‚¹ãƒˆã€‚
        """
        if target_date is None:
            target_date = datetime.now().date()

        date_str = target_date.strftime("%Y-%m-%d")
        filename = f"arxiv_ids-{date_str}.txt"

        content = await self.storage.load(filename)
        if not content:
            return []

        return [line.strip() for line in content.split("\n") if line.strip()]

    async def _save_processed_ids_by_date(
        self, paper_ids: list[str], target_dates: list[date]
    ) -> None:
        """
        å‡¦ç†æ¸ˆã¿ã®è«–æ–‡IDã‚’æ—¥ä»˜ã”ã¨ã«ä¿å­˜ã—ã¾ã™ã€‚

        Parameters
        ----------
        paper_ids : List[str]
            å‡¦ç†æ¸ˆã¿ã®è«–æ–‡IDã®ãƒªã‚¹ãƒˆã€‚
        target_dates : Set[date]
            å¯¾è±¡ã®æ—¥ä»˜ã‚»ãƒƒãƒˆã€‚
        """
        # è«–æ–‡æƒ…å ±ã‚’å–å¾—ã—ã¦å…¬é–‹æ—¥ã‚’ç¢ºèª
        tasks = []
        for paper_id in paper_ids:
            tasks.append(self._get_paper_date(paper_id))

        paper_dates = await asyncio.gather(*tasks, return_exceptions=True)

        # æ—¥ä»˜ã”ã¨ã«IDã‚’ã‚°ãƒ«ãƒ¼ãƒ—åŒ–
        ids_by_date = {}
        for paper_id, paper_date in zip(paper_ids, paper_dates, strict=True):
            if isinstance(paper_date, date):
                date_str = paper_date.strftime("%Y-%m-%d")
                if date_str not in ids_by_date:
                    ids_by_date[date_str] = []
                ids_by_date[date_str].append(paper_id)
            else:
                # æ—¥ä»˜ãŒä¸æ˜ã®å ´åˆã¯ä»Šæ—¥ã®æ—¥ä»˜ã‚’ä½¿ç”¨
                today = datetime.now()
                date_str = today.strftime("%Y-%m-%d")
                if date_str not in ids_by_date:
                    ids_by_date[date_str] = []
                ids_by_date[date_str].append(paper_id)

        # æ—¥ä»˜ã”ã¨ã«ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜
        for date_str, ids in ids_by_date.items():
            filename = f"arxiv_ids-{date_str}.txt"

            # æ—¢å­˜ã®IDã‚’èª­ã¿è¾¼ã‚€
            existing_ids = await self._load_ids_from_file(filename)

            # æ–°ã—ã„IDã‚’è¿½åŠ 
            all_ids = existing_ids + ids
            all_ids = list(dict.fromkeys(all_ids))  # é‡è¤‡ã‚’å‰Šé™¤

            content = "\n".join(all_ids)
            await self.save_data(content, filename)

    async def _load_ids_from_file(self, filename: str) -> list[str]:
        """
        æŒ‡å®šã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰IDã‚’èª­ã¿è¾¼ã¿ã¾ã™ã€‚

        Parameters
        ----------
        filename : str
            ãƒ•ã‚¡ã‚¤ãƒ«åã€‚

        Returns
        -------
        List[str]
            IDã®ãƒªã‚¹ãƒˆã€‚
        """
        content = await self.storage.load(filename)
        if not content:
            return []

        return [line.strip() for line in content.split("\n") if line.strip()]

    async def _get_paper_date(self, paper_id: str) -> date | None:
        """
        è«–æ–‡ã®å…¬é–‹æ—¥ã‚’å–å¾—ã—ã¾ã™ã€‚

        Parameters
        ----------
        paper_id : str
            è«–æ–‡IDã€‚

        Returns
        -------
        date or None
            è«–æ–‡ã®å…¬é–‹æ—¥ã€‚å–å¾—ã§ããªã„å ´åˆã¯Noneã€‚
        """
        try:
            # arxivãƒ©ã‚¤ãƒ–ãƒ©ãƒªã¯åŒæœŸçš„ãªã®ã§ã€åˆ¥ã‚¹ãƒ¬ãƒƒãƒ‰ã§å®Ÿè¡Œ
            loop = asyncio.get_event_loop()

            def get_paper():
                client = arxiv.Client()
                search = arxiv.Search(id_list=[paper_id])
                results = list(client.results(search))
                return results[0] if results else None

            paper = await loop.run_in_executor(None, get_paper)

            if not paper:
                return None

            published = getattr(paper, "published", None)
            if isinstance(published, datetime):
                return published.date()

            return None
        except Exception as e:
            self.logger.error(f"Error getting paper date for {paper_id}: {str(e)}")
            return None

    async def _retrieve_paper_info(self, paper_id: str) -> PaperInfo | None:
        """
        è«–æ–‡æƒ…å ±ã‚’å–å¾—ã—ã¾ã™ã€‚

        Parameters
        ----------
        paper_id : str
            è«–æ–‡IDã€‚

        Returns
        -------
        PaperInfo or None
            å–å¾—ã—ãŸè«–æ–‡æƒ…å ±ã€‚å–å¾—ã«å¤±æ•—ã—ãŸå ´åˆã¯Noneã€‚
        """
        try:
            # arxivãƒ©ã‚¤ãƒ–ãƒ©ãƒªã¯åŒæœŸçš„ãªã®ã§ã€åˆ¥ã‚¹ãƒ¬ãƒƒãƒ‰ã§å®Ÿè¡Œ
            loop = asyncio.get_event_loop()

            def get_paper():
                client = arxiv.Client()
                search = arxiv.Search(id_list=[paper_id])
                results = list(client.results(search))
                return results[0] if results else None

            paper = await loop.run_in_executor(None, get_paper)

            if not paper:
                return None

            # PDFã‹ã‚‰æœ¬æ–‡ã‚’æŠ½å‡º
            arxiv_id = paper.entry_id.split("/")[-1]  # URLã‹ã‚‰IDã‚’æŠ½å‡º
            contents = await self._extract_body_text(arxiv_id)
            if not contents:  # HTMLæŠ½å‡ºã«å¤±æ•—ã—ãŸå ´åˆã¯ã‚¢ãƒ–ã‚¹ãƒˆãƒ©ã‚¯ãƒˆã‚’ä½¿ç”¨
                contents = paper.summary

            # ã‚¿ã‚¤ãƒˆãƒ«ã¨ã‚¢ãƒ–ã‚¹ãƒˆãƒ©ã‚¯ãƒˆã‚’æ—¥æœ¬èªã«ç¿»è¨³
            title = paper.title
            abstract_ja = await self._translate_to_japanese(paper.summary)

            published_at = getattr(paper, "published", None)
            if isinstance(published_at, datetime):
                if published_at.tzinfo is None:
                    published_at = published_at.replace(tzinfo=UTC)
            else:
                published_at = datetime.now(UTC)

            return PaperInfo(
                title=title,
                abstract=abstract_ja,
                url=paper.entry_id,
                contents=contents,
                published_at=published_at,
            )

        except Exception as e:
            self.logger.error(f"Error retrieving paper {paper_id}: {str(e)}")
            return None

    async def _translate_to_japanese(self, text: str) -> str:
        """
        ãƒ†ã‚­ã‚¹ãƒˆã‚’æ—¥æœ¬èªã«ç¿»è¨³ã—ã¾ã™ã€‚

        Parameters
        ----------
        text : str
            ç¿»è¨³ã™ã‚‹ãƒ†ã‚­ã‚¹ãƒˆã€‚

        Returns
        -------
        str
            ç¿»è¨³ã•ã‚ŒãŸãƒ†ã‚­ã‚¹ãƒˆã€‚
        """
        try:
            prompt = f"ä»¥ä¸‹ã®è‹±èªã®å­¦è¡“è«–æ–‡ã®ãƒ†ã‚­ã‚¹ãƒˆã‚’è‡ªç„¶ãªæ—¥æœ¬èªã«ç¿»è¨³ã—ã¦ãã ã•ã„ã€‚å°‚é–€ç”¨èªã¯é©åˆ‡ã«ç¿»è¨³ã—ã€å¿…è¦ã«å¿œã˜ã¦è‹±èªã®å°‚é–€ç”¨èªã‚’æ‹¬å¼§å†…ã«æ®‹ã—ã¦ãã ã•ã„ã€‚\n\n{text}"

            translated_text = await self.gpt_client.generate_async(
                prompt=prompt,
                temperature=0.3,
                max_tokens=1000,
                service_name=self.service_name,
            )

            await self.rate_limit()

            return translated_text
        except Exception as e:
            self.logger.error(f"Error translating text: {str(e)}")
            return text  # ç¿»è¨³ã«å¤±æ•—ã—ãŸå ´åˆã¯åŸæ–‡ã‚’è¿”ã™

    async def _extract_body_text(self, arxiv_id: str, min_line_length: int = 40) -> str:
        """
        ArXivã‹ã‚‰æœ¬æ–‡ã‚’æŠ½å‡ºï¼ˆHTMLâ†’PDFâ†’ã‚¢ãƒ–ã‚¹ãƒˆãƒ©ã‚¯ãƒˆã®ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ãƒã‚§ãƒ¼ãƒ³ï¼‰

        Parameters
        ----------
        arxiv_id : str
            arXivè«–æ–‡ID
        min_line_length : int, default=40
            æœ¬æ–‡ã¨ã—ã¦æ‰±ã†æœ€å°è¡Œé•·

        Returns
        -------
        str
            æŠ½å‡ºã•ã‚ŒãŸæœ¬æ–‡ãƒ†ã‚­ã‚¹ãƒˆ
        """
        # 1. HTMLå½¢å¼ã‚’è©¦ã™
        html_text = await self._extract_from_html(arxiv_id, min_line_length)
        if html_text:
            self.logger.debug(f"HTMLã‹ã‚‰æœ¬æ–‡ã‚’æŠ½å‡º: {arxiv_id}")
            return html_text

        # 2. HTMLãŒå–å¾—ã§ããªã„å ´åˆã¯PDFã‚’è©¦ã™
        self.logger.info(f"HTMLå½¢å¼ãŒåˆ©ç”¨ã§ãã¾ã›ã‚“: {arxiv_id} - PDFæŠ½å‡ºã«ç§»è¡Œã—ã¾ã™")
        pdf_text = await self._extract_from_pdf(arxiv_id, min_line_length)
        if pdf_text:
            self.logger.info(f"PDFã‹ã‚‰æœ¬æ–‡ã‚’æŠ½å‡º: {arxiv_id}")
            return pdf_text

        # 3. ã©ã¡ã‚‰ã‚‚å¤±æ•—ã—ãŸå ´åˆã¯ç©ºæ–‡å­—åˆ—ï¼ˆå‘¼ã³å‡ºã—å…ƒã§ã‚¢ãƒ–ã‚¹ãƒˆãƒ©ã‚¯ãƒˆã‚’ä½¿ç”¨ï¼‰
        self.logger.warning(f"æœ¬æ–‡æŠ½å‡ºå¤±æ•—: {arxiv_id} - ã‚¢ãƒ–ã‚¹ãƒˆãƒ©ã‚¯ãƒˆã‚’ä½¿ç”¨ã—ã¾ã™")
        return ""

    async def _download_html_without_retry(self, html_url: str) -> str:
        """
        ãƒªãƒˆãƒ©ã‚¤ãªã—ã§HTMLã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ï¼ˆãƒ‡ã‚³ãƒ¬ãƒ¼ã‚¿ã‚’å›é¿ï¼‰

        Parameters
        ----------
        html_url : str
            HTMLã®URL

        Returns
        -------
        str
            HTMLã‚³ãƒ³ãƒ†ãƒ³ãƒ„
        """
        try:
            async with httpx.AsyncClient(timeout=30.0) as client:
                response = await client.get(html_url)
                response.raise_for_status()
                return response.text
        except httpx.HTTPStatusError as e:
            if e.response.status_code == 404:
                # 404ã¯æ­£å¸¸ãªã‚±ãƒ¼ã‚¹ãªã®ã§é™ã‹ã«å‡¦ç†
                return ""
            raise
        except Exception:
            # ãã®ä»–ã®ã‚¨ãƒ©ãƒ¼ã¯å†ç™ºç”Ÿ
            raise

    async def _extract_from_html(self, arxiv_id: str, min_line_length: int = 40) -> str:
        """
        HTMLå½¢å¼ã‹ã‚‰æœ¬æ–‡ã‚’æŠ½å‡º

        Parameters
        ----------
        arxiv_id : str
            arXivè«–æ–‡ID
        min_line_length : int
            æœ€å°è¡Œé•·

        Returns
        -------
        str
            æŠ½å‡ºã•ã‚ŒãŸãƒ†ã‚­ã‚¹ãƒˆ
        """
        try:
            # HTMLã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ï¼ˆãƒªãƒˆãƒ©ã‚¤ãªã—ï¼‰
            html_url = f"https://arxiv.org/html/{arxiv_id}"
            html_content = await self._download_html_without_retry(html_url)

            if not html_content:
                return ""

            soup = BeautifulSoup(html_content, "html.parser")

            body = soup.body
            if body:
                for tag in body.find_all(["header", "nav", "footer", "script", "style"]):
                    tag.decompose()
                full_text = body.get_text(separator="\n", strip=True)
            else:
                full_text = ""

            lines = full_text.splitlines()

            # ãƒ’ãƒ¥ãƒ¼ãƒªã‚¹ãƒ†ã‚£ãƒƒã‚¯ã«ã‚ˆã‚Šã€å®Ÿéš›ã®è«–æ–‡æœ¬æ–‡ã®é–‹å§‹è¡Œã‚’æ¢ã™
            start_index = 0
            for i, line in enumerate(lines):
                clean_line = line.strip()
                # å…ˆé ­éƒ¨åˆ†ã®ç©ºè¡Œã‚„çŸ­ã™ãã‚‹è¡Œã¯ã‚¹ã‚­ãƒƒãƒ—
                if len(clean_line) < min_line_length:
                    continue
                if self._is_valid_body_line(clean_line, min_length=100):
                    start_index = i
                    break

            # é–‹å§‹è¡Œä»¥é™ã‚’æœ¬æ–‡ã¨ã—ã¦æŠ½å‡º
            body_lines = lines[start_index:]
            # ãƒã‚¤ã‚ºé™¤å»: çŸ­ã™ãã‚‹è¡Œã¯é™¤å¤–
            filtered_lines = []
            for line in body_lines:
                if len(line.strip()) >= min_line_length:
                    line = line.strip()
                    line = line.replace("Ã‚", " ")
                    filtered_lines.append(line.strip())
            return "\n".join(filtered_lines)
        except Exception as e:
            self.logger.debug(f"HTMLæŠ½å‡ºå¤±æ•—: {arxiv_id} - {str(e)}")
            return ""

    async def _download_pdf_without_retry(self, pdf_url: str) -> httpx.Response:
        """
        ãƒªãƒˆãƒ©ã‚¤ãªã—ã§PDFã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ï¼ˆãƒ‡ã‚³ãƒ¬ãƒ¼ã‚¿ã‚’å›é¿ï¼‰

        Parameters
        ----------
        pdf_url : str
            PDFã®URL

        Returns
        -------
        httpx.Response
            HTTPãƒ¬ã‚¹ãƒãƒ³ã‚¹
        """
        async with httpx.AsyncClient(timeout=30.0) as client:
            response = await client.get(pdf_url)
            response.raise_for_status()
            return response

    async def _extract_from_pdf(self, arxiv_id: str, min_line_length: int = 40) -> str:
        """
        PDFå½¢å¼ã‹ã‚‰æœ¬æ–‡ã‚’æŠ½å‡º

        Parameters
        ----------
        arxiv_id : str
            arXivè«–æ–‡ID
        min_line_length : int
            æœ€å°è¡Œé•·

        Returns
        -------
        str
            æŠ½å‡ºã•ã‚ŒãŸãƒ†ã‚­ã‚¹ãƒˆ
        """
        try:
            # PDFã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ï¼ˆãƒªãƒˆãƒ©ã‚¤ãªã—ï¼‰
            pdf_url = f"https://arxiv.org/pdf/{arxiv_id}"

            # ãƒªãƒˆãƒ©ã‚¤ãªã—ã®ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ãƒ¡ã‚½ãƒƒãƒ‰ã‚’ä½¿ç”¨
            response = await self._download_pdf_without_retry(pdf_url)

            if not response.content:
                return ""

            # pdfplumberã§ãƒ†ã‚­ã‚¹ãƒˆæŠ½å‡º
            with pdfplumber.open(BytesIO(response.content)) as pdf:
                text_parts = []

                for _page_num, page in enumerate(pdf.pages):
                    try:
                        page_text = page.extract_text()
                        if page_text and len(page_text.strip()) > 100:  # æœ‰æ„ãªãƒ†ã‚­ã‚¹ãƒˆã®ã¿
                            # ãƒšãƒ¼ã‚¸ç•ªå·ã‚„ãƒ˜ãƒƒãƒ€ãƒ¼/ãƒ•ãƒƒã‚¿ãƒ¼ã‚’é™¤å»
                            lines = page_text.split("\n")
                            filtered_lines = []

                            for line in lines:
                                clean_line = line.strip()
                                # ãƒšãƒ¼ã‚¸ç•ªå·ã‚„çŸ­ã™ãã‚‹è¡Œã‚’é™¤å¤–
                                if (
                                    len(clean_line) >= min_line_length
                                    and not clean_line.isdigit()
                                    and not clean_line.startswith("arXiv:")
                                    and "References" not in clean_line[:20]
                                ):  # å‚è€ƒæ–‡çŒ®ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã‚’é™¤å¤–
                                    filtered_lines.append(clean_line)

                            if filtered_lines:
                                text_parts.append("\n".join(filtered_lines))

                    except Exception as page_error:
                        self.logger.debug(
                            f"ãƒšãƒ¼ã‚¸æŠ½å‡ºå¤±æ•—: {arxiv_id} page {_page_num} - {page_error}"
                        )
                        continue

                if text_parts:
                    full_text = "\n\n".join(text_parts)
                    return full_text
                else:
                    return ""

        except Exception as e:
            self.logger.debug(f"PDFæŠ½å‡ºå¤±æ•—: {arxiv_id} - {str(e)}")
            return ""

    async def _summarize_papers(self, papers: list[PaperInfo]) -> None:
        """è¤‡æ•°ã®è«–æ–‡ã‚’ä¸¦è¡Œã—ã¦è¦ç´„"""
        tasks = []
        for paper in papers:
            tasks.append(self._summarize_paper_info(paper))

        await asyncio.gather(*tasks, return_exceptions=True)

    async def _summarize_paper_info(self, paper_info: PaperInfo) -> None:
        """è«–æ–‡ã‚’è¦ç´„ã—ã¾ã™ã€‚"""
        prompt = """
        ä»¥ä¸‹ã®8ã¤ã®è³ªå•ã«ã¤ã„ã¦ã€é †ã‚’è¿½ã£ã¦éå¸¸ã«è©³ç´°ã«ã€åˆ†ã‹ã‚Šã‚„ã™ãç­”ãˆã¦ãã ã•ã„ã€‚

        1. æ—¢å­˜ç ”ç©¶ã§ã¯ä½•ãŒã§ããªã‹ã£ãŸã®ã‹
        2. ã©ã®ã‚ˆã†ãªã‚¢ãƒ—ãƒ­ãƒ¼ãƒã§ãã‚Œã‚’è§£æ±ºã—ã‚ˆã†ã¨ã—ãŸã‹
        3. çµæœã€ä½•ãŒé”æˆã§ããŸã®ã‹
        4. åˆ¶é™ã‚„å•é¡Œç‚¹ã¯ä½•ã§ã™ã‹ã€‚æœ¬æ–‡ã§è¨€åŠã•ã‚Œã¦ã„ã‚‹ã‚„ã‚ãªãŸãŒè€ƒãˆã‚‹ã‚‚ã®ã‚‚å«ã‚ã¦æ•™ãˆã¦ãã ã•ã„
        5. æŠ€è¡“çš„ãªè©³ç´°ã«ã¤ã„ã¦ã€‚æŠ€è¡“è€…ãŒèª­ã‚€ã“ã¨ã‚’æƒ³å®šã—ãŸãƒˆãƒ¼ãƒ³ã§æ•™ãˆã¦ãã ã•ã„
        6. ã‚³ã‚¹ãƒˆã‚„ç‰©ç†çš„ãªè©³ç´°ã«ã¤ã„ã¦æ•™ãˆã¦ãã ã•ã„ã€‚ä¾‹ãˆã°ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã«ä½¿ç”¨ã—ãŸGPUã®æ•°ã‚„æ™‚é–“ã€ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã€ãƒ¢ãƒ‡ãƒ«ã®ã‚µã‚¤ã‚ºãªã©
        7. å‚è€ƒæ–‡çŒ®ã®ã†ã¡ã€ç‰¹ã«å‚ç…§ã™ã¹ãã‚‚ã®ã‚’æ•™ãˆã¦ãã ã•ã„
        8. ã“ã®è«–æ–‡ã‚’140å­—ä»¥å†…ã§è¦ç´„ã™ã‚‹ã¨ã©ã†ãªã‚Šã¾ã™ã‹ï¼Ÿ

        ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã¯ä»¥ä¸‹ã®é€šã‚Šã§ã€markdownå½¢å¼ã§å›ç­”ã—ã¦ãã ã•ã„ã€‚ã“ã®ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã«æ²¿ã£ãŸæ–‡è¨€ä»¥å¤–ã®å‡ºåŠ›ã¯ä¸è¦ã§ã™ã€‚
        ãªãŠã€æ•°å¼ã¯è¡¨ç¤ºãŒå´©ã‚ŒãŒã¡ã§é¢å€’ãªã®ã§ã€èª¬æ˜ã«æ•°å¼ã‚’ä½¿ã†ã¨ãã¯ã€ä»£ã‚ã‚Šã«Pythoné¢¨ã®ç–‘ä¼¼ã‚³ãƒ¼ãƒ‰ã‚’æ›¸ã„ã¦ãã ã•ã„ã€‚

        1. æ—¢å­˜ç ”ç©¶ã§ã¯ä½•ãŒã§ããªã‹ã£ãŸã®ã‹

        ...

        2. ã©ã®ã‚ˆã†ãªã‚¢ãƒ—ãƒ­ãƒ¼ãƒã§ãã‚Œã‚’è§£æ±ºã—ã‚ˆã†ã¨ã—ãŸã‹

        ...

        ï¼ˆä»¥ä¸‹åŒæ§˜ï¼‰
        """

        system_instruction = f"""
        ä»¥ä¸‹ã®ãƒ†ã‚­ã‚¹ãƒˆã¯ã€ã‚ã‚‹è«–æ–‡ã®ã‚¿ã‚¤ãƒˆãƒ«ã¨URLã€abstractã€ãŠã‚ˆã³æœ¬æ–‡ã®ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã§ã™ã€‚
        æœ¬æ–‡ã¯htmlã‹ã‚‰æŠ½å‡ºã•ã‚ŒãŸã‚‚ã®ã§ã€ãƒã‚¤ã‚ºã‚„ä¸è¦ãªéƒ¨åˆ†ãŒå«ã¾ã‚Œã¦ã„ã‚‹å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚
        ã‚ˆãèª­ã‚“ã§ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è³ªå•ã«ç­”ãˆã¦ãã ã•ã„ã€‚

        title
        '''
        {paper_info.title}
        '''

        url
        '''
        {paper_info.url}
        '''

        abstract
        '''
        {paper_info.abstract}
        '''

        contents
        '''
        {paper_info.contents}
        '''
        """

        try:
            summary = await self.gpt_client.generate_async(
                prompt=prompt,
                system_instruction=system_instruction,
                temperature=0.3,
                max_tokens=3000,  # 8ã¤ã®è³ªå•ã«å¯¾å¿œã™ã‚‹ãŸã‚å¢—é‡
                service_name=self.service_name,
            )

            # å‡ºåŠ›ã®æ•´å½¢
            summary = remove_tex_backticks(summary)
            summary = remove_outer_markdown_markers(summary)
            summary = remove_outer_singlequotes(summary)

            paper_info.summary = summary
            await self.rate_limit()
        except Exception as e:
            self.logger.error(f"Error generating summary: {type(e).__name__}: {str(e)}")
            if hasattr(e, "last_attempt") and hasattr(e.last_attempt, "exception"):
                inner_error = e.last_attempt.exception()
                self.logger.error(f"Inner error: {type(inner_error).__name__}: {str(inner_error)}")
            paper_info.summary = f"è¦ç´„ã®ç”Ÿæˆä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}"

    async def _store_summaries(
        self,
        papers: list[PaperInfo],
        limit: int,
        target_dates: list[date],
    ) -> list[tuple[str, str]]:
        """
        è¦ç´„ã‚’ä¿å­˜ã—ã¾ã™ã€‚

        Parameters
        ----------
        papers : List[PaperInfo]
            ä¿å­˜ã™ã‚‹è«–æ–‡ã®ãƒªã‚¹ãƒˆã€‚

        Returns
        -------
        list[tuple[str, str]]
            ä¿å­˜ã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ã®ãƒªã‚¹ãƒˆ [(json_path, md_path), ...]
        """
        if not papers:
            return []

        default_date = max(target_dates) if target_dates else datetime.now().date()
        records = self._serialize_papers(papers)
        records_by_date = group_records_by_date(records, default_date=default_date)

        saved_files = await store_daily_snapshots(
            records_by_date,
            load_existing=self._load_existing_papers,
            save_json=self.save_json,
            save_markdown=self.save_markdown,
            render_markdown=self._render_markdown,
            key=lambda item: item.get("title", ""),
            sort_key=self._paper_sort_key,
            limit=limit,
            logger=self.logger,
        )

        return saved_files

    def _serialize_papers(self, papers: list[PaperInfo]) -> list[dict]:
        records: list[dict] = []
        for paper in papers:
            published = paper.published_at or datetime.now(UTC)
            records.append(
                {
                    "title": paper.title,
                    "abstract": paper.abstract,
                    "url": paper.url,
                    "summary": getattr(paper, "summary", ""),
                    "contents": paper.contents,
                    "published_at": published.isoformat(),
                }
            )
        return records

    async def _load_existing_papers(self, target_date: datetime) -> list[dict]:
        date_str = target_date.strftime("%Y-%m-%d")
        filename_json = f"{date_str}.json"
        existing_json = await self.load_json(filename_json)
        if existing_json:
            return existing_json

        markdown = await self.storage.load(f"{date_str}.md")
        if not markdown:
            return []

        return self._parse_markdown(markdown)

    def _paper_sort_key(self, item: dict) -> tuple[int, datetime]:
        # arXivã§ã¯ã‚¹ã‚³ã‚¢ãŒç„¡ã„ã®ã§ã‚¢ãƒƒãƒ—ãƒ‡ãƒ¼ãƒˆæ—¥æ™‚ã®ã¿ã§ã‚½ãƒ¼ãƒˆ
        published_raw = item.get("published_at")
        if published_raw:
            try:
                published = datetime.fromisoformat(published_raw)
            except ValueError:
                published = datetime.min.replace(tzinfo=UTC)
        else:
            published = datetime.min.replace(tzinfo=UTC)
        return (0, published)

    def _render_markdown(self, records: list[dict], today: datetime) -> str:
        content = f"# arXiv è«–æ–‡è¦ç´„ ({today.strftime('%Y-%m-%d')})\n\n"
        for paper in records:
            content += f"## [{paper['title']}]({paper['url']})\n\n"
            content += f"**abstract**:\n{paper.get('abstract', '')}\n\n"
            content += f"**summary**:\n{paper.get('summary', '')}\n\n"
            content += "---\n\n"
        return content

    def _parse_markdown(self, markdown: str) -> list[dict]:
        pattern = re.compile(
            r"## \[(?P<title>.+?)\]\((?P<url>[^\)]+)\)\n\n"
            r"\*\*abstract\*\*:\n(?P<abstract>.*?)(?:\n\n)?"
            r"\*\*summary\*\*:\n(?P<summary>.*?)(?:\n\n)?---",
            re.DOTALL,
        )

        records: list[dict] = []
        for match in pattern.finditer(markdown + "---"):
            records.append(
                {
                    "title": match.group("title").strip(),
                    "url": match.group("url").strip(),
                    "abstract": match.group("abstract").strip(),
                    "summary": match.group("summary").strip(),
                    "contents": None,
                }
            )

        return records
