"""æŠ€è¡“ãƒ‹ãƒ¥ãƒ¼ã‚¹ã®RSSãƒ•ã‚£ãƒ¼ãƒ‰ã‚’ç›£è¦–ãƒ»åé›†ãƒ»è¦ç´„ã™ã‚‹ã‚µãƒ¼ãƒ“ã‚¹ã€‚"""

import asyncio
import json
from datetime import date, datetime
from pathlib import Path

import feedparser
import tomli
from bs4 import BeautifulSoup

from nook.common.daily_snapshot import group_records_by_date
from nook.common.date_utils import is_within_target_dates, target_dates_set
from nook.common.dedup import load_existing_titles_from_storage
from nook.common.feed_utils import parse_entry_datetime
from nook.common.logging_utils import (
    log_article_counts,
    log_no_new_articles,
    log_processing_start,
    log_storage_complete,
    log_summarization_progress,
    log_summarization_start,
    log_summary_candidates,
)
from nook.services.base_feed_service import Article, BaseFeedService


class TechFeed(BaseFeedService):
    """
    æŠ€è¡“ãƒ‹ãƒ¥ãƒ¼ã‚¹ã®RSSãƒ•ã‚£ãƒ¼ãƒ‰ã‚’ç›£è¦–ãƒ»åé›†ãƒ»è¦ç´„ã™ã‚‹ã‚¯ãƒ©ã‚¹ã€‚

    Parameters
    ----------
    storage_dir : str, default="data"
        ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®ãƒ‘ã‚¹ã€‚
    """

    TOTAL_LIMIT = 15
    SUMMARY_LIMIT = 15

    def __init__(self, storage_dir: str = "data"):
        """
        TechFeedã‚’åˆæœŸåŒ–ã—ã¾ã™ã€‚

        Parameters
        ----------
        storage_dir : str, default="data"
            ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®ãƒ‘ã‚¹ã€‚
        """
        super().__init__("tech_feed")
        self.http_client = None  # setup_http_clientã§åˆæœŸåŒ–

        # ãƒ•ã‚£ãƒ¼ãƒ‰ã®è¨­å®šã‚’èª­ã¿è¾¼ã‚€
        script_dir = Path(__file__).parent
        with open(script_dir / "feed.toml", "rb") as f:
            self.feed_config = tomli.load(f)

    def run(self, days: int = 1, limit: int | None = None) -> None:
        """
        æŠ€è¡“ãƒ‹ãƒ¥ãƒ¼ã‚¹ã®RSSãƒ•ã‚£ãƒ¼ãƒ‰ã‚’ç›£è¦–ãƒ»åé›†ãƒ»è¦ç´„ã—ã¦ä¿å­˜ã—ã¾ã™ã€‚

        Parameters
        ----------
        days : int, default=1
            ä½•æ—¥å‰ã¾ã§ã®è¨˜äº‹ã‚’å–å¾—ã™ã‚‹ã‹ã€‚
        limit : Optional[int], default=None
            å„ãƒ•ã‚£ãƒ¼ãƒ‰ã‹ã‚‰å–å¾—ã™ã‚‹è¨˜äº‹æ•°ã€‚Noneã®å ´åˆã¯åˆ¶é™ãªã—ã€‚
        """
        asyncio.run(self.collect(days, limit))

    async def collect(
        self,
        days: int = 1,
        limit: int | None = None,
        *,
        target_dates: list[date] | None = None,
    ) -> list[tuple[str, str]]:
        """
        æŠ€è¡“ãƒ‹ãƒ¥ãƒ¼ã‚¹ã®RSSãƒ•ã‚£ãƒ¼ãƒ‰ã‚’ç›£è¦–ãƒ»åé›†ãƒ»è¦ç´„ã—ã¦ä¿å­˜ã—ã¾ã™ï¼ˆéåŒæœŸç‰ˆï¼‰ã€‚

        Parameters
        ----------
        days : int, default=1
            ä½•æ—¥å‰ã¾ã§ã®è¨˜äº‹ã‚’å–å¾—ã™ã‚‹ã‹ã€‚
        limit : Optional[int], default=None
            å„ãƒ•ã‚£ãƒ¼ãƒ‰ã‹ã‚‰å–å¾—ã™ã‚‹è¨˜äº‹æ•°ã€‚Noneã®å ´åˆã¯åˆ¶é™ãªã—ã€‚

        Returns
        -------
        list[tuple[str, str]]
            ä¿å­˜ã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ã®ãƒªã‚¹ãƒˆ [(json_path, md_path), ...]
        """
        # HTTPã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã®åˆæœŸåŒ–ã‚’ç¢ºèª
        if self.http_client is None:
            await self.setup_http_client()

        candidate_articles: list[Article] = []
        effective_target_dates = target_dates or target_dates_set(days)

        # ã‚«ãƒ†ã‚´ãƒªæ¨ªæ–­ã®ã‚¿ã‚¤ãƒˆãƒ«é‡è¤‡ãƒã‚§ãƒƒã‚¯ç”¨ï¼ˆæ—¢å­˜ãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰ãƒ­ãƒ¼ãƒ‰ï¼‰
        # ãƒã‚°ä¿®æ­£ï¼šå…¨ã¦ã®æ—¢å­˜ãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰é‡è¤‡ãƒã‚§ãƒƒã‚¯
        all_existing_dates = await self._get_all_existing_dates()
        dedup_tracker = await load_existing_titles_from_storage(
            self.storage, all_existing_dates, self.logger
        )

        self.logger.info("\nğŸ“¡ ãƒ•ã‚£ãƒ¼ãƒ‰å–å¾—ä¸­...")

        try:
            # å„ã‚«ãƒ†ã‚´ãƒªã®ãƒ•ã‚£ãƒ¼ãƒ‰ã‹ã‚‰è¨˜äº‹ã‚’å–å¾—
            for category, feeds in self.feed_config.items():
                for feed_url in feeds:
                    try:
                        # ãƒ•ã‚£ãƒ¼ãƒ‰ã‚’è§£æ
                        feed = feedparser.parse(feed_url)
                        feed_name = (
                            feed.feed.title
                            if hasattr(feed, "feed") and hasattr(feed.feed, "title")
                            else feed_url
                        )

                        effective_limit = None
                        if limit is not None:
                            effective_limit = limit * max(days, 1)

                        entries = self._filter_entries(
                            feed.entries, effective_target_dates, effective_limit
                        )
                        self.logger.info(f"   â€¢ {feed_name}: {len(entries)}ä»¶å–å¾—")

                        for entry in entries:
                            # è¨˜äº‹ã‚’å–å¾—
                            article = await self._retrieve_article(
                                entry, feed_name, category
                            )
                            if article:
                                # é‡è¤‡ã‚¿ã‚¤ãƒˆãƒ«ã‚’ã‚¹ã‚­ãƒƒãƒ—ï¼ˆã‚«ãƒ†ã‚´ãƒªæ¨ªæ–­ãƒ»æ­£è¦åŒ–æ¸ˆã¿ï¼‰
                                is_dup, normalized_title = dedup_tracker.is_duplicate(
                                    article.title
                                )
                                if is_dup:
                                    original = dedup_tracker.get_original_title(
                                        normalized_title
                                    )
                                    self.logger.info(
                                        f"é‡è¤‡è¨˜äº‹ã‚’ã‚¹ã‚­ãƒƒãƒ—: '{article.title}' "
                                        f"(æ­£è¦åŒ–å¾Œ: '{normalized_title}', åˆå‡º: '{original}')"  # noqa: E501
                                    )
                                    continue

                                # æ—¥ä»˜ç¯„å›²ãƒã‚§ãƒƒã‚¯
                                if not is_within_target_dates(
                                    article.published_at, effective_target_dates
                                ):
                                    continue

                                dedup_tracker.add(article.title)
                                candidate_articles.append(article)

                    except Exception as e:
                        self.logger.error(
                            f"ãƒ•ã‚£ãƒ¼ãƒ‰ {feed_url} ã®å‡¦ç†ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}"  # noqa: E501
                        )

            # æ—¥ä»˜ã”ã¨ã«ã‚°ãƒ«ãƒ¼ãƒ—åŒ–
            articles_by_date = self._group_articles_by_date(candidate_articles)

            # æ—¥ä»˜ã”ã¨ã«ä¸Šä½Nä»¶ã‚’é¸æŠã—ã¦è¦ç´„
            saved_files: list[tuple[str, str]] = []
            for date_str in sorted(articles_by_date.keys()):
                date_articles = articles_by_date[date_str]

                # ãã®æ—¥ã®æ—¢å­˜è¨˜äº‹ã‚¿ã‚¤ãƒˆãƒ«ã‚’å–å¾—
                existing_titles_for_date = set()
                try:
                    json_content = await self.storage.load(f"{date_str}.json")
                    if json_content:
                        existing_articles = json.loads(json_content)
                        existing_titles_for_date = {
                            article.get("title", "") for article in existing_articles
                        }
                except Exception as e:
                    self.logger.debug(
                        f"æ—¢å­˜è¨˜äº‹ãƒ•ã‚¡ã‚¤ãƒ« {date_str}.json ã®èª­ã¿è¾¼ã¿ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}"  # noqa: E501
                    )

                # æ—¢å­˜/æ–°è¦è¨˜äº‹æ•°ã‚’ã‚«ã‚¦ãƒ³ãƒˆ
                existing_count = len(existing_titles_for_date)
                new_count = len(date_articles)

                # ãƒ­ã‚°æ”¹å–„ï¼šçœŸã«æ–°è¦ã®è¨˜äº‹ã‚’ç¢ºèª
                truly_new_articles = [
                    article
                    for article in date_articles
                    if article.title not in existing_titles_for_date
                ]

                # æ—¥ä»˜æƒ…å ±ã‚’å…ˆé ­ã«è¡¨ç¤ºï¼ˆãƒ­ã‚°æ”¹å–„ç‰ˆï¼‰
                log_processing_start(self.logger, date_str)
                log_article_counts(self.logger, existing_count, len(truly_new_articles))

                # æ–°è¦è¨˜äº‹ã®ã¿ã‚’è¦ç´„å¯¾è±¡ã¨ã—ã¦é¸æŠ
                selected = self._select_top_articles(truly_new_articles, limit)

                if selected:
                    log_summary_candidates(self.logger, selected)

                    # è¦ç´„ç”Ÿæˆ
                    log_summarization_start(self.logger)
                    for idx, article in enumerate(selected, 1):
                        await self._summarize_article(article)
                        log_summarization_progress(
                            self.logger, idx, len(selected), article.title
                        )

                    # ãƒ­ã‚°æ”¹å–„ï¼šä¿å­˜å®Œäº†ã®å‰ã«æ”¹è¡Œ
                    json_path, md_path = await self._store_summaries_for_date(
                        selected, date_str
                    )
                    log_storage_complete(self.logger, json_path, md_path)
                    saved_files.append((json_path, md_path))
                else:
                    log_no_new_articles(self.logger)

            # å‡¦ç†å®Œäº†ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸
            if saved_files:
                self.logger.info(f"\nğŸ’¾ {len(saved_files)}æ—¥åˆ†ã®ãƒ‡ãƒ¼ã‚¿ã‚’ä¿å­˜å®Œäº†")
            else:
                self.logger.info("\nä¿å­˜ã™ã‚‹è¨˜äº‹ãŒã‚ã‚Šã¾ã›ã‚“")

            return saved_files

        finally:
            pass

    def _select_top_articles(
        self, articles: list[Article], limit: int | None = None
    ) -> list[Article]:
        """
        è¨˜äº‹ã‚’äººæ°—ã‚¹ã‚³ã‚¢ã§ã‚½ãƒ¼ãƒˆã—ã€ä¸Šä½Nä»¶ã‚’é¸æŠã—ã¾ã™ã€‚

        Parameters
        ----------
        articles : list[Article]
            é¸æŠå¯¾è±¡ã®è¨˜äº‹ãƒªã‚¹ãƒˆ
        limit : Optional[int]
            é¸æŠã™ã‚‹è¨˜äº‹æ•°ã€‚Noneã®å ´åˆã¯SUMMARY_LIMITã‚’ä½¿ç”¨

        Returns
        -------
        list[Article]
            é¸æŠã•ã‚ŒãŸè¨˜äº‹ãƒªã‚¹ãƒˆ
        """
        if not articles:
            return []

        # äººæ°—ã‚¹ã‚³ã‚¢ã§é™é †ã‚½ãƒ¼ãƒˆ
        sorted_articles = sorted(
            articles, key=lambda x: x.popularity_score, reverse=True
        )

        # ä¸Šä½Nä»¶ã‚’é¸æŠï¼ˆlimitãŒæŒ‡å®šã•ã‚Œã¦ã„ã‚Œã°ãã‚Œã‚’ä½¿ç”¨ã€ãªã‘ã‚Œã°SUMMARY_LIMITï¼‰
        selection_limit = limit if limit is not None else self.SUMMARY_LIMIT
        return sorted_articles[:selection_limit]

    async def _retrieve_article(
        self, entry: dict, feed_name: str, category: str
    ) -> Article | None:
        """
        è¨˜äº‹ã‚’å–å¾—ã—ã¾ã™ï¼ˆæ—¥æœ¬èªåˆ¤å®šå«ã‚€ï¼‰ã€‚

        Parameters
        ----------
        entry : dict
            ã‚¨ãƒ³ãƒˆãƒªæƒ…å ±ã€‚
        feed_name : str
            ãƒ•ã‚£ãƒ¼ãƒ‰åã€‚
        category : str
            ã‚«ãƒ†ã‚´ãƒªã€‚

        Returns
        -------
        Article or None
            å–å¾—ã—ãŸè¨˜äº‹ã€‚å–å¾—ã«å¤±æ•—ã—ãŸå ´åˆã¯Noneã€‚
        """
        try:
            # URLã‚’å–å¾—
            url = entry.link if hasattr(entry, "link") else None
            if not url:
                return None

            # ã‚¿ã‚¤ãƒˆãƒ«ã‚’å–å¾—
            title = entry.title if hasattr(entry, "title") else "ç„¡é¡Œ"

            # è¨˜äº‹ã®å†…å®¹ã‚’å–å¾—
            response = await self.http_client.get(url)
            soup = BeautifulSoup(response.text, "html.parser")

            # æ—¥æœ¬èªåˆ¤å®šï¼ˆtech_feedç‰¹æœ‰ï¼‰
            if not self._detect_japanese_content(soup, title, entry):
                self.logger.debug(f"éæ—¥æœ¬èªè¨˜äº‹ã‚’ã‚¹ã‚­ãƒƒãƒ—: {title}")
                return None

            # æœ¬æ–‡ã‚’æŠ½å‡º
            text = ""

            # ã¾ãšã¯ã‚¨ãƒ³ãƒˆãƒªã®è¦ç´„ã‚’ä½¿ç”¨
            if hasattr(entry, "summary"):
                text = entry.summary

            # æ¬¡ã«è¨˜äº‹ã®æœ¬æ–‡ã‚’æŠ½å‡º
            if not text:
                # ãƒ¡ã‚¿ãƒ‡ã‚£ã‚¹ã‚¯ãƒªãƒ—ã‚·ãƒ§ãƒ³ã‚’å–å¾—
                meta_desc = soup.find("meta", attrs={"name": "description"})
                if meta_desc and meta_desc.get("content"):
                    text = meta_desc.get("content")
                else:
                    # æœ¬æ–‡ã®æœ€åˆã®æ®µè½ã‚’å–å¾—
                    paragraphs = soup.find_all("p")
                    if paragraphs:
                        text = "\n".join([p.get_text() for p in paragraphs[:5]])

            popularity = self._extract_popularity(entry, soup)
            published_at = parse_entry_datetime(entry)

            return Article(
                feed_name=feed_name,
                title=title,
                url=url,
                text=text,
                soup=soup,
                category=category,
                popularity_score=popularity,
                published_at=published_at,
            )

        except Exception as e:
            self.logger.error(
                f"è¨˜äº‹ {entry.get('link', 'ä¸æ˜')} ã®å–å¾—ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}"  # noqa: E501
            )
            return None

    async def _store_summaries(
        self, articles: list[Article], target_dates: list[date]
    ) -> list[tuple[str, str]]:
        if not articles:
            self.logger.info("ä¿å­˜ã™ã‚‹è¨˜äº‹ãŒã‚ã‚Šã¾ã›ã‚“")
            return []

        default_date = max(target_dates) if target_dates else datetime.now().date()
        incoming_records = self._serialize_articles(articles)
        records_by_date = group_records_by_date(
            incoming_records,
            default_date=default_date,
        )

        from nook.common.daily_snapshot import store_daily_snapshots

        saved_files = await store_daily_snapshots(
            records_by_date,
            load_existing=self._load_existing_articles,
            save_json=self.save_json,
            save_markdown=self.save_markdown,
            render_markdown=self._render_markdown,
            key=lambda item: item.get("title", ""),
            sort_key=self._article_sort_key,
            limit=self.TOTAL_LIMIT,
            logger=None,
        )

        return saved_files

    # ========================================
    # æŠ½è±¡ãƒ¡ã‚½ãƒƒãƒ‰ã®å®Ÿè£…
    # ========================================

    def _extract_popularity(self, entry, soup: BeautifulSoup) -> float:
        """è¨˜äº‹ã®äººæ°—æŒ‡æ¨™ã‚’æŠ½å‡ºã—ã¾ã™ã€‚"""
        # 1. ãƒ¡ã‚¿ã‚¿ã‚°ã‹ã‚‰æŠ½å‡º
        meta_tags = [
            ("property", "article:reaction_count"),
            ("name", "reaction-count"),
            ("property", "og:reaction_count"),
        ]

        for attr, value in meta_tags:
            meta = soup.find("meta", attrs={attr: value})
            if meta and meta.get("content"):
                parsed = self._safe_parse_int(meta.get("content"))
                if parsed is not None:
                    return float(parsed)

        candidates: list[int] = []

        # 2. dataå±æ€§ã‚’æŒã¤è¦ç´ 
        for element in soup.select("[data-reaction-count], [data-like-count]"):
            value = element.get("data-reaction-count") or element.get("data-like-count")
            parsed = self._safe_parse_int(value)
            if parsed is not None:
                candidates.append(parsed)

        # 3. ãƒœã‚¿ãƒ³ã‚„ã‚¹ãƒ‘ãƒ³ã®ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰æŠ½å‡º
        for selector in ["button", "span", "div"]:
            for element in soup.select(selector):
                text = element.get_text(strip=True)
                if any(keyword in text for keyword in ["ã„ã„ã­", "Like", "Reaction"]):
                    parsed = self._safe_parse_int(text)
                    if parsed is not None:
                        candidates.append(parsed)

        if candidates:
            return float(max(candidates))

        return 0.0

    def _get_markdown_header(self) -> str:
        """Markdownãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ˜ãƒƒãƒ€ãƒ¼ãƒ†ã‚­ã‚¹ãƒˆã‚’è¿”ã—ã¾ã™ã€‚"""
        return "æŠ€è¡“ãƒ‹ãƒ¥ãƒ¼ã‚¹è¨˜äº‹"

    def _get_summary_system_instruction(self) -> str:
        """è¦ç´„ç”Ÿæˆç”¨ã®ã‚·ã‚¹ãƒ†ãƒ ã‚¤ãƒ³ã‚¹ãƒˆãƒ©ã‚¯ã‚·ãƒ§ãƒ³ã‚’è¿”ã—ã¾ã™ã€‚"""
        return """
        ã‚ãªãŸã¯æŠ€è¡“ãƒ‹ãƒ¥ãƒ¼ã‚¹ã®è¦ç´„ã‚’è¡Œã†ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚
        ä¸ãˆã‚‰ã‚ŒãŸè¨˜äº‹ã‚’åˆ†æã—ã€ç°¡æ½”ã§æƒ…å ±é‡ã®å¤šã„è¦ç´„ã‚’ä½œæˆã—ã¦ãã ã•ã„ã€‚
        æŠ€è¡“çš„ãªå†…å®¹ã¯æ­£ç¢ºã«ã€ä¸€èˆ¬çš„ãªå†…å®¹ã¯åˆ†ã‹ã‚Šã‚„ã™ãè¦ç´„ã—ã¦ãã ã•ã„ã€‚
        å›ç­”ã¯å¿…ãšæ—¥æœ¬èªã§è¡Œã£ã¦ãã ã•ã„ã€‚
        """

    def _get_summary_prompt_template(self, article: Article) -> str:
        """è¦ç´„ç”Ÿæˆç”¨ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã‚’è¿”ã—ã¾ã™ã€‚"""
        return f"""
        ä»¥ä¸‹ã®æŠ€è¡“ãƒ‹ãƒ¥ãƒ¼ã‚¹è¨˜äº‹ã‚’è¦ç´„ã—ã¦ãã ã•ã„ã€‚

        ã‚¿ã‚¤ãƒˆãƒ«: {article.title}
        æœ¬æ–‡: {article.text[:2000]}

        è¦ç´„ã¯ä»¥ä¸‹ã®å½¢å¼ã§è¡Œã„ã€æ—¥æœ¬èªã§å›ç­”ã—ã¦ãã ã•ã„:
        1. è¨˜äº‹ã®ä¸»ãªå†…å®¹ï¼ˆ1-2æ–‡ï¼‰
        2. æŠ€è¡“çš„ãªé‡è¦ãƒã‚¤ãƒ³ãƒˆï¼ˆç®‡æ¡æ›¸ã3-5ç‚¹ï¼‰
        3. é–‹ç™ºè€…ã‚„æ¥­ç•Œã¸ã®å½±éŸ¿
        """

    def _needs_japanese_check(self) -> bool:
        """æ—¥æœ¬èªåˆ¤å®šãŒå¿…è¦ï¼ˆtech_feedç‰¹æœ‰ï¼‰ã€‚"""
        return True
