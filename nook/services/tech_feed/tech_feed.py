"""ÊäÄË°ì„Éã„É•„Éº„Çπ„ÅÆRSS„Éï„Ç£„Éº„Éâ„ÇíÁõ£Ë¶ñ„ÉªÂèéÈõÜ„ÉªË¶ÅÁ¥Ñ„Åô„Çã„Çµ„Éº„Éì„Çπ„ÄÇ"""

import asyncio
import json
import re
from dataclasses import dataclass, field
from datetime import date, datetime, time, timedelta, timezone
from pathlib import Path

import feedparser
import tomli
from bs4 import BeautifulSoup

from nook.common.base_service import BaseService
from nook.common.daily_snapshot import group_records_by_date, store_daily_snapshots
from nook.common.daily_merge import merge_records
from nook.common.date_utils import is_within_target_dates, target_dates_set
from nook.common.dedup import DedupTracker, load_existing_titles_from_storage
from nook.common.feed_utils import parse_entry_datetime


@dataclass
class Article:
    """
    ÊäÄË°ì„Éã„É•„Éº„Çπ„ÅÆË®ò‰∫ãÊÉÖÂ†±„ÄÇ

    Parameters
    ----------
    feed_name : str
        „Éï„Ç£„Éº„ÉâÂêç„ÄÇ
    title : str
        „Çø„Ç§„Éà„É´„ÄÇ
    url : str
        URL„ÄÇ
    text : str
        Êú¨Êñá„ÄÇ
    soup : BeautifulSoup
        BeautifulSoup„Ç™„Éñ„Ç∏„Çß„ÇØ„Éà„ÄÇ
    category : str | None
        „Ç´„ÉÜ„Ç¥„É™„ÄÇ
    """

    feed_name: str
    title: str
    url: str
    text: str
    soup: BeautifulSoup
    category: str | None = None
    summary: str = field(default="")
    popularity_score: float = field(default=0.0)
    published_at: datetime | None = None


class TechFeed(BaseService):
    """
    ÊäÄË°ì„Éã„É•„Éº„Çπ„ÅÆRSS„Éï„Ç£„Éº„Éâ„ÇíÁõ£Ë¶ñ„ÉªÂèéÈõÜ„ÉªË¶ÅÁ¥Ñ„Åô„Çã„ÇØ„É©„Çπ„ÄÇ

    Parameters
    ----------
    storage_dir : str, default="data"
        „Çπ„Éà„É¨„Éº„Ç∏„Éá„Ç£„É¨„ÇØ„Éà„É™„ÅÆ„Éë„Çπ„ÄÇ
    """

    TOTAL_LIMIT = 15

    def __init__(self, storage_dir: str = "data"):
        """
        TechFeed„ÇíÂàùÊúüÂåñ„Åó„Åæ„Åô„ÄÇ

        Parameters
        ----------
        storage_dir : str, default="data"
            „Çπ„Éà„É¨„Éº„Ç∏„Éá„Ç£„É¨„ÇØ„Éà„É™„ÅÆ„Éë„Çπ„ÄÇ
        """
        super().__init__("tech_feed")
        self.http_client = None  # setup_http_client„ÅßÂàùÊúüÂåñ

        # „Éï„Ç£„Éº„Éâ„ÅÆË®≠ÂÆö„ÇíË™≠„ÅøËæº„ÇÄ
        script_dir = Path(__file__).parent
        with open(script_dir / "feed.toml", "rb") as f:
            self.feed_config = tomli.load(f)

    def run(self, days: int = 1, limit: int | None = None) -> None:
        """
        ÊäÄË°ì„Éã„É•„Éº„Çπ„ÅÆRSS„Éï„Ç£„Éº„Éâ„ÇíÁõ£Ë¶ñ„ÉªÂèéÈõÜ„ÉªË¶ÅÁ¥Ñ„Åó„Å¶‰øùÂ≠ò„Åó„Åæ„Åô„ÄÇ

        Parameters
        ----------
        days : int, default=1
            ‰ΩïÊó•Ââç„Åæ„Åß„ÅÆË®ò‰∫ã„ÇíÂèñÂæó„Åô„Çã„Åã„ÄÇ
        limit : Optional[int], default=None
            ÂêÑ„Éï„Ç£„Éº„Éâ„Åã„ÇâÂèñÂæó„Åô„ÇãË®ò‰∫ãÊï∞„ÄÇNone„ÅÆÂ†¥Âêà„ÅØÂà∂Èôê„Å™„Åó„ÄÇ
        """
        asyncio.run(self.collect(days, limit))

    async def collect(
        self,
        days: int = 1,
        limit: int | None = None,
        *,
        target_dates: set[date] | None = None,
    ) -> list[tuple[str, str]]:
        """
        ÊäÄË°ì„Éã„É•„Éº„Çπ„ÅÆRSS„Éï„Ç£„Éº„Éâ„ÇíÁõ£Ë¶ñ„ÉªÂèéÈõÜ„ÉªË¶ÅÁ¥Ñ„Åó„Å¶‰øùÂ≠ò„Åó„Åæ„ÅôÔºàÈùûÂêåÊúüÁâàÔºâ„ÄÇ

        Parameters
        ----------
        days : int, default=1
            ‰ΩïÊó•Ââç„Åæ„Åß„ÅÆË®ò‰∫ã„ÇíÂèñÂæó„Åô„Çã„Åã„ÄÇ
        limit : Optional[int], default=None
            ÂêÑ„Éï„Ç£„Éº„Éâ„Åã„ÇâÂèñÂæó„Åô„ÇãË®ò‰∫ãÊï∞„ÄÇNone„ÅÆÂ†¥Âêà„ÅØÂà∂Èôê„Å™„Åó„ÄÇ

        Returns
        -------
        list[tuple[str, str]]
            ‰øùÂ≠ò„Åï„Çå„Åü„Éï„Ç°„Ç§„É´„Éë„Çπ„ÅÆ„É™„Çπ„Éà [(json_path, md_path), ...]
        """
        total_limit = self.TOTAL_LIMIT
        effective_target_dates = target_dates or target_dates_set(days)
        # HTTP„ÇØ„É©„Ç§„Ç¢„É≥„Éà„ÅÆÂàùÊúüÂåñ„ÇíÁ¢∫Ë™ç
        if self.http_client is None:
            await self.setup_http_client()

        candidate_articles: list[Article] = []

        # „Ç´„ÉÜ„Ç¥„É™Ê®™Êñ≠„ÅÆ„Çø„Ç§„Éà„É´ÈáçË§á„ÉÅ„Çß„ÉÉ„ÇØÁî®ÔºàÊó¢Â≠ò„Éï„Ç°„Ç§„É´„Åã„Çâ„É≠„Éº„ÉâÔºâ
        dedup_tracker = await load_existing_titles_from_storage(
            self.storage, effective_target_dates, self.logger
        )

        try:
            # ÂêÑ„Ç´„ÉÜ„Ç¥„É™„ÅÆ„Éï„Ç£„Éº„Éâ„Åã„ÇâË®ò‰∫ã„ÇíÂèñÂæó
            for category, feeds in self.feed_config.items():
                self.logger.info(f"„Ç´„ÉÜ„Ç¥„É™ {category} „ÅÆÂá¶ÁêÜ„ÇíÈñãÂßã„Åó„Åæ„Åô...")
                for feed_url in feeds:
                    try:
                        # „Éï„Ç£„Éº„Éâ„ÇíËß£Êûê
                        self.logger.info(f"„Éï„Ç£„Éº„Éâ {feed_url} „ÇíËß£Êûê„Åó„Å¶„ÅÑ„Åæ„Åô...")
                        feed = feedparser.parse(feed_url)
                        feed_name = (
                            feed.feed.title
                            if hasattr(feed, "feed") and hasattr(feed.feed, "title")
                            else feed_url
                        )

                        # Êñ∞„Åó„ÅÑ„Ç®„É≥„Éà„É™„Çí„Éï„Ç£„É´„Çø„É™„É≥„Ç∞
                        effective_limit = limit
                        if effective_limit is not None:
                            effective_limit = effective_limit * max(days, 1)

                        entries = self._filter_entries(
                            feed.entries,
                            effective_target_dates,
                            effective_limit,
                        )
                        self.logger.info(
                            f"„Éï„Ç£„Éº„Éâ {feed_name} „Åã„Çâ {len(entries)} ‰ª∂„ÅÆ„Ç®„É≥„Éà„É™„ÇíÂèñÂæó„Åó„Åæ„Åó„Åü"
                        )

                        for entry in entries:
                            entry_title = (
                                entry.title if hasattr(entry, "title") else "ÁÑ°È°å"
                            )

                            is_dup, normalized = dedup_tracker.is_duplicate(entry_title)
                            if is_dup:
                                original = (
                                    dedup_tracker.get_original_title(normalized)
                                    or entry_title
                                )
                                self.logger.info(
                                    "ÈáçË§áË®ò‰∫ã„Çí„Çπ„Ç≠„ÉÉ„Éó: '%s' (ÂàùÂá∫: '%s')",
                                    entry_title,
                                    original,
                                )
                                continue

                            article = await self._retrieve_article(
                                entry, feed_name, category
                            )
                            if article:
                                if not is_within_target_dates(
                                    article.published_at, effective_target_dates
                                ):
                                    continue
                                dedup_tracker.add(article.title)
                                candidate_articles.append(article)

                    except Exception as e:
                        self.logger.error(f"Error processing feed {feed_url}: {str(e)}")

            self.logger.info(
                f"ÂêàË®à {len(candidate_articles)} ‰ª∂„ÅÆË®ò‰∫ãÂÄôË£ú„ÇíÂèñÂæó„Åó„Åæ„Åó„Åü"
            )

            # Êó•‰ªò„Åî„Å®„Å´„Ç∞„É´„Éº„ÉóÂåñ
            articles_by_date = self._group_articles_by_date(candidate_articles)

            # Êó•‰ªò„Åî„Å®„Å´‰∏ä‰ΩçN‰ª∂„ÇíÈÅ∏Êäû„Åó„Å¶Ë¶ÅÁ¥ÑÔºàÂè§„ÅÑÊó•‰ªò„Åã„ÇâÊñ∞„Åó„ÅÑÊó•‰ªò„Å∏Ôºâ
            saved_files: list[tuple[str, str]] = []
            for date_str in sorted(articles_by_date.keys()):
                date_articles = articles_by_date[date_str]

                # „Åù„ÅÆÊó•„ÅÆÊó¢Â≠òË®ò‰∫ã„Çø„Ç§„Éà„É´„ÇíÂèñÂæó
                existing_titles_for_date = set()
                try:
                    json_content = await self.storage.load(f"{date_str}.json")
                    if json_content:
                        existing_articles = json.loads(json_content)
                        existing_titles_for_date = {
                            article.get("title", "") for article in existing_articles
                        }
                except Exception as e:
                    # „Éï„Ç°„Ç§„É´„ÅåÂ≠òÂú®„Åó„Å™„ÅÑÂ†¥Âêà„ÅØÁ©∫„ÅÆ„Çª„ÉÉ„Éà
                    self.logger.debug(
                        f"Êó¢Â≠òË®ò‰∫ã„Éï„Ç°„Ç§„É´ {date_str}.json „ÅÆË™≠„ÅøËæº„Åø„Å´Â§±Êïó„Åó„Åæ„Åó„Åü: {e}"
                    )

                # Êó¢Â≠ò/Êñ∞Ë¶èË®ò‰∫ãÊï∞„Çí„Ç´„Ç¶„É≥„Éà
                existing_count = len(existing_titles_for_date)
                new_count = len(date_articles)

                # Êó•‰ªòÊÉÖÂ†±„ÇíÂÖàÈ†≠„Å´Ë°®Á§∫
                self.logger.info(
                    f"\nüì∞ [{date_str}] „ÅÆË®ò‰∫ã„ÇíÂá¶ÁêÜ‰∏≠... (Êó¢Â≠ò: {existing_count}‰ª∂, Êñ∞Ë¶è: {new_count}‰ª∂)"
                )

                # Êñ∞Ë¶èË®ò‰∫ã„ÅÆ„Åø„ÇíË¶ÅÁ¥ÑÂØæË±°„Å®„Åó„Å¶ÈÅ∏Êäû
                selected = self._select_top_articles(date_articles, total_limit)

                if selected:
                    self.logger.info(f"   ‚úÖ ÈÅ∏Êäû: {len(selected)}‰ª∂")
                    for article in selected:
                        await self._summarize_article(article)

                    # „Åì„ÅÆÊó•‰ªò„ÅÆË®ò‰∫ã„Çí„Åô„Åê„Å´‰øùÂ≠ò
                    json_path, md_path = await self._store_summaries_for_date(
                        selected, date_str
                    )
                    self.logger.info(f"   üíæ ‰øùÂ≠òÂÆå‰∫Ü: {json_path}, {md_path}")
                    saved_files.append((json_path, md_path))
                else:
                    self.logger.info(f"   ‚ÑπÔ∏è  Êñ∞Ë¶èË®ò‰∫ã„Åå„ÅÇ„Çä„Åæ„Åõ„Çì")

            # Âá¶ÁêÜÂÆå‰∫Ü„É°„ÉÉ„Çª„Éº„Ç∏
            if saved_files:
                self.logger.info(f"\nüíæ {len(saved_files)}Êó•ÂàÜ„ÅÆ„Éá„Éº„Çø„Çí‰øùÂ≠òÂÆå‰∫Ü")
            else:
                self.logger.info("\n‰øùÂ≠ò„Åô„ÇãË®ò‰∫ã„Åå„ÅÇ„Çä„Åæ„Åõ„Çì")

            return saved_files

        finally:
            # „Ç∞„É≠„Éº„Éê„É´„ÇØ„É©„Ç§„Ç¢„É≥„Éà„Å™„ÅÆ„Åß„ÇØ„É≠„Éº„Ç∫‰∏çË¶Å
            pass

    def _group_articles_by_date(
        self, articles: list[Article]
    ) -> dict[str, list[Article]]:
        """Ë®ò‰∫ã„ÇíÊó•‰ªò„Åî„Å®„Å´„Ç∞„É´„Éº„ÉóÂåñ„Åó„Åæ„Åô„ÄÇ"""
        by_date: dict[str, list[Article]] = {}
        default_date = datetime.now().strftime("%Y-%m-%d")

        for article in articles:
            date_key = (
                article.published_at.strftime("%Y-%m-%d")
                if article.published_at
                else default_date
            )
            by_date.setdefault(date_key, []).append(article)

        return by_date

    def _filter_entries(
        self, entries: list[dict], target_dates: set[date], limit: int | None
    ) -> list[dict]:
        """
        Êñ∞„Åó„ÅÑ„Ç®„É≥„Éà„É™„Çí„Éï„Ç£„É´„Çø„É™„É≥„Ç∞„Åó„Åæ„Åô„ÄÇ

        Parameters
        ----------
        entries : List[dict]
            „Ç®„É≥„Éà„É™„ÅÆ„É™„Çπ„Éà„ÄÇ
        days : int
            ‰ΩïÊó•Ââç„Åæ„Åß„ÅÆË®ò‰∫ã„ÇíÂèñÂæó„Åô„Çã„Åã„ÄÇ
        limit : Optional[int]
            ÂèñÂæó„Åô„ÇãË®ò‰∫ãÊï∞„ÄÇNone„ÅÆÂ†¥Âêà„ÅØÂà∂Èôê„Å™„Åó„ÄÇ

        Returns
        -------
        List[dict]
            „Éï„Ç£„É´„Çø„É™„É≥„Ç∞„Åï„Çå„Åü„Ç®„É≥„Éà„É™„ÅÆ„É™„Çπ„Éà„ÄÇ
        """
        self.logger.info(f"„Ç®„É≥„Éà„É™„ÅÆ„Éï„Ç£„É´„Çø„É™„É≥„Ç∞„ÇíÈñãÂßã„Åó„Åæ„Åô({len(entries)}‰ª∂)...")

        # Êó•‰ªò„Åß„Éï„Ç£„É´„Çø„É™„É≥„Ç∞
        recent_entries = []

        for entry in entries:
            entry_date = parse_entry_datetime(entry)

            if entry_date:
                if is_within_target_dates(entry_date, target_dates):
                    recent_entries.append(entry)
                else:
                    self.logger.debug(
                        "ÂØæË±°Â§ñÊó•‰ªò„ÅÆË®ò‰∫ã„Çí„Çπ„Ç≠„ÉÉ„Éó„Åó„Åæ„Åô„ÄÇ raw=%s",
                        getattr(entry, "published", getattr(entry, "updated", "")),
                    )
            else:
                self.logger.debug(
                    "„Ç®„É≥„Éà„É™„Å´Êó•‰ªòÊÉÖÂ†±„Åå„ÅÇ„Çä„Åæ„Åõ„Çì„ÄÇÂê´„ÇÅ„Åæ„Åô„ÄÇ raw=%s",
                    getattr(entry, "published", getattr(entry, "updated", "")),
                )
                recent_entries.append(entry)

        self.logger.info(f"„Éï„Ç£„É´„Çø„É™„É≥„Ç∞Âæå„ÅÆ„Ç®„É≥„Éà„É™Êï∞: {len(recent_entries)}")

        if limit is None:
            return recent_entries

        return recent_entries[:limit]

    async def _retrieve_article(
        self, entry: dict, feed_name: str, category: str
    ) -> Article | None:
        """
        Ë®ò‰∫ã„ÇíÂèñÂæó„Åó„Åæ„Åô„ÄÇ

        Parameters
        ----------
        entry : dict
            „Ç®„É≥„Éà„É™ÊÉÖÂ†±„ÄÇ
        feed_name : str
            „Éï„Ç£„Éº„ÉâÂêç„ÄÇ
        category : str
            „Ç´„ÉÜ„Ç¥„É™„ÄÇ

        Returns
        -------
        Article or None
            ÂèñÂæó„Åó„ÅüË®ò‰∫ã„ÄÇÂèñÂæó„Å´Â§±Êïó„Åó„ÅüÂ†¥Âêà„ÅØNone„ÄÇ
        """
        try:
            # URL„ÇíÂèñÂæó
            url = entry.link if hasattr(entry, "link") else None
            if not url:
                return None

            # „Çø„Ç§„Éà„É´„ÇíÂèñÂæó
            title = entry.title if hasattr(entry, "title") else "ÁÑ°È°å"

            # Ë®ò‰∫ã„ÅÆÂÜÖÂÆπ„ÇíÂèñÂæó
            response = await self.http_client.get(url)
            soup = BeautifulSoup(response.text, "html.parser")

            # Êó•Êú¨Ë™ûË®ò‰∫ã„Åã„Å©„ÅÜ„Åã„ÇíÂà§ÂÆö
            is_japanese = self._detect_japanese_content(soup, title, entry)

            if not is_japanese:
                self.logger.debug(f"Êó•Êú¨Ë™û„Åß„Å™„ÅÑË®ò‰∫ã„Çí„Çπ„Ç≠„ÉÉ„Éó„Åó„Åæ„Åô: {title}")
                return None

            # Êú¨Êñá„ÇíÊäΩÂá∫
            text = ""

            # „Åæ„Åö„ÅØ„Ç®„É≥„Éà„É™„ÅÆË¶ÅÁ¥Ñ„Çí‰ΩøÁî®
            if hasattr(entry, "summary"):
                text = entry.summary

            # Ê¨°„Å´Ë®ò‰∫ã„ÅÆÊú¨Êñá„ÇíÊäΩÂá∫
            if not text:
                # „É°„Çø„Éá„Ç£„Çπ„ÇØ„É™„Éó„Ç∑„Éß„É≥„ÇíÂèñÂæó
                meta_desc = soup.find("meta", attrs={"name": "description"})
                if meta_desc and meta_desc.get("content"):
                    text = meta_desc.get("content")
                else:
                    # Êú¨Êñá„ÅÆÊúÄÂàù„ÅÆÊÆµËêΩ„ÇíÂèñÂæó
                    paragraphs = soup.find_all("p")
                    if paragraphs:
                        text = "\n".join([p.get_text() for p in paragraphs[:5]])

            published_at = parse_entry_datetime(entry)
            popularity_score = self._extract_popularity(entry, soup)

            return Article(
                feed_name=feed_name,
                title=title,
                url=url,
                text=text,
                soup=soup,
                category=category,
                popularity_score=popularity_score,
                published_at=published_at,
            )

        except Exception as e:
            self.logger.error(
                f"Error retrieving article {entry.get('link', 'unknown')}: {str(e)}"
            )
            return None

    def _load_existing_titles(self) -> DedupTracker:
        tracker = DedupTracker()
        try:
            content = self.storage.load_markdown("", datetime.now())
            if content:
                for match in re.finditer(r"^### \[(.+?)\]", content, re.MULTILINE):
                    tracker.add(match.group(1))
        except Exception as exc:
            self.logger.debug(f"Êó¢Â≠ò„Çø„Ç§„Éà„É´„ÅÆË™≠„ÅøËæº„Åø„Å´Â§±Êïó„Åó„Åæ„Åó„Åü: {exc}")
        return tracker

    def _extract_popularity(self, entry, soup: BeautifulSoup) -> float:
        candidates: list[int] = []

        for attr in [
            "slash_comments",
            "comments",
            "engagement",
            "likes",
            "favorites",
        ]:
            value = getattr(entry, attr, None)
            if value is None and isinstance(entry, dict):
                value = entry.get(attr)
            parsed = self._safe_parse_int(value)
            if parsed is not None:
                candidates.append(parsed)

        for meta_name in [
            "og:likes",
            "og:rating",
            "twitter:data1",
            "likes",
            "favorites",
        ]:
            meta_tag = soup.find("meta", attrs={"name": meta_name}) or soup.find(
                "meta", attrs={"property": meta_name}
            )
            if meta_tag and meta_tag.get("content"):
                parsed = self._safe_parse_int(meta_tag.get("content"))
                if parsed is not None:
                    candidates.append(parsed)

        for attr in ["data-like-count", "data-favorite-count", "data-score"]:
            for element in soup.select(f"[{attr}]"):
                parsed = self._safe_parse_int(element.get(attr))
                if parsed is not None:
                    candidates.append(parsed)

        if candidates:
            return float(max(candidates))

        published = parse_entry_datetime(entry)
        if published:
            return published.timestamp()

        return 0.0

    def _safe_parse_int(self, value) -> int | None:
        if value is None:
            return None
        if isinstance(value, (int, float)):
            return int(value)
        if isinstance(value, str):
            cleaned = value.replace(",", "")
            digits = ""
            for char in cleaned:
                if char.isdigit() or (char == "-" and not digits):
                    digits += char
                elif digits:
                    break
            if digits:
                try:
                    return int(digits)
                except ValueError:
                    return None
        return None

    def _select_top_articles(
        self, articles: list[Article], limit: int
    ) -> list[Article]:
        if not articles:
            return []

        if len(articles) <= limit:
            return articles

        def sort_key(article: Article):
            published = article.published_at or datetime.min
            return (article.popularity_score, published)

        sorted_articles = sorted(articles, key=sort_key, reverse=True)
        return sorted_articles[:limit]

    def _detect_japanese_content(self, soup, title, entry) -> bool:
        """
        Ë®ò‰∫ã„ÅåÊó•Êú¨Ë™û„Åß„ÅÇ„Çã„Åã„Å©„ÅÜ„Åã„ÇíÂà§ÂÆö„Åó„Åæ„Åô„ÄÇ

        Parameters
        ----------
        soup : BeautifulSoup
            Ë®ò‰∫ã„ÅÆHTML„Éë„Éº„Çµ„Éº„ÄÇ
        title : str
            Ë®ò‰∫ã„ÅÆ„Çø„Ç§„Éà„É´„ÄÇ
        entry : dict
            „Ç®„É≥„Éà„É™ÊÉÖÂ†±„ÄÇ

        Returns
        -------
        bool
            Êó•Êú¨Ë™ûË®ò‰∫ã„Åß„ÅÇ„Çå„Å∞True„ÄÅ„Åù„ÅÜ„Åß„Å™„Åë„Çå„Å∞False„ÄÇ
        """
        # ÊñπÊ≥ï1: HTML„ÅÆlangÂ±ûÊÄß„Çí„ÉÅ„Çß„ÉÉ„ÇØ
        html_tag = soup.find("html")
        if html_tag and html_tag.get("lang"):
            lang = html_tag.get("lang").lower()
            if lang.startswith("ja") or lang == "jp":
                return True

        # ÊñπÊ≥ï2: meta „Çø„Ç∞„ÅÆË®ÄË™ûÊÉÖÂ†±„Çí„ÉÅ„Çß„ÉÉ„ÇØ
        meta_lang = soup.find("meta", attrs={"http-equiv": "content-language"})
        if meta_lang and meta_lang.get("content"):
            if meta_lang.get("content").lower().startswith("ja"):
                return True

        # ÊñπÊ≥ï3: Êó•Êú¨Ë™û„ÅÆÊñáÂ≠ó„Ç≥„Éº„Éâ„Éë„Çø„Éº„É≥„Çí„ÉÅ„Çß„ÉÉ„ÇØ
        # „Å≤„Çâ„Åå„Å™„ÄÅ„Ç´„Çø„Ç´„Éä„ÄÅÊº¢Â≠ó„ÅÆÊñáÂ≠ó„Ç≥„Éº„ÉâÁØÑÂõ≤
        hiragana_pattern = range(0x3040, 0x309F)
        katakana_pattern = range(0x30A0, 0x30FF)
        kanji_pattern = range(0x4E00, 0x9FBF)

        # „Çø„Ç§„Éà„É´„Çí„ÉÅ„Çß„ÉÉ„ÇØ
        japanese_chars_count = 0
        for char in title:
            code = ord(char)
            if (
                code in hiragana_pattern
                or code in katakana_pattern
                or code in kanji_pattern
            ):
                japanese_chars_count += 1

        if japanese_chars_count > 2:  # Ë§áÊï∞„ÅÆÊó•Êú¨Ë™ûÊñáÂ≠ó„Åå„ÅÇ„Çå„Å∞Êó•Êú¨Ë™û„Å®„Åø„Å™„Åô
            return True

        # ÊñπÊ≥ï4: „Çµ„Éû„É™„Éº„ÇÑ„Éá„Ç£„Çπ„ÇØ„É™„Éó„Ç∑„Éß„É≥„ÇÇ„ÉÅ„Çß„ÉÉ„ÇØ
        text_to_check = ""
        if hasattr(entry, "summary"):
            text_to_check += entry.summary

        meta_desc = soup.find("meta", attrs={"name": "description"})
        if meta_desc and meta_desc.get("content"):
            text_to_check += meta_desc.get("content")

        # ÊúÄÂàù„ÅÆÊÆµËêΩ„Çí„Çµ„É≥„Éó„É™„É≥„Ç∞
        paragraphs = soup.find_all("p")
        if paragraphs and len(paragraphs) > 0:
            text_to_check += paragraphs[0].get_text()

        # „Çµ„É≥„Éó„É™„É≥„Ç∞„Åó„Åü„ÉÜ„Ç≠„Çπ„Éà„ÅßÊó•Êú¨Ë™ûÊñáÂ≠ó„Çí„ÉÅ„Çß„ÉÉ„ÇØ
        japanese_chars_count = 0
        for char in text_to_check[:100]:  # ÊúÄÂàù„ÅÆ100ÊñáÂ≠ó„Å†„Åë„ÉÅ„Çß„ÉÉ„ÇØ
            code = ord(char)
            if (
                code in hiragana_pattern
                or code in katakana_pattern
                or code in kanji_pattern
            ):
                japanese_chars_count += 1

        if japanese_chars_count > 5:  # Ë§áÊï∞„ÅÆÊó•Êú¨Ë™ûÊñáÂ≠ó„Åå„ÅÇ„Çå„Å∞Êó•Êú¨Ë™û„Å®„Åø„Å™„Åô
            return True

        # ÊñπÊ≥ï5: ÁâπÂÆö„ÅÆÊó•Êú¨Ë™û„Çµ„Ç§„Éà„ÅÆ„Éâ„É°„Ç§„É≥„É™„Çπ„ÉàÔºà„Éê„ÉÉ„ÇØ„Ç¢„ÉÉ„Éó„Å®„Åó„Å¶Ôºâ
        japanese_domains = [
            "zenn.dev",
            "qiita.com",
            "gihyo.jp",
            "codezine.jp",
            "techplay.jp",
            "itmedia.co.jp",
            "atmarkit.co.jp",
        ]

        url = entry.link if hasattr(entry, "link") else ""
        for domain in japanese_domains:
            if domain in url:
                return True

        # „Éá„Éï„Ç©„É´„Éà„Åß„ÅØÈùûÊó•Êú¨Ë™û„Å®Âà§ÂÆö
        return False

    async def _summarize_article(self, article: Article) -> None:
        """
        Ë®ò‰∫ã„ÇíË¶ÅÁ¥Ñ„Åó„Åæ„Åô„ÄÇ

        Parameters
        ----------
        article : Article
            Ë¶ÅÁ¥Ñ„Åô„ÇãË®ò‰∫ã„ÄÇ
        """
        prompt = f"""
        ‰ª•‰∏ã„ÅÆÊäÄË°ì„Éã„É•„Éº„Çπ„ÅÆË®ò‰∫ã„ÇíË¶ÅÁ¥Ñ„Åó„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇ

        „Çø„Ç§„Éà„É´: {article.title}
        Êú¨Êñá: {article.text[:2000]}

        Ë¶ÅÁ¥Ñ„ÅØ‰ª•‰∏ã„ÅÆÂΩ¢Âºè„ÅßË°å„ÅÑ„ÄÅÊó•Êú¨Ë™û„ÅßÂõûÁ≠î„Åó„Å¶„Åè„Å†„Åï„ÅÑ:
        1. Ë®ò‰∫ã„ÅÆ‰∏ª„Å™ÂÜÖÂÆπÔºà1-2ÊñáÔºâ
        2. ÈáçË¶Å„Å™„Éù„Ç§„É≥„ÉàÔºàÁÆáÊù°Êõ∏„Åç3-5ÁÇπÔºâ
        3. ÊäÄË°ìÁöÑ„Å™Ê¥ûÂØü
        """

        system_instruction = """
        „ÅÇ„Å™„Åü„ÅØÊäÄË°ì„Éã„É•„Éº„Çπ„ÅÆË®ò‰∫ã„ÅÆË¶ÅÁ¥Ñ„ÇíË°å„ÅÜ„Ç¢„Ç∑„Çπ„Çø„É≥„Éà„Åß„Åô„ÄÇ
        ‰∏é„Åà„Çâ„Çå„ÅüË®ò‰∫ã„ÇíÂàÜÊûê„Åó„ÄÅÁ∞°ÊΩî„ÅßÊÉÖÂ†±Èáè„ÅÆÂ§ö„ÅÑË¶ÅÁ¥Ñ„Çí‰ΩúÊàê„Åó„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇ
        ÊäÄË°ìÁöÑ„Å™ÂÜÖÂÆπ„ÅØÊ≠£Á¢∫„Å´„ÄÅ‰∏ÄËà¨ÁöÑ„Å™ÂÜÖÂÆπ„ÅØÂàÜ„Åã„Çä„ÇÑ„Åô„ÅèË¶ÅÁ¥Ñ„Åó„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇ
        ÂõûÁ≠î„ÅØÂøÖ„ÅöÊó•Êú¨Ë™û„ÅßË°å„Å£„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇ
        """

        try:
            summary = self.gpt_client.generate_content(
                prompt=prompt,
                system_instruction=system_instruction,
                temperature=0.3,
                max_tokens=1000,
            )
            article.summary = summary
        except Exception as e:
            self.logger.error(f"Ë¶ÅÁ¥Ñ„ÅÆÁîüÊàê‰∏≠„Å´„Ç®„É©„Éº„ÅåÁô∫Áîü„Åó„Åæ„Åó„Åü: {str(e)}")
            article.summary = f"Ë¶ÅÁ¥Ñ„ÅÆÁîüÊàê‰∏≠„Å´„Ç®„É©„Éº„ÅåÁô∫Áîü„Åó„Åæ„Åó„Åü: {str(e)}"

    async def _store_summaries_for_date(
        self, articles: list[Article], date_str: str
    ) -> tuple[str, str]:
        """
        Âçò‰∏ÄÊó•‰ªò„ÅÆË®ò‰∫ã„ÇíJSON„Å®Markdown„Éï„Ç°„Ç§„É´„Å´‰øùÂ≠ò„Åó„Åæ„Åô„ÄÇ

        Parameters
        ----------
        articles : list[Article]
            ‰øùÂ≠ò„Åô„ÇãË®ò‰∫ã„ÅÆ„É™„Çπ„Éà„ÄÇ
        date_str : str
            Êó•‰ªòÊñáÂ≠óÂàóÔºà"YYYY-MM-DD" ÂΩ¢ÂºèÔºâ„ÄÇ

        Returns
        -------
        tuple[str, str]
            ‰øùÂ≠ò„Åï„Çå„Åü„Éï„Ç°„Ç§„É´„Éë„Çπ„ÅÆÁµÑ„ÅøÂêà„Çè„Åõ (json_path, md_path)
        """
        if not articles:
            return ("", "")

        # Êó•‰ªò„Çídatetime„Å´Â§âÊèõ
        target_date = datetime.strptime(date_str, "%Y-%m-%d").date()
        snapshot_datetime = datetime.combine(target_date, time.min)

        # Ë®ò‰∫ã„Çí„Ç∑„É™„Ç¢„É©„Ç§„Ç∫
        records = self._serialize_articles(articles)

        # Êó¢Â≠òË®ò‰∫ã„ÇíË™≠„ÅøËæº„Çì„Åß„Éû„Éº„Ç∏
        existing = await self._load_existing_articles(snapshot_datetime)

        merged = merge_records(
            existing,
            records,
            key=lambda item: item.get("title", ""),
            sort_key=self._article_sort_key,
            limit=self.TOTAL_LIMIT,
            reverse=True,
        )

        # JSON„Éï„Ç°„Ç§„É´„Çí‰øùÂ≠ò
        filename_json = f"{date_str}.json"
        json_path = await self.save_json(merged, filename_json)

        # Markdown„Éï„Ç°„Ç§„É´„Çí‰øùÂ≠ò
        filename_md = f"{date_str}.md"
        markdown = self._render_markdown(merged, snapshot_datetime)
        md_path = await self.save_markdown(markdown, filename_md)

        return (str(json_path), str(md_path))

    async def _store_summaries(
        self, articles: list[Article], target_dates: set[date]
    ) -> list[tuple[str, str]]:
        """
        Ë¶ÅÁ¥Ñ„Çí‰øùÂ≠ò„Åó„Åæ„Åô„ÄÇ

        Parameters
        ----------
        articles : List[Article]
            ‰øùÂ≠ò„Åô„ÇãË®ò‰∫ã„ÅÆ„É™„Çπ„Éà„ÄÇ

        Returns
        -------
        list[tuple[str, str]]
            ‰øùÂ≠ò„Åï„Çå„Åü„Éï„Ç°„Ç§„É´„Éë„Çπ„ÅÆ„É™„Çπ„Éà [(json_path, md_path), ...]
        """
        if not articles:
            self.logger.info("‰øùÂ≠ò„Åô„ÇãË®ò‰∫ã„Åå„ÅÇ„Çä„Åæ„Åõ„Çì")
            return []

        default_date = max(target_dates) if target_dates else datetime.now().date()
        incoming_records = self._serialize_articles(articles)
        records_by_date = group_records_by_date(
            incoming_records,
            default_date=default_date,
        )

        saved_files = await store_daily_snapshots(
            records_by_date,
            load_existing=self._load_existing_articles,
            save_json=self.save_json,
            save_markdown=self.save_markdown,
            render_markdown=self._render_markdown,
            key=lambda item: item.get("title", ""),
            sort_key=self._article_sort_key,
            limit=self.TOTAL_LIMIT,
            logger=None,  # Êó•‰ªòÊÉÖÂ†±„ÅÆ‰∫åÈáçË°®Á§∫„ÇíÈò≤„Åê
        )

        return saved_files

    def _serialize_articles(self, articles: list[Article]) -> list[dict]:
        records: list[dict] = []
        for article in articles:
            category = article.category or "uncategorized"
            records.append(
                {
                    "title": article.title,
                    "url": article.url,
                    "feed_name": article.feed_name,
                    "summary": article.summary,
                    "popularity_score": article.popularity_score,
                    "published_at": (
                        article.published_at.isoformat()
                        if article.published_at
                        else None
                    ),
                    "category": category,
                }
            )
        return records

    async def _load_existing_articles(self, target_date: datetime) -> list[dict]:
        date_str = target_date.strftime("%Y-%m-%d")
        filename_json = f"{date_str}.json"
        filename_md = f"{date_str}.md"

        existing_json = await self.load_json(filename_json)
        if existing_json:
            return existing_json

        markdown = await self.storage.load(filename_md)
        if not markdown:
            return []

        return self._parse_markdown(markdown)

    def _article_sort_key(self, item: dict) -> tuple[float, datetime]:
        popularity = float(item.get("popularity_score", 0.0) or 0.0)
        published_raw = item.get("published_at")
        if published_raw:
            try:
                published = datetime.fromisoformat(published_raw)
            except ValueError:
                published = datetime.min
        else:
            published = datetime.min
        return (popularity, published)

    def _render_markdown(self, records: list[dict], today: datetime) -> str:
        content = f"# ÊäÄË°ì„Éã„É•„Éº„ÇπË®ò‰∫ã ({today.strftime('%Y-%m-%d')})\n\n"
        grouped: dict[str, list[dict]] = {}
        for record in records:
            category = record.get("category", "uncategorized")
            grouped.setdefault(category, []).append(record)

        for category, articles in grouped.items():
            heading = category.replace("_", " ").capitalize()
            content += f"## {heading}\n\n"
            for article in articles:
                content += f"### [{article['title']}]({article['url']})\n\n"
                content += f"**„Éï„Ç£„Éº„Éâ**: {article.get('feed_name', '')}\n\n"
                content += f"**Ë¶ÅÁ¥Ñ**:\n{article.get('summary', '')}\n\n"
                content += "---\n\n"
        return content

    def _parse_markdown(self, markdown: str) -> list[dict]:
        result: list[dict] = []
        category_pattern = re.compile(r"^##\s+(.+)$", re.MULTILINE)
        article_pattern = re.compile(
            r"### \[(?P<title>.+?)\]\((?P<url>[^\)]+)\)\n\n"
            r"\*\*„Éï„Ç£„Éº„Éâ\*\*: (?P<feed>.+?)\n\n"
            r"\*\*Ë¶ÅÁ¥Ñ\*\*:\n(?P<summary>.*?)(?:\n\n)?---",
            re.DOTALL,
        )

        sections = list(category_pattern.finditer(markdown))
        for idx, match in enumerate(sections):
            start = match.end()
            end = (
                sections[idx + 1].start() if idx + 1 < len(sections) else len(markdown)
            )
            block = markdown[start:end]
            category = match.group(1).strip().lower().replace(" ", "_")

            for article_match in article_pattern.finditer(block + "---"):
                result.append(
                    {
                        "title": article_match.group("title").strip(),
                        "url": article_match.group("url").strip(),
                        "feed_name": article_match.group("feed").strip(),
                        "summary": article_match.group("summary").strip(),
                        "popularity_score": 0.0,
                        "published_at": None,
                        "category": category,
                    }
                )

        return result
