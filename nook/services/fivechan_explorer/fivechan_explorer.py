import asyncio
import re
from dataclasses import dataclass, field
from datetime import UTC, date, datetime
from pathlib import Path
from typing import Any

import cloudscraper
from dateutil import parser

from nook.common.base_service import BaseService
from nook.common.daily_snapshot import group_records_by_date, store_daily_snapshots
from nook.common.date_utils import (
    is_within_target_dates,
    normalize_datetime_to_local,
    target_dates_set,
)
from nook.common.dedup import DedupTracker
from nook.common.gpt_client import GPTClient
from nook.common.logging_utils import (
    log_article_counts,
    log_no_new_articles,
    log_processing_start,
    log_storage_complete,
    log_summarization_progress,
    log_summarization_start,
    log_summary_candidates,
)
from nook.common.storage import LocalStorage


@dataclass
class Thread:
    """
    5chanã‚¹ãƒ¬ãƒƒãƒ‰æƒ…å ±ã€‚

    Parameters
    ----------
    thread_id : int
        ã‚¹ãƒ¬ãƒƒãƒ‰IDã€‚
    title : str
        ã‚¹ãƒ¬ãƒƒãƒ‰ã‚¿ã‚¤ãƒˆãƒ«ã€‚
    url : str
        ã‚¹ãƒ¬ãƒƒãƒ‰URLã€‚
    board : str
        æ¿åã€‚
    posts : List[Dict[str, Any]]
        æŠ•ç¨¿ãƒªã‚¹ãƒˆã€‚
    timestamp : int
        ä½œæˆã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ã€‚
    """

    thread_id: int
    title: str
    url: str
    board: str
    posts: list[dict[str, Any]]
    timestamp: int
    summary: str = field(default="")
    popularity_score: float = field(default=0.0)


class FiveChanExplorer(BaseService):
    """
    5chanï¼ˆæ—§2ã¡ã‚ƒã‚“ã­ã‚‹ï¼‰ã‹ã‚‰æƒ…å ±ã‚’åé›†ã™ã‚‹ã‚¯ãƒ©ã‚¹ã€‚

    Parameters
    ----------
    storage_dir : str, default="data"
        ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®ãƒ‘ã‚¹ã€‚
    """

    TOTAL_LIMIT = 15

    def __init__(self, storage_dir: str = "data"):
        """
        FiveChanExplorerã‚’åˆæœŸåŒ–ã—ã¾ã™ã€‚

        Parameters
        ----------
        storage_dir : str, default="data"
            ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®ãƒ‘ã‚¹ã€‚
        """
        super().__init__("fivechan_explorer")
        self.http_client = None  # setup_http_clientã§åˆæœŸåŒ–
        self.gpt_client = GPTClient()

        storage_path = Path(storage_dir)
        if storage_path.name != self.service_name:
            storage_path = storage_path / self.service_name
        self.storage = LocalStorage(str(storage_path))

        # å¯¾è±¡ã¨ãªã‚‹æ¿
        self.target_boards = self._load_boards()

        # è©¦ã™ã‚µãƒ–ãƒ‰ãƒ¡ã‚¤ãƒ³ã®ãƒªã‚¹ãƒˆï¼ˆã™ã¹ã¦ã®æ¿ã§è©¦ã™ï¼‰
        self.subdomains = [
            "mevius.5ch.net",
            "egg.5ch.net",
            "medaka.5ch.net",
            "hayabusa9.5ch.net",
            "mi.5ch.net",
            "lavender.5ch.net",
            "eagle.5ch.net",
            "rosie.5ch.net",
            "fate.5ch.net",
        ]

        # AIã«é–¢é€£ã™ã‚‹ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰
        self.ai_keywords = [
            "ai",
            "äººå·¥çŸ¥èƒ½",
            "æ©Ÿæ¢°å­¦ç¿’",
            "ãƒ‡ã‚£ãƒ¼ãƒ—ãƒ©ãƒ¼ãƒ‹ãƒ³ã‚°",
            "ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯",
            "gpt",
            "llm",
            "chatgpt",
            "claude",
            "gemini",
            "grok",
            "anthropic",
            "openai",
            "stable diffusion",
            "dalle",
            "midjourney",
            "è‡ªç„¶è¨€èªå‡¦ç†",
            "å¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«",
            "ãƒãƒ£ãƒƒãƒˆãƒœãƒƒãƒˆ",
            "å¯¾è©±å‹ai",
            "ç”Ÿæˆai",
            "ç”»åƒç”Ÿæˆ",
            "alphaã‚´ãƒ¼",
            "alphago",
            "deepmind",
            "å¼·åŒ–å­¦ç¿’",
            "è‡ªå·±å­¦ç¿’",
            "å¼·ã„äººå·¥çŸ¥èƒ½",
            "å¼±ã„äººå·¥çŸ¥èƒ½",
            "ç‰¹åŒ–å‹äººå·¥çŸ¥èƒ½",
            "pixai",
            "comfyui",
            "stablediffusion",
            "aiç”»åƒ",
            "aiå‹•ç”»",
        ]

        # æ”¹å–„ã•ã‚ŒãŸãƒªã‚¯ã‚¨ã‚¹ãƒˆåˆ¶å¾¡è¨­å®š
        self.min_request_delay = 5  # æœ€å°é…å»¶æ™‚é–“ï¼ˆç§’ï¼‰
        self.max_request_delay = 10  # æœ€å¤§é…å»¶æ™‚é–“ï¼ˆç§’ï¼‰
        self.request_delay = 2  # ä¸‹ä½äº’æ›æ€§ã®ãŸã‚ä¿æŒ

        # User-Agentãƒ­ãƒ¼ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ç”¨ã®ãƒªã‚¹ãƒˆ
        self.user_agents = [
            "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36",
            "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
            "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36",
            "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36",
            "Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:122.0) Gecko/20100101 Firefox/122.0",
            "Mozilla/5.0 (Macintosh; Intel Mac OS X 10.15; rv:122.0) Gecko/20100101 Firefox/122.0",
        ]

        # ãƒ–ãƒ©ã‚¦ã‚¶ãƒ˜ãƒƒãƒ€ãƒ¼ã®å®Œå…¨è¨­å®šï¼ˆUser-Agentã¯å‹•çš„ã«è¨­å®šï¼‰
        self.browser_headers = {
            "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8",
            "Accept-Language": "ja,en-US;q=0.9,en;q=0.8",
            "Accept-Encoding": "gzip, deflate, br",
            "Cache-Control": "no-cache",
            "Referer": "https://5ch.net/",
        }

    def _load_boards(self) -> dict[str, str]:
        """
        å¯¾è±¡ã¨ãªã‚‹æ¿ã®è¨­å®šã‚’èª­ã¿è¾¼ã¿ã¾ã™ã€‚

        Returns
        -------
        Dict[str, str]
            æ¿ã®ID: æ¿ã®åå‰ã®ãƒ‡ã‚£ã‚¯ã‚·ãƒ§ãƒŠãƒª
        """
        script_dir = Path(__file__).parent
        with open(script_dir / "boards.toml", "rb") as f:
            import tomli

            config = tomli.load(f)
            boards_config = config.get("boards", {})

            # æ–°ã—ã„å½¢å¼å¯¾å¿œ: {board_id: {name: "åå‰", server: "ã‚µãƒ¼ãƒãƒ¼"}}
            # æ—§å½¢å¼ã‚‚å¯¾å¿œ: {board_id: "åå‰"}
            boards = {}
            self.board_servers = {}  # ã‚µãƒ¼ãƒãƒ¼æƒ…å ±ã‚’ä¿å­˜

            for board_id, board_info in boards_config.items():
                if isinstance(board_info, dict):
                    # æ–°å½¢å¼: {name: "åå‰", server: "ã‚µãƒ¼ãƒãƒ¼"}
                    boards[board_id] = board_info.get("name", board_id)
                    self.board_servers[board_id] = board_info.get("server", "mevius.5ch.net")
                else:
                    # æ—§å½¢å¼: "åå‰"
                    boards[board_id] = board_info
                    self.board_servers[board_id] = "mevius.5ch.net"  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ

            return boards

    def _get_random_user_agent(self) -> str:
        """
        ãƒ©ãƒ³ãƒ€ãƒ ãªUser-Agentã‚’å–å¾—ã—ã¾ã™ã€‚

        Returns
        -------
        str
            ãƒ©ãƒ³ãƒ€ãƒ ã«é¸æŠã•ã‚ŒãŸUser-Agentæ–‡å­—åˆ—ã€‚
        """
        import random

        return random.choice(self.user_agents)

    def _calculate_backoff_delay(self, retry_count: int) -> float:
        """
        æŒ‡æ•°ãƒãƒƒã‚¯ã‚ªãƒ•ã«ã‚ˆã‚‹é…å»¶æ™‚é–“ã‚’è¨ˆç®—ã—ã¾ã™ã€‚

        Parameters
        ----------
        retry_count : int
            ãƒªãƒˆãƒ©ã‚¤å›æ•°ã€‚

        Returns
        -------
        float
            é…å»¶æ™‚é–“ï¼ˆç§’ï¼‰ã€‚
        """
        # åŸºæœ¬é…å»¶æ™‚é–“: 2^retry_countç§’ã€æœ€å¤§300ç§’
        base_delay = min(2**retry_count, 300)
        return base_delay

    async def _get_with_retry(self, url: str, max_retries: int = 3, **kwargs) -> any:
        """
        ãƒªãƒˆãƒ©ã‚¤æ©Ÿèƒ½ä»˜ãHTTP GETãƒªã‚¯ã‚¨ã‚¹ãƒˆã€‚

        Parameters
        ----------
        url : str
            ãƒªã‚¯ã‚¨ã‚¹ãƒˆURLã€‚
        max_retries : int, default=3
            æœ€å¤§ãƒªãƒˆãƒ©ã‚¤å›æ•°ã€‚

        Returns
        -------
        any
            HTTPãƒ¬ã‚¹ãƒãƒ³ã‚¹ã€‚
        """
        import asyncio

        for attempt in range(max_retries + 1):
            try:
                # å‹•çš„ãªUser-Agentã§ãƒ˜ãƒƒãƒ€ãƒ¼ã‚’æ›´æ–°
                headers = self.browser_headers.copy()
                headers["User-Agent"] = self._get_random_user_agent()

                response = await self.http_client.get(url, headers=headers, **kwargs)

                # æˆåŠŸãƒ¬ã‚¹ãƒãƒ³ã‚¹ï¼ˆ200ç•ªå°ï¼‰ã®å ´åˆã¯è¿”ã™
                if 200 <= response.status_code < 300:
                    return response

                # ãƒ¬ãƒ¼ãƒˆåˆ¶é™ã‚¨ãƒ©ãƒ¼ï¼ˆ429ï¼‰ã®å ´åˆ
                if response.status_code == 429:
                    retry_after = response.headers.get("Retry-After")
                    if retry_after:
                        wait_time = int(retry_after)
                    else:
                        wait_time = self._calculate_backoff_delay(attempt)

                    self.logger.warning(f"ãƒ¬ãƒ¼ãƒˆåˆ¶é™æ¤œçŸ¥ (429): {wait_time}ç§’å¾…æ©Ÿã—ã¾ã™")
                    await asyncio.sleep(wait_time)
                    continue

                # ã‚µãƒ¼ãƒãƒ¼ã‚¨ãƒ©ãƒ¼ï¼ˆ503ç­‰ï¼‰ã®å ´åˆ
                if response.status_code >= 500:
                    if attempt < max_retries:
                        wait_time = self._calculate_backoff_delay(attempt)
                        self.logger.warning(
                            f"ã‚µãƒ¼ãƒãƒ¼ã‚¨ãƒ©ãƒ¼ ({response.status_code}): {wait_time}ç§’å¾Œã«ãƒªãƒˆãƒ©ã‚¤ã—ã¾ã™"
                        )
                        await asyncio.sleep(wait_time)
                        continue

                # ãã®ä»–ã®ã‚¨ãƒ©ãƒ¼ã¯æœ€å¾Œã®è©¦è¡Œã®å ´åˆã¯è¿”ã™
                if attempt == max_retries:
                    return response

            except Exception as e:
                if attempt == max_retries:
                    raise e

                wait_time = self._calculate_backoff_delay(attempt)
                self.logger.warning(f"ãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚¨ãƒ©ãƒ¼: {e}, {wait_time}ç§’å¾Œã«ãƒªãƒˆãƒ©ã‚¤ã—ã¾ã™")
                await asyncio.sleep(wait_time)

        # ã“ã“ã«ã¯åˆ°é”ã—ãªã„ã¯ãšã§ã™ãŒã€å®‰å…¨ã®ãŸã‚
        return response

    def run(self, thread_limit: int | None = None) -> None:
        """
        5chanã‹ã‚‰AIé–¢é€£ã‚¹ãƒ¬ãƒƒãƒ‰ã‚’åé›†ã—ã¦ä¿å­˜ã—ã¾ã™ã€‚

        Parameters
        ----------
        thread_limit : Optional[int], default=None
            å„æ¿ã‹ã‚‰å–å¾—ã™ã‚‹ã‚¹ãƒ¬ãƒƒãƒ‰æ•°ã€‚Noneã®å ´åˆã¯åˆ¶é™ãªã—ã€‚
        """
        asyncio.run(self.collect(thread_limit))

    async def collect(
        self,
        thread_limit: int | None = None,
        *,
        target_dates: list[date] | None = None,
    ) -> list[tuple[str, str]]:
        """
        5chanã‹ã‚‰AIé–¢é€£ã‚¹ãƒ¬ãƒƒãƒ‰ã‚’åé›†ã—ã¦ä¿å­˜ã—ã¾ã™ï¼ˆéåŒæœŸç‰ˆï¼‰ã€‚

        Parameters
        ----------
        thread_limit : Optional[int], default=None
            å„æ¿ã‹ã‚‰å–å¾—ã™ã‚‹ã‚¹ãƒ¬ãƒƒãƒ‰æ•°ã€‚Noneã®å ´åˆã¯åˆ¶é™ãªã—ã€‚

        Returns
        -------
        list[tuple[str, str]]
            ä¿å­˜ã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ã®ãƒªã‚¹ãƒˆ [(json_path, md_path), ...]
        """
        total_limit = self.TOTAL_LIMIT
        effective_target_dates = target_dates or target_dates_set(1)

        # å¯¾è±¡æ—¥ä»˜ã®ãƒ­ã‚°å‡ºåŠ›
        date_str = max(effective_target_dates).strftime("%Y-%m-%d")
        log_processing_start(self.logger, date_str)

        # HTTPã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã®åˆæœŸåŒ–ã‚’ç¢ºèª
        if self.http_client is None:
            await self.setup_http_client()

        candidate_threads: list[Thread] = []
        selected_threads: list[Thread] = []
        dedup_tracker = self._load_existing_titles()

        try:
            # å„æ¿ã‹ã‚‰ã‚¹ãƒ¬ãƒƒãƒ‰ã‚’å–å¾—
            for board_id, board_name in self.target_boards.items():
                try:
                    self.logger.info(
                        f"æ¿ /{board_id}/({board_name}) ã‹ã‚‰ã®ã‚¹ãƒ¬ãƒƒãƒ‰å–å¾—ã‚’é–‹å§‹ã—ã¾ã™..."
                    )
                    threads = await self._retrieve_ai_threads(
                        board_id,
                        thread_limit,
                        dedup_tracker,
                        effective_target_dates,
                    )
                    self.logger.info(
                        f"æ¿ /{board_id}/({board_name}) ã‹ã‚‰ {len(threads)} ä»¶ã®ã‚¹ãƒ¬ãƒƒãƒ‰ã‚’å–å¾—ã—ã¾ã—ãŸ"
                    )

                    candidate_threads.extend(threads)

                    # æ”¹å–„ã•ã‚ŒãŸãƒªã‚¯ã‚¨ã‚¹ãƒˆé–“é…å»¶ï¼ˆãƒ©ãƒ³ãƒ€ãƒ åŒ–ï¼‰
                    import random

                    delay = random.uniform(self.min_request_delay, self.max_request_delay)
                    self.logger.debug(f"ãƒªã‚¯ã‚¨ã‚¹ãƒˆé–“é…å»¶: {delay:.1f}ç§’")

                    await asyncio.sleep(delay)

                except Exception as e:
                    self.logger.error(f"Error processing board /{board_id}/: {str(e)}")

            self.logger.info(f"åˆè¨ˆ {len(candidate_threads)} ä»¶ã®ã‚¹ãƒ¬ãƒƒãƒ‰å€™è£œã‚’å–å¾—ã—ã¾ã—ãŸ")

            # æ—¥ä»˜ã”ã¨ã«ã‚°ãƒ«ãƒ¼ãƒ—åŒ–ã—ã¦å„æ—¥ç‹¬ç«‹ã§ä¸Šä½15ä»¶ã‚’é¸æŠ
            threads_by_date = {}
            for thread in candidate_threads:
                thread_date = datetime.fromtimestamp(thread.timestamp).date()
                if thread_date not in threads_by_date:
                    threads_by_date[thread_date] = []
                threads_by_date[thread_date].append(thread)

            # å„æ—¥ç‹¬ç«‹ã§ä¸Šä½15ä»¶ã‚’é¸æŠã—ã¦çµåˆ
            selected_threads = []
            for target_date in sorted(effective_target_dates):
                if target_date in threads_by_date:
                    date_threads = threads_by_date[target_date]
                    if len(date_threads) <= total_limit:
                        selected_threads.extend(date_threads)
                    else:

                        def sort_key(thread: Thread):
                            created = datetime.fromtimestamp(thread.timestamp)
                            return (thread.popularity_score, created)

                        sorted_threads = sorted(date_threads, key=sort_key, reverse=True)
                        selected_threads.extend(sorted_threads[:total_limit])

            # æ—¢å­˜/æ–°è¦ã‚¹ãƒ¬ãƒƒãƒ‰æ•°ã‚’ã‚«ã‚¦ãƒ³ãƒˆ
            existing_count = 0  # æ—¢å­˜ã‚¹ãƒ¬ãƒƒãƒ‰æ•°ï¼ˆç°¡ç•¥åŒ–ï¼‰
            new_count = len(selected_threads)  # æ–°è¦ã‚¹ãƒ¬ãƒƒãƒ‰æ•°

            # ã‚¹ãƒ¬ãƒƒãƒ‰æƒ…å ±ã‚’è¡¨ç¤º
            log_article_counts(self.logger, existing_count, new_count)

            if selected_threads:
                log_summary_candidates(self.logger, selected_threads, "popularity_score")

                # è¦ç´„ç”Ÿæˆ
                log_summarization_start(self.logger)
                for idx, thread in enumerate(selected_threads, 1):
                    await self._summarize_thread(thread)
                    log_summarization_progress(
                        self.logger, idx, len(selected_threads), thread.title
                    )

            # è¦ç´„ã‚’ä¿å­˜
            saved_files: list[tuple[str, str]] = []
            if selected_threads:
                saved_files = await self._store_summaries(selected_threads, effective_target_dates)

                # å‡¦ç†å®Œäº†ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸
                if saved_files:
                    self.logger.info(f"\nğŸ’¾ {len(saved_files)}æ—¥åˆ†ã®ãƒ‡ãƒ¼ã‚¿ã‚’ä¿å­˜å®Œäº†")
                    for json_path, md_path in saved_files:
                        log_storage_complete(self.logger, json_path, md_path)
                else:
                    log_no_new_articles(self.logger)
            else:
                log_no_new_articles(self.logger)

            return saved_files

        finally:
            # ã‚°ãƒ­ãƒ¼ãƒãƒ«ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆãªã®ã§ã‚¯ãƒ­ãƒ¼ã‚ºä¸è¦
            pass

    def _build_board_url(self, board_id: str, server: str) -> str:
        """
        æ¿URLã‚’æ§‹ç¯‰ã—ã¾ã™ã€‚

        Parameters
        ----------
        board_id : str
            æ¿ã®IDã€‚
        server : str
            ã‚µãƒ¼ãƒãƒ¼ã®ãƒ›ã‚¹ãƒˆåã€‚

        Returns
        -------
        str
            æ§‹ç¯‰ã•ã‚ŒãŸæ¿URLã€‚
        """
        return f"https://{server}/{board_id}/"

    def _get_board_server(self, board_id: str) -> str:
        """
        boards.tomlã‹ã‚‰æ¿ã®ã‚µãƒ¼ãƒãƒ¼æƒ…å ±ã‚’å–å¾—ã—ã¾ã™ã€‚
        TASK-068: bbsmenu.htmlä¾å­˜ã‚’é™¤å»ã—ã€é™çš„è¨­å®šã‹ã‚‰å–å¾—

        Parameters
        ----------
        board_id : str
            æ¿ã®IDã€‚

        Returns
        -------
        str
            ã‚µãƒ¼ãƒãƒ¼ã®ãƒ›ã‚¹ãƒˆåã€‚å­˜åœ¨ã—ãªã„å ´åˆã¯ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤ã€‚
        """
        # boards.tomlã‹ã‚‰ç›´æ¥ã‚µãƒ¼ãƒãƒ¼æƒ…å ±ã‚’å–å¾—ï¼ˆbbsmenu.htmlä¾å­˜é™¤å»ï¼‰
        server = self.board_servers.get(board_id, "mevius.5ch.net")
        self.logger.info(f"æ¿ {board_id} ã®ã‚µãƒ¼ãƒãƒ¼: {server} (é™çš„è¨­å®š)")
        return server

    async def _get_with_403_tolerance(self, url: str, board_id: str) -> any:
        """
        403ã‚¨ãƒ©ãƒ¼è€æ€§HTTP GETãƒªã‚¯ã‚¨ã‚¹ãƒˆ - think harderã®çµæœ
        è¤‡æ•°ã®User-Agentã€ãƒ˜ãƒƒãƒ€ãƒ¼æˆ¦ç•¥ã€é–“éš”èª¿æ•´ã‚’è©¦è¡Œ

        Parameters
        ----------
        url : str
            ãƒªã‚¯ã‚¨ã‚¹ãƒˆURL
        board_id : str
            æ¿IDï¼ˆãƒ­ã‚°ç”¨ï¼‰

        Returns
        -------
        any
            HTTPãƒ¬ã‚¹ãƒãƒ³ã‚¹ï¼ˆæˆåŠŸæ™‚ã®ã¿ã€å¤±æ•—æ™‚ã¯Noneï¼‰
        """
        # æ®µéšçš„User-Agentæˆ¦ç•¥ï¼ˆå¤ã„é †ã«è©¦è¡Œï¼‰
        user_agent_strategies = [
            # æˆ¦ç•¥1: æœ€å¤å…¸çš„ãƒ–ãƒ©ã‚¦ã‚¶ï¼ˆ2010å¹´ä»£å‰åŠï¼‰
            "Mozilla/4.0 (compatible; MSIE 8.0; Windows NT 6.1; Trident/4.0)",
            # æˆ¦ç•¥2: å¤ã„Firefoxï¼ˆæ¤œå‡ºå›é¿ï¼‰
            "Mozilla/5.0 (Windows NT 6.1; rv:12.0) Gecko/20100101 Firefox/12.0",
            # æˆ¦ç•¥3: å¤ã„Chromeï¼ˆæœ€ä½é™ï¼‰
            "Mozilla/5.0 (Windows NT 6.1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/30.0.1599.101 Safari/537.36",
            # æˆ¦ç•¥4: ãƒ¢ãƒã‚¤ãƒ«å›é¿ï¼ˆã‚µãƒ¼ãƒãƒ¼è² è·è»½æ¸›ã¨åˆ¤æ–­ã•ã‚Œã‚‹å ´åˆï¼‰
            "Mozilla/5.0 (iPhone; CPU iPhone OS 7_0 like Mac OS X) AppleWebKit/537.51.1 (KHTML, like Gecko) Version/7.0 Mobile/11A465 Safari/9537.53",
            # æˆ¦ç•¥5: æ¤œç´¢ã‚¨ãƒ³ã‚¸ãƒ³botæ¨¡å€£ï¼ˆã‚¢ã‚¯ã‚»ã‚¹è¨±å¯ã•ã‚Œã‚‹å ´åˆï¼‰
            "Mozilla/5.0 (compatible; Googlebot/2.1; +http://www.google.com/bot.html)",
        ]

        for i, user_agent in enumerate(user_agent_strategies):
            try:
                self.logger.info(
                    f"403å¯¾ç­–æˆ¦ç•¥ {i + 1}/{len(user_agent_strategies)}: {user_agent[:50]}..."
                )

                # æ¥µé™ã¾ã§ç°¡ç´ åŒ–ã•ã‚ŒãŸãƒ˜ãƒƒãƒ€ãƒ¼
                headers = {
                    "User-Agent": user_agent,
                    "Accept": "text/html",
                    "Connection": "close",  # æŒç¶šæ¥ç¶šã‚’å›é¿
                }

                # æˆ¦ç•¥åˆ¥ã®å¾…æ©Ÿæ™‚é–“ï¼ˆæ®µéšçš„ã«å»¶é•·ï¼‰
                wait_time = 2 + (i * 3)  # 2ç§’ã‹ã‚‰å§‹ã¾ã‚Š3ç§’ãšã¤å¢—åŠ 
                await asyncio.sleep(wait_time)

                # HTTPã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã«ç›´æ¥ã‚¢ã‚¯ã‚»ã‚¹ï¼ˆãƒªãƒˆãƒ©ã‚¤å‡¦ç†ã‚’å›é¿ï¼‰
                try:
                    response = await self.http_client._client.get(
                        url, headers=headers, timeout=30.0
                    )

                    # ãƒ¬ã‚¹ãƒãƒ³ã‚¹å†…å®¹ã®è©³ç´°åˆ†æï¼ˆCloudflareæ¤œå‡ºï¼‰
                    is_cloudflare = (
                        "Just a moment..." in response.text or "challenge" in response.text.lower()
                    )

                    if response.status_code == 200:
                        if not is_cloudflare:
                            self.logger.info(f"æˆåŠŸ: æˆ¦ç•¥{i + 1}ã§æ­£å¸¸ã‚¢ã‚¯ã‚»ã‚¹")
                            return response
                        else:
                            self.logger.warning(f"æˆ¦ç•¥{i + 1}: Cloudflareãƒãƒ£ãƒ¬ãƒ³ã‚¸ãƒšãƒ¼ã‚¸æ¤œå‡º")
                    elif response.status_code == 403:
                        if is_cloudflare:
                            self.logger.warning(f"æˆ¦ç•¥{i + 1}: Cloudflareä¿è­·ã«ã‚ˆã‚Š403ã‚¨ãƒ©ãƒ¼")
                            # Cloudflareã®å ´åˆã¯é•·æ™‚é–“å¾…æ©Ÿå¾Œã«ãƒªãƒˆãƒ©ã‚¤
                            if i < 2:  # æœ€åˆã®2æˆ¦ç•¥ã®ã¿ãƒªãƒˆãƒ©ã‚¤
                                self.logger.info("Cloudflareå›é¿: 30ç§’å¾…æ©Ÿå¾Œã«ãƒªãƒˆãƒ©ã‚¤")
                                await asyncio.sleep(30)
                                continue
                        elif response.text and len(response.text) > 100 and not is_cloudflare:
                            self.logger.warning(
                                f"403ã‚¨ãƒ©ãƒ¼ã ãŒæœ‰åŠ¹ã‚³ãƒ³ãƒ†ãƒ³ãƒ„å–å¾—: æˆ¦ç•¥{i + 1} ({len(response.text)}æ–‡å­—)"
                            )
                            return response
                        else:
                            self.logger.warning(f"æˆ¦ç•¥{i + 1}: 403ã‚¨ãƒ©ãƒ¼ï¼ˆåˆ©ç”¨ä¸å¯ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ï¼‰")
                    else:
                        self.logger.warning(f"æˆ¦ç•¥{i + 1}: HTTPã‚¨ãƒ©ãƒ¼ {response.status_code}")

                except Exception as e:
                    self.logger.warning(f"æˆ¦ç•¥{i + 1}: ãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚¨ãƒ©ãƒ¼ - {str(e)}")
                    continue

            except Exception as e:
                self.logger.error(f"æˆ¦ç•¥{i + 1}: äºˆæœŸã—ãªã„ã‚¨ãƒ©ãƒ¼ - {str(e)}")
                continue

        # æœ€çµ‚æˆ¦ç•¥: ä»£æ›¿ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆè©¦è¡Œ
        self.logger.info("ä»£æ›¿ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆæˆ¦ç•¥ã‚’é–‹å§‹...")
        alternative_response = await self._try_alternative_endpoints(url, board_id)
        if alternative_response:
            return alternative_response

        # ã™ã¹ã¦ã®æˆ¦ç•¥ãŒå¤±æ•—
        self.logger.error(f"å…¨æˆ¦ç•¥å¤±æ•—: æ¿ {board_id} ã¸ã®ã‚¢ã‚¯ã‚»ã‚¹ã‚’æ–­å¿µ")
        return None

    async def _try_alternative_endpoints(self, original_url: str, board_id: str) -> any:
        """
        ä»£æ›¿ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆæˆ¦ç•¥ - æœ€çµ‚æ‰‹æ®µã®ã‚¢ã‚¯ã‚»ã‚¹æ–¹æ³•

        Parameters
        ----------
        original_url : str
            å…ƒã®URL
        board_id : str
            æ¿ID

        Returns
        -------
        any
            æˆåŠŸæ™‚ã®ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã€å¤±æ•—æ™‚ã¯None
        """
        # URLè§£æ
        from urllib.parse import urlparse

        parsed = urlparse(original_url)
        server = parsed.netloc

        alternative_strategies = [
            # æˆ¦ç•¥1: ã‚¹ãƒãƒ¼ãƒˆãƒ•ã‚©ãƒ³ç‰ˆ
            f"https://sp.5ch.net/{board_id}/",
            # æˆ¦ç•¥2: æ—§å½¢å¼URL
            f"https://{server.replace('.5ch.net', '.2ch.net')}/{board_id}/",
            # æˆ¦ç•¥3: èª­ã¿å–ã‚Šå°‚ç”¨APIé¢¨
            f"https://{server}/{board_id}/subject.txt",
            # æˆ¦ç•¥4: åˆ¥ã‚µãƒ–ãƒ‰ãƒ¡ã‚¤ãƒ³
            f"https://itest.5ch.net/{board_id}/",
            # æˆ¦ç•¥5: HTTPSãªã—ï¼ˆæœ€çµ‚æ‰‹æ®µï¼‰
            f"http://{server}/{board_id}/",
        ]

        headers = {
            "User-Agent": "Mozilla/5.0 (Linux; Android 8.0.0; SM-G973F) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/70.0.3538.80 Mobile Safari/537.36",
            "Accept": "text/plain, text/html, */*",
            "Accept-Language": "ja,en;q=0.9",
            "Connection": "close",
        }

        for i, alt_url in enumerate(alternative_strategies):
            try:
                self.logger.info(f"ä»£æ›¿æˆ¦ç•¥ {i + 1}/{len(alternative_strategies)}: {alt_url}")
                await asyncio.sleep(3)  # çŸ­ã„é–“éš”

                response = await self.http_client._client.get(
                    alt_url, headers=headers, timeout=20.0
                )

                # æˆåŠŸåˆ¤å®šã‚’ç·©ãè¨­å®š
                if response.status_code in [200, 403]:
                    content = response.text
                    is_valid = (
                        len(content) > 50
                        and "Just a moment" not in content
                        and "challenge" not in content.lower()
                        and (
                            "5ch" in content or "2ch" in content or "\n" in content
                        )  # æœ€ä½é™ã®ã‚³ãƒ³ãƒ†ãƒ³ãƒ„æ¤œè¨¼
                    )

                    if is_valid:
                        self.logger.info(
                            f"ä»£æ›¿æˆ¦ç•¥{i + 1}æˆåŠŸ: {response.status_code} ({len(content)}æ–‡å­—)"
                        )
                        return response
                    else:
                        self.logger.warning(f"ä»£æ›¿æˆ¦ç•¥{i + 1}: ç„¡åŠ¹ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ ({len(content)}æ–‡å­—)")
                else:
                    self.logger.warning(f"ä»£æ›¿æˆ¦ç•¥{i + 1}: HTTPã‚¨ãƒ©ãƒ¼ {response.status_code}")

            except Exception as e:
                self.logger.warning(f"ä»£æ›¿æˆ¦ç•¥{i + 1}: ã‚¨ãƒ©ãƒ¼ - {str(e)}")
                continue

        return None

    async def _get_subject_txt_data(self, board_id: str) -> list[dict]:
        """
        subject.txtå½¢å¼ã§ã‚¹ãƒ¬ãƒƒãƒ‰ä¸€è¦§ã‚’å–å¾—ï¼ˆCloudflareçªç ´æˆåŠŸæ‰‹æ³•ï¼‰

        Parameters
        ----------
        board_id : str
            æ¿ID

        Returns
        -------
        List[dict]
            ã‚¹ãƒ¬ãƒƒãƒ‰æƒ…å ±ãƒªã‚¹ãƒˆ
        """
        # æˆåŠŸç¢ºèªæ¸ˆã¿ã‚µãƒ¼ãƒãƒ¼ãƒãƒƒãƒ”ãƒ³ã‚°ï¼ˆå®Ÿéš›ã®ãƒ†ã‚¹ãƒˆçµæœã«åŸºã¥ãï¼‰
        server_mapping = {
            "ai": ["krsw.5ch.net", "egg.5ch.net", "mevius.5ch.net"],
            "prog": ["medaka.5ch.net", "mevius.5ch.net"],
            "tech": ["mevius.5ch.net"],  # ä¿®æ­£: techã¯mevius.5ch.netã®ã¿
            "esite": ["mevius.5ch.net"],  # ä¿®æ­£: esiteã¯mevius.5ch.netã®ã¿
            "software": ["egg.5ch.net"],
            "bizplus": ["egg.5ch.net"],
            "news": ["hayabusa9.5ch.net"],
        }

        servers = server_mapping.get(board_id, [self._get_board_server(board_id)])

        headers = {
            "User-Agent": "Mozilla/5.0 (compatible; 5ch subject reader)",
            "Accept": "text/plain",
        }

        for server in servers:
            try:
                url = f"https://{server}/{board_id}/subject.txt"
                self.logger.info(f"subject.txtå–å¾—: {url}")

                # ç›´æ¥httpxã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã‚’ä½¿ç”¨ï¼ˆ403å›é¿ã®ãŸã‚ï¼‰
                import httpx

                async with httpx.AsyncClient() as client:
                    response = await client.get(url, headers=headers, timeout=10.0)

                if response.status_code == 200:
                    # æ–‡å­—åŒ–ã‘å¯¾ç­–ï¼ˆShift_JIS + ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼‰
                    try:
                        content = response.content.decode("shift_jis", errors="ignore")
                    except (UnicodeDecodeError, LookupError):
                        try:
                            content = response.content.decode("cp932", errors="ignore")
                        except (UnicodeDecodeError, LookupError):
                            try:
                                content = response.content.decode("utf-8", errors="ignore")
                            except (UnicodeDecodeError, LookupError):
                                content = response.text

                    threads_data = []
                    lines = content.split("\n")

                    import re

                    for line in lines:
                        if line.strip():
                            # datå½¢å¼è§£æ: timestamp.dat<>title (post_count)
                            match = re.match(r"(\d+)\.dat<>(.+?)\s+\((\d+)\)", line)
                            if match:
                                timestamp, title, post_count = match.groups()
                                threads_data.append(
                                    {
                                        "server": server,
                                        "board": board_id,
                                        "timestamp": timestamp,
                                        "title": title.strip(),
                                        "post_count": int(post_count),
                                        "dat_url": f"https://{server}/{board_id}/dat/{timestamp}.dat",
                                        "html_url": f"https://{server}/test/read.cgi/{board_id}/{timestamp}/",
                                    }
                                )

                    self.logger.info(f"subject.txtæˆåŠŸ: {len(threads_data)}ã‚¹ãƒ¬ãƒƒãƒ‰å–å¾—")
                    return threads_data

            except Exception as e:
                self.logger.warning(f"subject.txtå¤±æ•— {server}: {e}")
                continue

        return []

    async def _get_thread_posts_from_dat(
        self, dat_url: str
    ) -> tuple[list[dict[str, Any]], datetime | None]:
        """
        datå½¢å¼ã§ã‚¹ãƒ¬ãƒƒãƒ‰ã®æŠ•ç¨¿ã‚’å–å¾—ï¼ˆcloudscraperä½¿ç”¨ç‰ˆï¼‰
        """
        try:
            self.logger.info(f"datå–å¾—é–‹å§‹: {dat_url}")

            # cloudscraper ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚’ä½œæˆ
            scraper = cloudscraper.create_scraper(
                browser={"browser": "chrome", "platform": "windows", "desktop": True}
            )

            # Monazillaå½¢å¼ã®ãƒ˜ãƒƒãƒ€ãƒ¼ã‚’è¨­å®š
            scraper.headers.update(
                {
                    "User-Agent": "Monazilla/1.00 (NookCrawler/1.0)",
                    "Accept-Encoding": "gzip",
                    "Referer": dat_url.replace("/dat/", "/test/read.cgi/").replace(".dat", "/"),
                }
            )

            # åŒæœŸçš„ã«ãƒªã‚¯ã‚¨ã‚¹ãƒˆï¼ˆcloudscraperã¯åŒæœŸãƒ©ã‚¤ãƒ–ãƒ©ãƒªï¼‰
            # asyncio.to_threadã§éåŒæœŸåŒ–
            response = await asyncio.to_thread(scraper.get, dat_url, timeout=30)
            self.logger.info(f"datå–å¾—ãƒ¬ã‚¹ãƒãƒ³ã‚¹: {response.status_code}")

            if response.status_code == 200:
                # æ–‡å­—åŒ–ã‘å¯¾ç­–ï¼ˆShift_JIS + ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼‰
                try:
                    content = response.content.decode("shift_jis", errors="ignore")
                except (UnicodeDecodeError, LookupError):
                    try:
                        content = response.content.decode("cp932", errors="ignore")
                    except (UnicodeDecodeError, LookupError):
                        content = response.text

                posts: list[dict[str, Any]] = []
                latest_post_at: datetime | None = None
                lines = content.split("\n")

                for i, line in enumerate(lines):
                    if line.strip():
                        # datå½¢å¼: name<>mail<>date ID<>message<>title(1è¡Œç›®ã®ã¿)
                        parts = line.split("<>")
                        if len(parts) >= 4:
                            post_data = {
                                "no": i + 1,
                                "name": parts[0],
                                "mail": parts[1],
                                "date": parts[2],
                                "com": parts[3],
                                "time": parts[2],  # äº’æ›æ€§ã®ãŸã‚
                            }

                            # 1è¡Œç›®ã®å ´åˆã¯ã‚¿ã‚¤ãƒˆãƒ«ã‚‚å«ã¾ã‚Œã‚‹
                            if i == 0 and len(parts) >= 5:
                                post_data["title"] = parts[4]

                            posts.append(post_data)

                            date_field = parts[2]
                            try:
                                parsed = parser.parse(date_field, fuzzy=True, ignoretz=True)
                            except (ValueError, OverflowError):
                                parsed = None

                            if parsed and (latest_post_at is None or parsed > latest_post_at):
                                latest_post_at = parsed

                self.logger.info(f"datè§£æå®Œäº†: ç·è¡Œæ•°{len(lines)}, æœ‰åŠ¹æŠ•ç¨¿{len(posts)}ä»¶")
                if posts:
                    self.logger.info(f"datå–å¾—æˆåŠŸ: {len(posts)}æŠ•ç¨¿")
                    limited_posts = posts[:10]
                    return limited_posts, latest_post_at
                else:
                    self.logger.warning("datå†…å®¹ã¯å–å¾—ã—ãŸãŒæŠ•ç¨¿ãƒ‡ãƒ¼ã‚¿ãªã—")
                    return [], latest_post_at
            else:
                self.logger.error(f"datå–å¾—HTTP error: {response.status_code}")
                if "Just a moment" in response.text:
                    self.logger.error("Cloudflareãƒãƒ£ãƒ¬ãƒ³ã‚¸ãƒšãƒ¼ã‚¸ãŒæ¤œå‡ºã•ã‚Œã¾ã—ãŸ")
                return [], None

        except Exception as e:
            self.logger.error(f"datå–å¾—ã‚¨ãƒ©ãƒ¼ {dat_url}: {e}")
            import traceback

            self.logger.error(f"è©³ç´°ãªã‚¨ãƒ©ãƒ¼æƒ…å ±: {traceback.format_exc()}")

        return [], None

    async def _retrieve_ai_threads(
        self,
        board_id: str,
        limit: int | None,
        dedup_tracker: DedupTracker,
        target_dates: list[date],
    ) -> list[Thread]:
        """
        ç‰¹å®šã®æ¿ã‹ã‚‰AIé–¢é€£ã‚¹ãƒ¬ãƒƒãƒ‰ã‚’å–å¾—ã—ã¾ã™ã€‚
        ã€Cloudflareçªç ´æˆåŠŸç‰ˆã€‘subject.txt + datå½¢å¼ã«ã‚ˆã‚‹å®Œå…¨å®Ÿè£…

        Parameters
        ----------
        board_id : str
            æ¿ã®IDã€‚
        limit : Optional[int]
            å–å¾—ã™ã‚‹ã‚¹ãƒ¬ãƒƒãƒ‰æ•°ã€‚Noneã®å ´åˆã¯åˆ¶é™ãªã—ã€‚
        target_dates : list[date]
            ä¿å­˜å¯¾è±¡ã¨ã™ã‚‹æ—¥ä»˜é›†åˆã€‚

        Returns
        -------
        List[Thread]
            å–å¾—ã—ãŸã‚¹ãƒ¬ãƒƒãƒ‰ã®ãƒªã‚¹ãƒˆã€‚
        """
        try:
            self.logger.info(f"ã€çªç ´æ‰‹æ³•ã€‘æ¿ {board_id} ã‹ã‚‰AIé–¢é€£ã‚¹ãƒ¬ãƒƒãƒ‰ã‚’å–å¾—ã—ã¾ã™")

            # 1. subject.txtã‹ã‚‰ã‚¹ãƒ¬ãƒƒãƒ‰ä¸€è¦§ã‚’å–å¾—ï¼ˆçªç ´æˆåŠŸæ‰‹æ³•ï¼‰
            threads_data = await self._get_subject_txt_data(board_id)
            if not threads_data:
                self.logger.warning(f"subject.txtå–å¾—å¤±æ•—: æ¿ {board_id}")
                return []

            # 2. AIé–¢é€£ã‚¹ãƒ¬ãƒƒãƒ‰ã‚’ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
            ai_threads = []
            self.logger.info(f"AIé–¢é€£ã‚¹ãƒ¬ãƒƒãƒ‰æ¤œç´¢ä¸­... å¯¾è±¡: {len(threads_data)}ã‚¹ãƒ¬ãƒƒãƒ‰")

            for thread_data in threads_data:
                title = thread_data["title"]
                title_lower = title.lower()

                # AIã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒãƒƒãƒãƒ³ã‚°
                is_ai_related = any(keyword.lower() in title_lower for keyword in self.ai_keywords)

                if is_ai_related:
                    timestamp_raw = thread_data.get("timestamp")
                    thread_created = (
                        datetime.fromtimestamp(int(timestamp_raw), tz=UTC)
                        if timestamp_raw
                        else None
                    )

                    is_dup, normalized = dedup_tracker.is_duplicate(title)
                    if is_dup:
                        original = dedup_tracker.get_original_title(normalized) or title
                        self.logger.info(
                            "é‡è¤‡ã‚¹ãƒ¬ãƒƒãƒ‰ã‚’ã‚¹ã‚­ãƒƒãƒ—: '%s' (åˆå‡º: '%s')",
                            title,
                            original,
                        )
                        continue

                    self.logger.info(f"AIé–¢é€£ã‚¹ãƒ¬ãƒƒãƒ‰ç™ºè¦‹: {title}")

                    # 3. datå½¢å¼ã§æŠ•ç¨¿ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ï¼ˆçªç ´æˆåŠŸæ‰‹æ³•ï¼‰
                    posts, latest_post_at = await self._get_thread_posts_from_dat(
                        thread_data["dat_url"]
                    )

                    effective_dt = latest_post_at or thread_created
                    if not effective_dt or not is_within_target_dates(effective_dt, target_dates):
                        continue

                    effective_local = normalize_datetime_to_local(effective_dt)
                    timestamp_value = (
                        int(effective_local.timestamp())
                        if effective_local is not None
                        else int(timestamp_raw)
                        if timestamp_raw
                        else 0
                    )

                    if posts:  # æŠ•ç¨¿å–å¾—æˆåŠŸæ™‚ã®ã¿ã‚¹ãƒ¬ãƒƒãƒ‰ä½œæˆ
                        popularity_score = self._calculate_popularity(
                            post_count=thread_data.get("post_count", 0),
                            sample_count=len(posts),
                            timestamp=timestamp_value,
                        )

                        dedup_tracker.add(title)

                        thread = Thread(
                            thread_id=int(thread_data["timestamp"]),
                            title=title,
                            url=thread_data["html_url"],  # HTMLç‰ˆURL
                            board=board_id,
                            posts=posts,
                            timestamp=timestamp_value,
                            popularity_score=popularity_score,
                        )

                        ai_threads.append(thread)
                        self.logger.info(f"ã‚¹ãƒ¬ãƒƒãƒ‰è¿½åŠ æˆåŠŸ: {title} ({len(posts)}æŠ•ç¨¿)")

                        # åˆ¶é™æ•°ã«é”ã—ãŸã‚‰çµ‚äº†
                        if limit is not None and len(ai_threads) >= limit:
                            break
                    else:
                        self.logger.warning(f"æŠ•ç¨¿å–å¾—å¤±æ•—: {title}")

                    # ã‚¢ã‚¯ã‚»ã‚¹é–“éš”ï¼ˆä¸å¯§ãªã‚¢ã‚¯ã‚»ã‚¹ï¼‰
                    await asyncio.sleep(2)

            self.logger.info(
                f"ã€çªç ´æˆåŠŸã€‘æ¿ {board_id}: {len(ai_threads)}ä»¶ã®AIé–¢é€£ã‚¹ãƒ¬ãƒƒãƒ‰å–å¾—å®Œäº†"
            )
            return ai_threads

        except Exception as e:
            self.logger.error(f"ã€çªç ´æ‰‹æ³•ã‚¨ãƒ©ãƒ¼ã€‘æ¿ {board_id}: {str(e)}")
            return []

    def _load_existing_titles(self) -> DedupTracker:
        tracker = DedupTracker()
        try:
            content = self.storage.load_markdown("", datetime.now())
            if content:
                for match in re.finditer(r"^### \[(.+?)\]", content, re.MULTILINE):
                    tracker.add(match.group(1))
        except Exception as exc:
            self.logger.debug(f"æ—¢å­˜ã‚¹ãƒ¬ãƒƒãƒ‰ã‚¿ã‚¤ãƒˆãƒ«ã®èª­ã¿è¾¼ã¿ã«å¤±æ•—ã—ã¾ã—ãŸ: {exc}")
        return tracker

    def _calculate_popularity(self, post_count: int, sample_count: int, timestamp: int) -> float:
        recency_bonus = 0.0
        try:
            now = datetime.now()
            created = datetime.fromtimestamp(timestamp)
            hours = (now - created).total_seconds() / 3600
            recency_bonus = 24 / max(1.0, hours)
        except Exception:
            pass

        return float(post_count + sample_count + recency_bonus)

    def _select_top_threads(self, threads: list[Thread], limit: int) -> list[Thread]:
        if not threads:
            return []

        if len(threads) <= limit:
            return threads

        def sort_key(thread: Thread):
            created = datetime.fromtimestamp(thread.timestamp)
            return (thread.popularity_score, created)

        sorted_threads = sorted(threads, key=sort_key, reverse=True)
        return sorted_threads[:limit]

    async def _summarize_thread(self, thread: Thread) -> None:
        """
        ã‚¹ãƒ¬ãƒƒãƒ‰ã‚’è¦ç´„ã—ã¾ã™ã€‚

        Parameters
        ----------
        thread : Thread
            è¦ç´„ã™ã‚‹ã‚¹ãƒ¬ãƒƒãƒ‰ã€‚
        """
        # ã‚¹ãƒ¬ãƒƒãƒ‰ã®ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’æŠ½å‡º
        thread_content = ""

        # ã‚¹ãƒ¬ãƒƒãƒ‰ã®ã‚¿ã‚¤ãƒˆãƒ«ã‚’è¿½åŠ 
        thread_content += f"ã‚¿ã‚¤ãƒˆãƒ«: {thread.title}\n\n"

        # ã‚ªãƒªã‚¸ãƒŠãƒ«ãƒã‚¹ãƒˆï¼ˆOPï¼‰ã‚’è¿½åŠ 
        if thread.posts and len(thread.posts) > 0:
            op = thread.posts[0]
            op_text = op.get("com", "")
            if op_text:
                thread_content += f">>1: {op_text}\n\n"

        # è¿”ä¿¡ã‚’è¿½åŠ ï¼ˆæœ€å¤§5ä»¶ï¼‰
        replies = thread.posts[1:6] if len(thread.posts) > 1 else []
        for i, reply in enumerate(replies):
            reply_text = reply.get("com", "")
            if reply_text:
                post_number = reply.get("no", i + 2)
                thread_content += f">>{post_number}: {reply_text}\n\n"

        prompt = f"""
        ä»¥ä¸‹ã®5chanï¼ˆæ—§2ã¡ã‚ƒã‚“ã­ã‚‹ï¼‰ã‚¹ãƒ¬ãƒƒãƒ‰ã‚’è¦ç´„ã—ã¦ãã ã•ã„ã€‚

        æ¿: /{thread.board}/
        {thread_content}

        è¦ç´„ã¯ä»¥ä¸‹ã®å½¢å¼ã§è¡Œã„ã€æ—¥æœ¬èªã§å›ç­”ã—ã¦ãã ã•ã„:
        1. ã‚¹ãƒ¬ãƒƒãƒ‰ã®ä¸»ãªå†…å®¹ï¼ˆ1-2æ–‡ï¼‰
        2. è­°è«–ã®ä¸»è¦ãƒã‚¤ãƒ³ãƒˆï¼ˆç®‡æ¡æ›¸ã3-5ç‚¹ï¼‰
        3. ã‚¹ãƒ¬ãƒƒãƒ‰ã®å…¨ä½“çš„ãªè«–èª¿

        æ³¨æ„ï¼šæ”»æ’ƒçš„ãªå†…å®¹ã‚„ãƒ˜ã‚¤ãƒˆã‚¹ãƒ”ãƒ¼ãƒã¯ç·©å’Œã—ã€ä¸»è¦ãªæŠ€è¡“çš„è­°è«–ã«ç„¦ç‚¹ã‚’å½“ã¦ã¦ãã ã•ã„ã€‚
        """

        system_instruction = """
        ã‚ãªãŸã¯5chanï¼ˆæ—§2ã¡ã‚ƒã‚“ã­ã‚‹ï¼‰ã‚¹ãƒ¬ãƒƒãƒ‰ã®è¦ç´„ã‚’è¡Œã†ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚
        æŠ•ç¨¿ã•ã‚ŒãŸå†…å®¹ã‚’å®¢è¦³çš„ã«åˆ†æã—ã€æŠ€è¡“çš„è­°è«–ã‚„æƒ…å ±ã«ç„¦ç‚¹ã‚’å½“ã¦ãŸè¦ç´„ã‚’æä¾›ã—ã¦ãã ã•ã„ã€‚
        éåº¦ãªæ”»æ’ƒæ€§ã€ãƒ˜ã‚¤ãƒˆã‚¹ãƒ”ãƒ¼ãƒã€å·®åˆ¥çš„å†…å®¹ã¯ä¸­å’Œã—ã¦è¡¨ç¾ã—ã€æœ‰ç›Šãªæƒ…å ±ã®ã¿ã‚’æŠ½å‡ºã—ã¦ãã ã•ã„ã€‚
        å›ç­”ã¯æ—¥æœ¬èªã§è¡Œã„ã€AIã‚„ãƒ†ã‚¯ãƒãƒ­ã‚¸ãƒ¼ã«é–¢é€£ã™ã‚‹æƒ…å ±ã‚’å„ªå…ˆçš„ã«å«ã‚ã¦ãã ã•ã„ã€‚
        """

        try:
            summary = self.gpt_client.generate_content(
                prompt=prompt,
                system_instruction=system_instruction,
                temperature=0.3,
                max_tokens=1000,
            )
            thread.summary = summary
        except Exception as e:
            self.logger.error(f"è¦ç´„ã®ç”Ÿæˆä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}")
            thread.summary = f"è¦ç´„ã®ç”Ÿæˆä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}"

    async def _store_summaries(
        self, threads: list[Thread], target_dates: list[date]
    ) -> list[tuple[str, str]]:
        if not threads:
            self.logger.info("ä¿å­˜ã™ã‚‹ã‚¹ãƒ¬ãƒƒãƒ‰ãŒã‚ã‚Šã¾ã›ã‚“")
            return []

        default_date = max(target_dates) if target_dates else datetime.now().date()
        records = self._serialize_threads(threads)
        records_by_date = group_records_by_date(records, default_date=default_date)

        saved_files = await store_daily_snapshots(
            records_by_date,
            load_existing=self._load_existing_threads,
            save_json=self.save_json,
            save_markdown=self.save_markdown,
            render_markdown=self._render_markdown,
            key=lambda item: item.get("thread_id"),
            sort_key=self._thread_sort_key,
            limit=self.TOTAL_LIMIT,
            logger=self.logger,
        )

        return saved_files

    def _serialize_threads(self, threads: list[Thread]) -> list[dict]:
        records: list[dict] = []
        for thread in threads:
            published = datetime.fromtimestamp(thread.timestamp, tz=UTC)
            records.append(
                {
                    "thread_id": thread.thread_id,
                    "title": thread.title,
                    "url": thread.url,
                    "timestamp": thread.timestamp,
                    "summary": thread.summary,
                    "popularity_score": thread.popularity_score,
                    "board": thread.board,
                    "published_at": published.isoformat(),
                }
            )
        return records

    async def _load_existing_threads(self, target_date: datetime) -> list[dict]:
        date_str = target_date.strftime("%Y-%m-%d")
        filename_json = f"{date_str}.json"
        existing_json = await self.load_json(filename_json)
        if existing_json:
            if isinstance(existing_json, dict):
                flattened: list[dict] = []
                for board, items in existing_json.items():
                    for item in items:
                        flattened.append({"board": board, **item})
                return flattened
            return existing_json

        markdown = await self.storage.load(f"{date_str}.md")
        if not markdown:
            return []

        return self._parse_markdown(markdown)

    def _thread_sort_key(self, item: dict) -> tuple[float, datetime]:
        popularity = float(item.get("popularity_score", 0.0) or 0.0)
        published_raw = item.get("published_at")
        published: datetime
        if published_raw:
            try:
                published = datetime.fromisoformat(published_raw)
            except ValueError:
                published = datetime.min.replace(tzinfo=UTC)
        else:
            timestamp = item.get("timestamp")
            if timestamp:
                try:
                    published = datetime.fromtimestamp(int(timestamp), tz=UTC)
                except Exception:
                    published = datetime.min.replace(tzinfo=UTC)
            else:
                published = datetime.min.replace(tzinfo=UTC)
        return (popularity, published)

    def _render_markdown(self, records: list[dict], today: datetime) -> str:
        content = f"# 5chan AIé–¢é€£ã‚¹ãƒ¬ãƒƒãƒ‰ ({today.strftime('%Y-%m-%d')})\n\n"
        grouped: dict[str, list[dict]] = {}
        for record in records:
            board = record.get("board", "unknown")
            grouped.setdefault(board, []).append(record)

        for board, threads in grouped.items():
            board_name = self.target_boards.get(board, board)
            content += f"## {board_name} (/{board}/)\n\n"
            for thread in threads:
                title = thread.get("title") or f"ç„¡é¡Œã‚¹ãƒ¬ãƒƒãƒ‰ #{thread.get('thread_id')}"
                content += f"### [{title}]({thread.get('url')})\n\n"
                published_raw = thread.get("published_at")
                if published_raw:
                    try:
                        date_str = datetime.fromisoformat(published_raw).strftime(
                            "%Y-%m-%d %H:%M:%S"
                        )
                    except ValueError:
                        date_str = published_raw
                else:
                    timestamp = thread.get("timestamp")
                    if timestamp:
                        try:
                            date_str = datetime.fromtimestamp(int(timestamp)).strftime(
                                "%Y-%m-%d %H:%M:%S"
                            )
                        except Exception:
                            date_str = "N/A"
                    else:
                        date_str = "N/A"
                content += f"ä½œæˆæ—¥æ™‚: {date_str}\n\n"
                content += f"**è¦ç´„**:\n{thread.get('summary', '')}\n\n"
                content += "---\n\n"
        return content

    def _parse_markdown(self, markdown: str) -> list[dict]:
        records: list[dict] = []
        board_pattern = re.compile(r"^##\s+(.+) \(/(.+)/\)$", re.MULTILINE)
        thread_pattern = re.compile(
            r"### \[(?P<title>.+?)\]\((?P<url>[^\)]+)\)\n\n"
            r"ä½œæˆæ—¥æ™‚: (?P<datetime>.+?)\n\n"
            r"\*\*è¦ç´„\*\*:\n(?P<summary>.*?)(?:\n\n)?---",
            re.DOTALL,
        )

        sections = list(board_pattern.finditer(markdown))
        for idx, match in enumerate(sections):
            start = match.end()
            end = sections[idx + 1].start() if idx + 1 < len(sections) else len(markdown)
            block = markdown[start:end]
            board_id = match.group(2).strip()

            for thread_match in thread_pattern.finditer(block + "---"):
                title = thread_match.group("title")
                url = thread_match.group("url")
                summary = thread_match.group("summary").strip()
                datetime_str = thread_match.group("datetime")
                try:
                    published = datetime.strptime(datetime_str, "%Y-%m-%d %H:%M:%S")
                except ValueError:
                    published = None

                record = {
                    "thread_id": 0,
                    "title": title.strip(),
                    "url": url.strip(),
                    "summary": summary,
                    "popularity_score": 0.0,
                    "board": board_id,
                }

                if published:
                    record["published_at"] = published.replace(tzinfo=UTC).isoformat()
                    record["timestamp"] = int(published.timestamp())

                records.append(record)

        return records
