import asyncio
import re
from dataclasses import dataclass, field
from datetime import date, datetime, timezone
from pathlib import Path
from typing import Any

import cloudscraper
import httpx

from nook.common.base_service import BaseService
from nook.common.daily_snapshot import group_records_by_date, store_daily_snapshots
from nook.common.date_utils import (
    is_within_target_dates,
    target_dates_set,
)
from nook.common.dedup import DedupTracker
from nook.common.logging_utils import (
    log_article_counts,
    log_no_new_articles,
    log_processing_start,
    log_storage_complete,
    log_summarization_progress,
    log_summarization_start,
    log_summary_candidates,
)

# å®šæ•°: ã‚¹ãƒ¬ãƒƒãƒ‰ã‚ãŸã‚Šã®æœ€å¤§æŠ•ç¨¿å–å¾—æ•°
# ãƒ¡ãƒ¢ãƒªåŠ¹ç‡ã¨ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã®ãƒãƒ©ãƒ³ã‚¹ã‚’ä¿ã¤ãŸã‚ã€å¤§è¦æ¨¡ã‚¹ãƒ¬ãƒƒãƒ‰ã§ã‚‚åˆ¶é™ã‚’è¨­ã‘ã‚‹
MAX_POSTS_PER_THREAD = 10


@dataclass
class Thread:
    """5chanã‚¹ãƒ¬ãƒƒãƒ‰æƒ…å ±ã€‚

    Parameters
    ----------
    thread_id : int
        ã‚¹ãƒ¬ãƒƒãƒ‰IDã€‚
    title : str
        ã‚¹ãƒ¬ãƒƒãƒ‰ã‚¿ã‚¤ãƒˆãƒ«ã€‚
    url : str
        ã‚¹ãƒ¬ãƒƒãƒ‰URLã€‚
    board : str
        æ¿åã€‚
    timestamp : int
        ã‚¹ãƒ¬ãƒƒãƒ‰ä½œæˆæ™‚åˆ»ï¼ˆUNIXã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ï¼‰ã€‚
    posts : list[Post]
        æŠ•ç¨¿ãƒªã‚¹ãƒˆã€‚
    summary : str
        ã‚¹ãƒ¬ãƒƒãƒ‰è¦ç´„ã€‚
    popularity_score : float
        äººæ°—åº¦ã‚¹ã‚³ã‚¢ï¼ˆæŠ•ç¨¿æ•°ãƒ™ãƒ¼ã‚¹ï¼‰ã€‚

    """

    thread_id: int
    title: str
    url: str
    board: str
    timestamp: int
    posts: list["Post"] = field(default_factory=list)
    summary: str = ""
    popularity_score: float = 0.0


@dataclass
class Post:
    """5chanæŠ•ç¨¿æƒ…å ±ã€‚

    Parameters
    ----------
    no : int
        æŠ•ç¨¿ç•ªå·ã€‚
    name : str
        æŠ•ç¨¿è€…åã€‚
    mail : str
        ãƒ¡ãƒ¼ãƒ«ã‚¢ãƒ‰ãƒ¬ã‚¹ã€‚
    date : str
        æŠ•ç¨¿æ—¥æ™‚ã€‚
    content : str
        æŠ•ç¨¿å†…å®¹ã€‚

    """

    no: int
    name: str
    mail: str
    date: str
    content: str


class FiveChanExplorer(BaseService):
    """5chanï¼ˆæ—§2ã¡ã‚ƒã‚“ã­ã‚‹ï¼‰ã‹ã‚‰AIé–¢é€£ã‚¹ãƒ¬ãƒƒãƒ‰ã‚’åé›†ã™ã‚‹ã‚µãƒ¼ãƒ“ã‚¹ã€‚

    Parameters
    ----------
    storage : LocalStorage, optional
        ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã€‚
    gpt_client : GPTClient, optional
        GPTã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã€‚

    """

    TOTAL_LIMIT = 15  # 1æ—¥ã‚ãŸã‚Šã®æœ€å¤§ã‚¹ãƒ¬ãƒƒãƒ‰æ•°

    def __init__(self, storage_dir: str | None = None):
        super().__init__(service_name="fivechan_explorer")
        if storage_dir:
            from nook.common.storage import LocalStorage

            self.storage = LocalStorage(storage_dir)

        self.target_boards = self._load_boards_config()
        self.dedup_tracker = DedupTracker()
        self.http_client = None
        self.browser_headers = {
            "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8",
            "Accept-Language": "ja,en-US;q=0.9,en;q=0.8",
            "Accept-Encoding": "gzip, deflate, br",
            "Connection": "keep-alive",
            "Upgrade-Insecure-Requests": "1",
            "Sec-Fetch-Dest": "document",
            "Sec-Fetch-Mode": "navigate",
            "Sec-Fetch-Site": "none",
            "Cache-Control": "max-age=0",
        }
        self.user_agents = [
            "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
            "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
            "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
            "Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:121.0) Gecko/20100101 Firefox/121.0",
            "Mozilla/5.0 (Macintosh; Intel Mac OS X 10.15; rv:121.0) Gecko/20100101 Firefox/121.0",
        ]

    def _load_boards_config(self) -> dict[str, str]:
        """boards.tomlã‹ã‚‰æ¿è¨­å®šã‚’èª­ã¿è¾¼ã¿ã¾ã™ã€‚

        Returns
        -------
        dict[str, str]
            æ¿IDâ†’æ¿åã®ãƒãƒƒãƒ”ãƒ³ã‚°ã€‚

        """
        config_path = Path(__file__).parent / "boards.toml"
        with open(config_path, "rb") as f:
            import tomllib

            config = tomllib.load(f)
            boards_config = config.get("boards", {})

            # æ–°ã—ã„å½¢å¼å¯¾å¿œ: {board_id: {name: "åå‰", server: "ã‚µãƒ¼ãƒãƒ¼"}}
            # æ—§å½¢å¼ã‚‚å¯¾å¿œ: {board_id: "åå‰"}
            boards = {}
            self.board_servers = {}  # ã‚µãƒ¼ãƒãƒ¼æƒ…å ±ã‚’ä¿å­˜

            for board_id, board_info in boards_config.items():
                if isinstance(board_info, dict):
                    # æ–°å½¢å¼: {name: "åå‰", server: "ã‚µãƒ¼ãƒãƒ¼"}
                    boards[board_id] = board_info.get("name", board_id)
                    self.board_servers[board_id] = board_info.get("server", "mevius.5ch.net")
                else:
                    # æ—§å½¢å¼: "åå‰"
                    boards[board_id] = board_info
                    self.board_servers[board_id] = "mevius.5ch.net"  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ

            return boards

    def _get_random_user_agent(self) -> str:
        """ãƒ©ãƒ³ãƒ€ãƒ ãªUser-Agentã‚’å–å¾—ã—ã¾ã™ã€‚

        Returns
        -------
        str
            ãƒ©ãƒ³ãƒ€ãƒ ã«é¸æŠã•ã‚ŒãŸUser-Agentæ–‡å­—åˆ—ã€‚

        """
        import random

        return random.choice(self.user_agents)

    def _calculate_backoff_delay(self, retry_count: int) -> float:
        """æŒ‡æ•°ãƒãƒƒã‚¯ã‚ªãƒ•ã«ã‚ˆã‚‹é…å»¶æ™‚é–“ã‚’è¨ˆç®—ã—ã¾ã™ã€‚

        Parameters
        ----------
        retry_count : int
            ãƒªãƒˆãƒ©ã‚¤å›æ•°ã€‚

        Returns
        -------
        float
            é…å»¶æ™‚é–“ï¼ˆç§’ï¼‰ã€‚

        """
        # åŸºæœ¬é…å»¶æ™‚é–“: 2^retry_countç§’ã€æœ€å¤§300ç§’
        base_delay = min(2**retry_count, 300)
        return base_delay

    async def _get_with_retry(self, url: str, max_retries: int = 3, **kwargs) -> Any:
        """ãƒªãƒˆãƒ©ã‚¤æ©Ÿèƒ½ä»˜ãHTTP GETãƒªã‚¯ã‚¨ã‚¹ãƒˆã€‚

        Parameters
        ----------
        url : str
            ãƒªã‚¯ã‚¨ã‚¹ãƒˆURLã€‚
        max_retries : int, default=3
            æœ€å¤§ãƒªãƒˆãƒ©ã‚¤å›æ•°ã€‚

        Returns
        -------
        any
            HTTPãƒ¬ã‚¹ãƒãƒ³ã‚¹ã€‚

        """
        for attempt in range(max_retries + 1):
            try:
                # å‹•çš„ãªUser-Agentã§ãƒ˜ãƒƒãƒ€ãƒ¼ã‚’æ›´æ–°
                headers = self.browser_headers.copy()
                headers["User-Agent"] = self._get_random_user_agent()

                if not self.http_client:
                    return None
                response = await self.http_client.get(url, headers=headers, **kwargs)

                # æˆåŠŸãƒ¬ã‚¹ãƒãƒ³ã‚¹ï¼ˆ200ç•ªå°ï¼‰ã®å ´åˆã¯è¿”ã™
                if 200 <= response.status_code < 300:
                    return response

                # ãƒ¬ãƒ¼ãƒˆåˆ¶é™ã‚¨ãƒ©ãƒ¼ï¼ˆ429ï¼‰ã®å ´åˆ
                if response.status_code == 429:
                    retry_after = response.headers.get("Retry-After")
                    if retry_after:
                        wait_time = int(float(retry_after))
                    else:
                        wait_time = int(self._calculate_backoff_delay(attempt))

                    self.logger.warning(f"ãƒ¬ãƒ¼ãƒˆåˆ¶é™æ¤œçŸ¥ (429): {wait_time}ç§’å¾…æ©Ÿã—ã¾ã™")
                    await asyncio.sleep(wait_time)
                    continue

                # ã‚µãƒ¼ãƒãƒ¼ã‚¨ãƒ©ãƒ¼ï¼ˆ503ç­‰ï¼‰ã®å ´åˆ
                if response.status_code >= 500:
                    if attempt < max_retries:
                        wait_time = int(self._calculate_backoff_delay(attempt))
                        self.logger.warning(
                            f"ã‚µãƒ¼ãƒãƒ¼ã‚¨ãƒ©ãƒ¼ ({response.status_code}): {wait_time}ç§’å¾Œã«ãƒªãƒˆãƒ©ã‚¤ã—ã¾ã™"
                        )
                        await asyncio.sleep(wait_time)
                        continue

                # ãã®ä»–ã®ã‚¨ãƒ©ãƒ¼ã¯æœ€å¾Œã®è©¦è¡Œã®å ´åˆã¯è¿”ã™
                if attempt == max_retries:
                    return response

            except Exception as e:
                if attempt == max_retries:
                    raise e

                wait_time = int(self._calculate_backoff_delay(attempt))
                self.logger.warning(f"ãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚¨ãƒ©ãƒ¼: {e}, {wait_time}ç§’å¾Œã«ãƒªãƒˆãƒ©ã‚¤ã—ã¾ã™")
                await asyncio.sleep(wait_time)

        return None

    async def collect(
        self,
        target_dates: list[date] | None = None,
        **kwargs,
    ) -> list[tuple[str, str]]:
        """5chanã‹ã‚‰AIé–¢é€£ã‚¹ãƒ¬ãƒƒãƒ‰ã‚’åé›†ã—ã¾ã™ã€‚

        Parameters
        ----------
        target_dates : list[date], optional
            åé›†å¯¾è±¡æ—¥ä»˜ãƒªã‚¹ãƒˆã€‚

        Returns
        -------
        list[tuple[str, str]]
            ä¿å­˜ã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ã®ãƒªã‚¹ãƒˆï¼ˆJSON, Markdownï¼‰ã€‚

        """
        try:
            log_processing_start(self.logger, "5chan AIé–¢é€£ã‚¹ãƒ¬ãƒƒãƒ‰")

            # å¯¾è±¡æ—¥ä»˜ã®æ­£è¦åŒ–
            effective_target_dates = set(target_dates) if target_dates else target_dates_set(1)

            # æ—¢å­˜ã‚¿ã‚¤ãƒˆãƒ«ã‚’èª­ã¿è¾¼ã¿
            existing_titles: set[str] = set()
            for target_date in effective_target_dates:
                target_datetime = datetime.combine(target_date, datetime.min.time())
                existing_records = await self._load_existing_threads(target_datetime)
                existing_titles.update(r.get("title", "") for r in existing_records)

            self.logger.info(f"ğŸ” æ—¢å­˜ã‚¿ã‚¤ãƒˆãƒ«æ•°: {len(existing_titles)}ä»¶")

            # å„æ¿ã‹ã‚‰ã‚¹ãƒ¬ãƒƒãƒ‰ä¸€è¦§ã‚’å–å¾—
            all_threads: list[Thread] = []
            self.logger.info("\nğŸ“¡ æ¿ã‹ã‚‰ã‚¹ãƒ¬ãƒƒãƒ‰å–å¾—ä¸­...")

            for board_id in self.target_boards.keys():
                try:
                    threads_data = await self._get_subject_txt_data(board_id)

                    if not threads_data:
                        self.logger.warning(f"   â€¢ {board_id}: ã‚¹ãƒ¬ãƒƒãƒ‰ä¸€è¦§ã®å–å¾—ã«å¤±æ•—")
                        continue

                    self.logger.info(f"   â€¢ {board_id}: {len(threads_data)}ä»¶ã®ã‚¹ãƒ¬ãƒƒãƒ‰ã‚’å–å¾—")

                    # ã‚¹ãƒ¬ãƒƒãƒ‰ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‚’ä½œæˆ
                    for thread_data in threads_data:
                        # æ—¢å­˜ã‚¿ã‚¤ãƒˆãƒ«ã¨é‡è¤‡ãƒã‚§ãƒƒã‚¯
                        if thread_data["title"] in existing_titles:
                            continue

                        # æ—¥ä»˜ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
                        thread_timestamp = int(thread_data["timestamp"])
                        thread_datetime = datetime.fromtimestamp(thread_timestamp, tz=timezone.utc)
                        if not is_within_target_dates(thread_datetime, effective_target_dates):
                            continue

                        # ã‚¹ãƒ¬ãƒƒãƒ‰è©³ç´°ã‚’å–å¾—
                        posts, error = await self._get_thread_posts_from_dat(thread_data["dat_url"])

                        if error or not posts:
                            continue

                        # Threadã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‚’ä½œæˆ
                        thread = Thread(
                            thread_id=thread_timestamp,
                            title=thread_data["title"],
                            url=thread_data["html_url"],
                            board=board_id,
                            timestamp=thread_timestamp,
                            posts=posts,
                            popularity_score=float(len(posts)),
                        )

                        all_threads.append(thread)

                except Exception as e:
                    self.logger.error(f"   â€¢ {board_id}: ã‚¨ãƒ©ãƒ¼ - {e}")
                    continue

            # æ—¥ä»˜ã”ã¨ã«ã‚¹ãƒ¬ãƒƒãƒ‰ã‚’ã‚°ãƒ«ãƒ¼ãƒ—åŒ–
            threads_by_date: dict[date, list[Thread]] = {}
            for thread in all_threads:
                thread_date = datetime.fromtimestamp(thread.timestamp, tz=timezone.utc).date()
                if thread_date not in threads_by_date:
                    threads_by_date[thread_date] = []
                threads_by_date[thread_date].append(thread)

            # å„æ—¥ç‹¬ç«‹ã§ä¸Šä½15ä»¶ã‚’é¸æŠã—ã¦çµåˆ
            selected_threads = []
            for target_date in sorted(effective_target_dates):
                if target_date in threads_by_date:
                    date_threads = threads_by_date[target_date]
                    if len(date_threads) <= self.TOTAL_LIMIT:
                        selected_threads.extend(date_threads)
                    else:

                        def sort_key(thread: Thread):
                            created = datetime.fromtimestamp(thread.timestamp, tz=timezone.utc)
                            return (thread.popularity_score, created)

                        sorted_threads = sorted(date_threads, key=sort_key, reverse=True)
                        selected_threads.extend(sorted_threads[: self.TOTAL_LIMIT])

            # æ—¢å­˜/æ–°è¦ã‚¹ãƒ¬ãƒƒãƒ‰æ•°ã‚’ã‚«ã‚¦ãƒ³ãƒˆ
            existing_count = 0  # æ—¢å­˜ã‚¹ãƒ¬ãƒƒãƒ‰æ•°ï¼ˆç°¡ç•¥åŒ–ï¼‰
            new_count = len(selected_threads)  # æ–°è¦ã‚¹ãƒ¬ãƒƒãƒ‰æ•°

            # ã‚¹ãƒ¬ãƒƒãƒ‰æƒ…å ±ã‚’è¡¨ç¤º
            log_article_counts(self.logger, existing_count, new_count)

            if selected_threads:
                log_summary_candidates(self.logger, selected_threads, "popularity_score")

                # è¦ç´„ç”Ÿæˆ
                log_summarization_start(self.logger)
                for idx, thread in enumerate(selected_threads, 1):
                    await self._summarize_thread(thread)
                    log_summarization_progress(
                        self.logger, idx, len(selected_threads), thread.title
                    )

            # è¦ç´„ã‚’ä¿å­˜
            saved_files: list[tuple[str, str]] = []
            if selected_threads:
                # ã‚¹ãƒ¬ãƒƒãƒ‰ã‚’ä¿å­˜
                saved_files = await self._store_summaries(
                    selected_threads, sorted(effective_target_dates)
                )

                # å‡¦ç†å®Œäº†ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸
                if saved_files:
                    self.logger.info(f"\nğŸ’¾ {len(saved_files)}æ—¥åˆ†ã®ãƒ‡ãƒ¼ã‚¿ã‚’ä¿å­˜å®Œäº†")
                    for json_path, md_path in saved_files:
                        log_storage_complete(self.logger, json_path, md_path)
                else:
                    log_no_new_articles(self.logger)
            else:
                log_no_new_articles(self.logger)

            return saved_files

        finally:
            # ã‚°ãƒ­ãƒ¼ãƒãƒ«ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆãªã®ã§ã‚¯ãƒ­ãƒ¼ã‚ºä¸è¦
            pass

    def _build_board_url(self, board_id: str, server: str) -> str:
        """æ¿URLã‚’æ§‹ç¯‰ã—ã¾ã™ã€‚

        Parameters
        ----------
        board_id : str
            æ¿ã®IDã€‚
        server : str
            ã‚µãƒ¼ãƒãƒ¼ã®ãƒ›ã‚¹ãƒˆåã€‚

        Returns
        -------
        str
            æ§‹ç¯‰ã•ã‚ŒãŸæ¿URLã€‚

        """
        return f"https://{server}/{board_id}/"

    def _get_board_server(self, board_id: str) -> str:
        """boards.tomlã‹ã‚‰æ¿ã®ã‚µãƒ¼ãƒãƒ¼æƒ…å ±ã‚’å–å¾—ã—ã¾ã™ã€‚
        TASK-068: bbsmenu.htmlä¾å­˜ã‚’é™¤å»ã—ã€é™çš„è¨­å®šã‹ã‚‰å–å¾—

        Parameters
        ----------
        board_id : str
            æ¿ã®IDã€‚

        Returns
        -------
        str
            ã‚µãƒ¼ãƒãƒ¼ã®ãƒ›ã‚¹ãƒˆåã€‚å­˜åœ¨ã—ãªã„å ´åˆã¯ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤ã€‚

        """
        # boards.tomlã‹ã‚‰ç›´æ¥ã‚µãƒ¼ãƒãƒ¼æƒ…å ±ã‚’å–å¾—ï¼ˆbbsmenu.htmlä¾å­˜é™¤å»ï¼‰
        server = self.board_servers.get(board_id, "mevius.5ch.net")
        self.logger.info(f"æ¿ {board_id} ã®ã‚µãƒ¼ãƒãƒ¼: {server} (é™çš„è¨­å®š)")
        return server

    async def _get_with_403_tolerance(self, url: str, board_id: str) -> Any:
        """403ã‚¨ãƒ©ãƒ¼è€æ€§HTTP GETãƒªã‚¯ã‚¨ã‚¹ãƒˆ - think harderã®çµæœ
        è¤‡æ•°ã®User-Agentã€ãƒ˜ãƒƒãƒ€ãƒ¼æˆ¦ç•¥ã€é–“éš”èª¿æ•´ã‚’è©¦è¡Œ

        Parameters
        ----------
        url : str
            ãƒªã‚¯ã‚¨ã‚¹ãƒˆURL
        board_id : str
            æ¿IDï¼ˆãƒ­ã‚°ç”¨ï¼‰

        Returns
        -------
        any
            HTTPãƒ¬ã‚¹ãƒãƒ³ã‚¹ï¼ˆæˆåŠŸæ™‚ã®ã¿ã€å¤±æ•—æ™‚ã¯Noneï¼‰

        """
        strategies: list[dict[str, dict[str, str] | float]] = [
            # æˆ¦ç•¥1: æ¨™æº–çš„ãªãƒ–ãƒ©ã‚¦ã‚¶ãƒ˜ãƒƒãƒ€ãƒ¼
            {
                "headers": {
                    **self.browser_headers,
                    "User-Agent": self._get_random_user_agent(),
                },
                "wait": 1.0,
            },
            # æˆ¦ç•¥2: ã‚ˆã‚Šè©³ç´°ãªãƒ–ãƒ©ã‚¦ã‚¶ãƒ˜ãƒƒãƒ€ãƒ¼
            {
                "headers": {
                    **self.browser_headers,
                    "User-Agent": self._get_random_user_agent(),
                    "Referer": f"https://mevius.5ch.net/{board_id}/",
                },
                "wait": 2.0,
            },
            # æˆ¦ç•¥3: ã‚·ãƒ³ãƒ—ãƒ«ãªãƒ˜ãƒƒãƒ€ãƒ¼
            {
                "headers": {
                    "User-Agent": self._get_random_user_agent(),
                    "Accept": "text/html",
                },
                "wait": 3.0,
            },
        ]

        for idx, strategy in enumerate(strategies, 1):
            try:
                headers = strategy["headers"]
                if not isinstance(headers, dict):
                    continue
                if not self.http_client:
                    return None
                response = await self.http_client.get(
                    url,
                    headers=headers,
                    timeout=10.0,
                    follow_redirects=True,
                )

                if response.status_code == 200:
                    self.logger.info(f"   âœ“ æˆ¦ç•¥{idx}ã§æˆåŠŸ: {board_id}")
                    return response

                if response.status_code == 403:
                    self.logger.warning(f"   âœ— æˆ¦ç•¥{idx}ã§403ã‚¨ãƒ©ãƒ¼: {board_id}, æ¬¡ã®æˆ¦ç•¥ã‚’è©¦è¡Œ...")
                    wait_time = strategy["wait"]
                    if isinstance(wait_time, (int, float)):
                        await asyncio.sleep(wait_time)
                    continue

                # ãã®ä»–ã®ã‚¨ãƒ©ãƒ¼
                self.logger.warning(f"   âœ— æˆ¦ç•¥{idx}ã§ã‚¨ãƒ©ãƒ¼ ({response.status_code}): {board_id}")
                wait_time = strategy["wait"]
                if isinstance(wait_time, (int, float)):
                    await asyncio.sleep(wait_time)

            except Exception as e:
                self.logger.warning(f"   âœ— æˆ¦ç•¥{idx}ã§ä¾‹å¤–: {board_id} - {e}")
                wait_time = strategy["wait"]
                if isinstance(wait_time, (int, float)):
                    await asyncio.sleep(wait_time)

        self.logger.error(f"   âœ— å…¨æˆ¦ç•¥å¤±æ•—: {board_id}")
        return None

    async def _get_subject_txt_data(self, board_id: str) -> list[dict]:
        """subject.txtå½¢å¼ã§ã‚¹ãƒ¬ãƒƒãƒ‰ä¸€è¦§ã‚’å–å¾—ï¼ˆCloudflareçªç ´æˆåŠŸæ‰‹æ³•ï¼‰

        Parameters
        ----------
        board_id : str
            æ¿ID

        Returns
        -------
        List[dict]
            ã‚¹ãƒ¬ãƒƒãƒ‰æƒ…å ±ãƒªã‚¹ãƒˆ

        """
        # æˆåŠŸç¢ºèªæ¸ˆã¿ã‚µãƒ¼ãƒãƒ¼ãƒãƒƒãƒ”ãƒ³ã‚°ï¼ˆå®Ÿéš›ã®ãƒ†ã‚¹ãƒˆçµæœã«åŸºã¥ãï¼‰
        server_mapping = {
            "ai": ["krsw.5ch.net", "egg.5ch.net", "mevius.5ch.net"],
            "prog": ["medaka.5ch.net", "mevius.5ch.net"],
            "tech": ["mevius.5ch.net"],  # ä¿®æ­£: techã¯mevius.5ch.netã®ã¿
            "esite": ["mevius.5ch.net"],  # ä¿®æ­£: esiteã¯mevius.5ch.netã®ã¿
            "software": ["egg.5ch.net"],
            "bizplus": ["egg.5ch.net"],
            "news": ["hayabusa9.5ch.net"],
        }

        servers = server_mapping.get(board_id, [self._get_board_server(board_id)])

        headers = {
            "User-Agent": "Mozilla/5.0 (compatible; 5ch subject reader)",
            "Accept": "text/plain",
        }

        for server in servers:
            try:
                url = f"https://{server}/{board_id}/subject.txt"
                self.logger.info(f"subject.txtå–å¾—: {url}")

                # ç›´æ¥httpxã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã‚’ä½¿ç”¨ï¼ˆ403å›é¿ã®ãŸã‚ï¼‰

                async with httpx.AsyncClient() as client:
                    response = await client.get(url, headers=headers, timeout=10.0)

                if response.status_code == 200:
                    # æ–‡å­—åŒ–ã‘å¯¾ç­–ï¼ˆShift_JIS + ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼‰
                    try:
                        content = response.content.decode("shift_jis", errors="ignore")
                    except (UnicodeDecodeError, LookupError):
                        try:
                            content = response.content.decode("cp932", errors="ignore")
                        except (UnicodeDecodeError, LookupError):
                            try:
                                content = response.content.decode("utf-8", errors="ignore")
                            except (UnicodeDecodeError, LookupError):
                                content = response.text

                    threads_data = []
                    lines = content.split("\n")

                    import re

                    for line in lines:
                        if line.strip():
                            # datå½¢å¼è§£æ: timestamp.dat<>title (post_count)
                            match = re.match(r"(\d+)\.dat<>(.+?)\s+\((\d+)\)", line)
                            if match:
                                timestamp, title, post_count = match.groups()
                                threads_data.append(
                                    {
                                        "server": server,
                                        "board": board_id,
                                        "timestamp": timestamp,
                                        "title": title.strip(),
                                        "post_count": int(post_count),
                                        "dat_url": f"https://{server}/{board_id}/dat/{timestamp}.dat",
                                        "html_url": f"https://{server}/test/read.cgi/{board_id}/{timestamp}/",
                                    }
                                )

                    self.logger.info(f"subject.txtæˆåŠŸ: {len(threads_data)}ã‚¹ãƒ¬ãƒƒãƒ‰å–å¾—")
                    return threads_data

            except Exception as e:
                self.logger.warning(f"subject.txtå¤±æ•— {server}: {e}")
                continue

        self.logger.error(f"subject.txtå–å¾—å¤±æ•—ï¼ˆå…¨ã‚µãƒ¼ãƒãƒ¼ï¼‰: {board_id}")
        return []

    async def _get_thread_posts_from_dat(self, dat_url: str) -> tuple[list[Post], str | None]:
        """.datãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰æŠ•ç¨¿ã‚’å–å¾—ã—ã¾ã™ã€‚

        Parameters
        ----------
        dat_url : str
            .datãƒ•ã‚¡ã‚¤ãƒ«ã®URLã€‚

        Returns
        -------
        tuple[list[Post], str | None]
            æŠ•ç¨¿ãƒªã‚¹ãƒˆã¨ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ï¼ˆã‚¨ãƒ©ãƒ¼ãŒãªã„å ´åˆã¯Noneï¼‰ã€‚

        """
        try:
            # cloudscraperã‚’ä½¿ç”¨ã—ã¦Cloudflareä¿è­·ã‚’çªç ´
            scraper = cloudscraper.create_scraper(
                browser={
                    "browser": "chrome",
                    "platform": "windows",
                    "mobile": False,
                }
            )

            # éåŒæœŸå®Ÿè¡Œã®ãŸã‚ã«asyncio.to_threadã‚’ä½¿ç”¨
            response = await asyncio.to_thread(scraper.get, dat_url, timeout=10)

            if response.status_code != 200:
                return [], f"HTTP {response.status_code}"

            # Shift_JISã§ãƒ‡ã‚³ãƒ¼ãƒ‰
            try:
                content = response.content.decode("shift_jis", errors="ignore")
            except (UnicodeDecodeError, LookupError):
                try:
                    content = response.content.decode("cp932", errors="ignore")
                except (UnicodeDecodeError, LookupError):
                    content = response.text

            # .datå½¢å¼ã‚’ãƒ‘ãƒ¼ã‚¹
            posts = []
            lines = content.split("\n")

            # æœ€å¤§æŠ•ç¨¿æ•°ã‚’åˆ¶é™ï¼ˆãƒ¡ãƒ¢ãƒªåŠ¹ç‡åŒ–ï¼‰
            limited_lines = lines[:MAX_POSTS_PER_THREAD]

            for idx, line in enumerate(limited_lines, 1):
                if not line.strip():
                    continue

                # .datå½¢å¼: name<>mail<>date<>content<>title
                parts = line.split("<>")
                if len(parts) >= 4:
                    post = Post(
                        no=idx,
                        name=parts[0].strip(),
                        mail=parts[1].strip(),
                        date=parts[2].strip(),
                        content=parts[3].strip(),
                    )
                    posts.append(post)

            return posts, None

        except Exception as e:
            return [], str(e)

    async def _summarize_thread(self, thread: Thread) -> None:
        """ã‚¹ãƒ¬ãƒƒãƒ‰ã‚’è¦ç´„ã—ã¾ã™ã€‚

        Parameters
        ----------
        thread : Thread
            è¦ç´„å¯¾è±¡ã®ã‚¹ãƒ¬ãƒƒãƒ‰ã€‚

        """
        # æŠ•ç¨¿å†…å®¹ã‚’çµåˆ
        thread_content = "\n".join(
            f"[{post.no}] {post.name} ({post.date}): {post.content}"
            for post in thread.posts[:MAX_POSTS_PER_THREAD]
        )

        prompt = f"""
        ä»¥ä¸‹ã®5chanã‚¹ãƒ¬ãƒƒãƒ‰ã‚’è¦ç´„ã—ã¦ãã ã•ã„:

        ã‚¿ã‚¤ãƒˆãƒ«: {thread.title}
        æ¿: {thread.board}

        {thread_content}

        è¦ç´„ã¯ä»¥ä¸‹ã®å½¢å¼ã§è¡Œã„ã€æ—¥æœ¬èªã§å›ç­”ã—ã¦ãã ã•ã„:
        1. ã‚¹ãƒ¬ãƒƒãƒ‰ã®ä¸»ãªå†…å®¹ï¼ˆ1-2æ–‡ï¼‰
        2. è­°è«–ã®ä¸»è¦ãƒã‚¤ãƒ³ãƒˆï¼ˆç®‡æ¡æ›¸ã3-5ç‚¹ï¼‰
        3. ã‚¹ãƒ¬ãƒƒãƒ‰ã®å…¨ä½“çš„ãªè«–èª¿

        æ³¨æ„ï¼šæ”»æ’ƒçš„ãªå†…å®¹ã‚„ãƒ˜ã‚¤ãƒˆã‚¹ãƒ”ãƒ¼ãƒã¯ç·©å’Œã—ã€ä¸»è¦ãªæŠ€è¡“çš„è­°è«–ã«ç„¦ç‚¹ã‚’å½“ã¦ã¦ãã ã•ã„ã€‚
        """

        system_instruction = """
        ã‚ãªãŸã¯5chanï¼ˆæ—§2ã¡ã‚ƒã‚“ã­ã‚‹ï¼‰ã‚¹ãƒ¬ãƒƒãƒ‰ã®è¦ç´„ã‚’è¡Œã†ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚
        æŠ•ç¨¿ã•ã‚ŒãŸå†…å®¹ã‚’å®¢è¦³çš„ã«åˆ†æã—ã€æŠ€è¡“çš„è­°è«–ã‚„æƒ…å ±ã«ç„¦ç‚¹ã‚’å½“ã¦ãŸè¦ç´„ã‚’æä¾›ã—ã¦ãã ã•ã„ã€‚
        éåº¦ãªæ”»æ’ƒæ€§ã€ãƒ˜ã‚¤ãƒˆã‚¹ãƒ”ãƒ¼ãƒã€å·®åˆ¥çš„å†…å®¹ã¯ä¸­å’Œã—ã¦è¡¨ç¾ã—ã€æœ‰ç›Šãªæƒ…å ±ã®ã¿ã‚’æŠ½å‡ºã—ã¦ãã ã•ã„ã€‚
        å›ç­”ã¯æ—¥æœ¬èªã§è¡Œã„ã€AIã‚„ãƒ†ã‚¯ãƒãƒ­ã‚¸ãƒ¼ã«é–¢é€£ã™ã‚‹æƒ…å ±ã‚’å„ªå…ˆçš„ã«å«ã‚ã¦ãã ã•ã„ã€‚
        """

        try:
            summary = self.gpt_client.generate_content(
                prompt=prompt,
                system_instruction=system_instruction,
                temperature=0.3,
                max_tokens=1000,
            )
            thread.summary = summary
        except Exception as e:
            self.logger.error(f"è¦ç´„ã®ç”Ÿæˆä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e!s}")
            thread.summary = f"è¦ç´„ã®ç”Ÿæˆä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e!s}"

    async def _store_summaries(
        self, threads: list[Thread], target_dates: list[date]
    ) -> list[tuple[str, str]]:
        if not threads:
            self.logger.info("ä¿å­˜ã™ã‚‹ã‚¹ãƒ¬ãƒƒãƒ‰ãŒã‚ã‚Šã¾ã›ã‚“")
            return []

        default_date = max(target_dates) if target_dates else datetime.now().date()
        records = self._serialize_threads(threads)
        records_by_date = group_records_by_date(records, default_date=default_date)

        saved_files = await store_daily_snapshots(
            records_by_date,
            load_existing=self._load_existing_threads,
            save_json=self.save_json,
            save_markdown=self.save_markdown,
            render_markdown=self._render_markdown,
            key=lambda item: item.get("thread_id"),
            sort_key=self._thread_sort_key,
            limit=self.TOTAL_LIMIT,
            logger=self.logger,
        )

        return saved_files

    def _serialize_threads(self, threads: list[Thread]) -> list[dict]:
        records: list[dict] = []
        for thread in threads:
            published = datetime.fromtimestamp(thread.timestamp, tz=timezone.utc)
            records.append(
                {
                    "thread_id": thread.thread_id,
                    "title": thread.title,
                    "url": thread.url,
                    "timestamp": thread.timestamp,
                    "summary": thread.summary,
                    "popularity_score": thread.popularity_score,
                    "board": thread.board,
                    "published_at": published.isoformat(),
                }
            )
        return records

    async def _load_existing_threads(self, target_date: datetime) -> list[dict]:
        date_str = target_date.strftime("%Y-%m-%d")
        filename_json = f"{date_str}.json"
        existing_json = await self.load_json(filename_json)
        if existing_json:
            if isinstance(existing_json, dict):
                flattened: list[dict] = []
                for board, items in existing_json.items():
                    for item in items:
                        flattened.append({"board": board, **item})
                return flattened
            return existing_json

        markdown = await self.storage.load(f"{date_str}.md")
        if not markdown:
            return []

        return self._parse_markdown(markdown)

    def _thread_sort_key(self, item: dict) -> tuple[float, datetime]:
        popularity = float(item.get("popularity_score", 0.0) or 0.0)
        published_raw = item.get("published_at")
        published: datetime
        if published_raw:
            try:
                published = datetime.fromisoformat(published_raw)
            except ValueError:
                published = datetime.min.replace(tzinfo=timezone.utc)
        else:
            timestamp = item.get("timestamp")
            if timestamp:
                try:
                    published = datetime.fromtimestamp(int(timestamp), tz=timezone.utc)
                except Exception:
                    published = datetime.min.replace(tzinfo=timezone.utc)
            else:
                published = datetime.min.replace(tzinfo=timezone.utc)
        return (popularity, published)

    def _render_markdown(self, records: list[dict], today: datetime) -> str:
        content = f"# 5chan AIé–¢é€£ã‚¹ãƒ¬ãƒƒãƒ‰ ({today.strftime('%Y-%m-%d')})\n\n"
        grouped: dict[str, list[dict]] = {}
        for record in records:
            board = record.get("board", "unknown")
            grouped.setdefault(board, []).append(record)

        for board, threads in grouped.items():
            board_name = self.target_boards.get(board, board)
            content += f"## {board_name} (/{board}/)\n\n"
            for thread in threads:
                title = thread.get("title") or f"ç„¡é¡Œã‚¹ãƒ¬ãƒƒãƒ‰ #{thread.get('thread_id')}"
                content += f"### [{title}]({thread.get('url')})\n\n"
                published_raw = thread.get("published_at")
                if published_raw:
                    try:
                        date_str = datetime.fromisoformat(published_raw).strftime(
                            "%Y-%m-%d %H:%M:%S"
                        )
                    except ValueError:
                        date_str = published_raw
                else:
                    timestamp = thread.get("timestamp")
                    if timestamp:
                        try:
                            date_str = datetime.fromtimestamp(
                                int(timestamp), tz=timezone.utc
                            ).strftime("%Y-%m-%d %H:%M:%S")
                        except Exception:
                            date_str = "N/A"
                    else:
                        date_str = "N/A"
                content += f"ä½œæˆæ—¥æ™‚: {date_str}\n\n"
                content += f"**è¦ç´„**:\n{thread.get('summary', '')}\n\n"
                content += "---\n\n"
        return content

    def _parse_markdown(self, markdown: str) -> list[dict]:
        records: list[dict] = []
        board_pattern = re.compile(r"^##\s+(.+) \(/(.+)/\)$", re.MULTILINE)
        thread_pattern = re.compile(
            r"### \[(?P<title>.+?)\]\((?P<url>[^\)]+)\)\n\n"
            r"ä½œæˆæ—¥æ™‚: (?P<datetime>.+?)\n\n"
            r"\*\*è¦ç´„\*\*:\n(?P<summary>.*?)(?:\n\n)?---",
            re.DOTALL,
        )

        sections = list(board_pattern.finditer(markdown))
        for idx, match in enumerate(sections):
            start = match.end()
            end = sections[idx + 1].start() if idx + 1 < len(sections) else len(markdown)
            block = markdown[start:end]
            board_id = match.group(2).strip()

            for thread_match in thread_pattern.finditer(block + "---"):
                title = thread_match.group("title")
                url = thread_match.group("url")
                summary = thread_match.group("summary").strip()
                datetime_str = thread_match.group("datetime")
                try:
                    published = datetime.strptime(datetime_str, "%Y-%m-%d %H:%M:%S")
                except ValueError:
                    published = None

                record = {
                    "thread_id": 0,
                    "title": title.strip(),
                    "url": url.strip(),
                    "summary": summary,
                    "popularity_score": 0.0,
                    "board": board_id,
                }

                if published:
                    record["published_at"] = published.replace(tzinfo=timezone.utc).isoformat()
                    record["timestamp"] = int(published.timestamp())

                records.append(record)

        return records
