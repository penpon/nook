"""ã‚³ãƒ³ãƒ†ãƒ³ãƒ„APIãƒ«ãƒ¼ã‚¿ãƒ¼ã€‚"""

from datetime import datetime

from fastapi import APIRouter, HTTPException, Response

from nook.api.models.schemas import ContentItem, ContentResponse
from nook.core.config import BaseConfig
from nook.core.storage import LocalStorage
from nook.services.explorers.trendradar.utils import parse_popularity_score

router = APIRouter()
storage = LocalStorage(BaseConfig().DATA_DIR)

# è«–æ–‡è¦ç´„ã®è³ªå•æ–‡ã‚’èª­ã¿ã‚„ã™ã„ã‚¿ã‚¤ãƒˆãƒ«ã«å¤‰æ›ã™ã‚‹ãƒãƒƒãƒ”ãƒ³ã‚°
PAPER_SUMMARY_TITLE_MAPPING = {
    "1. æ—¢å­˜ç ”ç©¶ã§ã¯ä½•ãŒã§ããªã‹ã£ãŸã®ã‹": "ğŸ” ç ”ç©¶èƒŒæ™¯ã¨èª²é¡Œ",
    "2. ã©ã®ã‚ˆã†ãªã‚¢ãƒ—ãƒ­ãƒ¼ãƒã§ãã‚Œã‚’è§£æ±ºã—ã‚ˆã†ã¨ã—ãŸã‹": "ğŸ’¡ ææ¡ˆæ‰‹æ³•",
    "3. çµæœã€ä½•ãŒé”æˆã§ããŸã®ã‹": "ğŸ¯ ä¸»è¦ãªæˆæœ",
    "4. åˆ¶é™ã‚„å•é¡Œç‚¹ã¯ä½•ã§ã™ã‹ã€‚æœ¬æ–‡ã§è¨€åŠã•ã‚Œã¦ã„ã‚‹ã‚„ã‚ãªãŸãŒè€ƒãˆã‚‹ã‚‚ã®ã‚‚å«ã‚ã¦æ•™ãˆã¦ãã ã•ã„": "âš ï¸ é™ç•Œã¨ä»Šå¾Œã®èª²é¡Œ",
    "5. æŠ€è¡“çš„ãªè©³ç´°ã«ã¤ã„ã¦ã€‚æŠ€è¡“è€…ãŒèª­ã‚€ã“ã¨ã‚’æƒ³å®šã—ãŸãƒˆãƒ¼ãƒ³ã§æ•™ãˆã¦ãã ã•ã„": "ğŸ”§ æŠ€è¡“è©³ç´°",
    "6. ã‚³ã‚¹ãƒˆã‚„ç‰©ç†çš„ãªè©³ç´°ã«ã¤ã„ã¦æ•™ãˆã¦ãã ã•ã„ã€‚ä¾‹ãˆã°ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã«ä½¿ç”¨ã—ãŸGPUã®æ•°ã‚„æ™‚é–“ã€ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã€ãƒ¢ãƒ‡ãƒ«ã®ã‚µã‚¤ã‚ºãªã©": "ğŸ’» è¨ˆç®—ãƒªã‚½ãƒ¼ã‚¹ã¨è¦æ¨¡",
    "7. å‚è€ƒæ–‡çŒ®ã®ã†ã¡ã€ç‰¹ã«å‚ç…§ã™ã¹ãã‚‚ã®ã‚’æ•™ãˆã¦ãã ã•ã„": "ğŸ“š é‡è¦ãªé–¢é€£ç ”ç©¶",
    "8. ã“ã®è«–æ–‡ã‚’140å­—ä»¥å†…ã§è¦ç´„ã™ã‚‹ã¨ã©ã†ãªã‚Šã¾ã™ã‹ï¼Ÿ": "ğŸ“ 140å­—è¦ç´„",
}


def convert_paper_summary_titles(content: str) -> str:
    """è«–æ–‡è¦ç´„ã®è³ªå•æ–‡ã‚’èª­ã¿ã‚„ã™ã„ã‚¿ã‚¤ãƒˆãƒ«ã«å¤‰æ›"""
    result = content

    # å„è³ªå•æ–‡ã‚’å¯¾å¿œã™ã‚‹ã‚¿ã‚¤ãƒˆãƒ«ã«ç½®æ›
    for original_title in PAPER_SUMMARY_TITLE_MAPPING:
        # è³ªå•æ–‡ã®å…¨ä½“ã¾ãŸã¯ä¸€éƒ¨ã«ãƒãƒƒãƒã™ã‚‹ã‚ˆã†èª¿æ•´
        # "4. åˆ¶é™ã‚„å•é¡Œç‚¹ã¯ä½•ã§ã™ã‹ã€‚"ã®ã‚ˆã†ãªè³ªå•æ–‡ã«å¯¾å¿œ
        if original_title in result:
            result = result.replace(
                original_title, PAPER_SUMMARY_TITLE_MAPPING[original_title]
            )

    return result


SOURCE_MAPPING = {
    "arxiv": "arxiv_summarizer",
    "github": "github_trending",
    "hacker-news": "hacker_news",
    "tech-news": "tech_feed",
    "business-news": "business_feed",
    "zenn": "zenn_explorer",
    "qiita": "qiita_explorer",
    "note": "note_explorer",
    "reddit": "reddit_explorer",
    "4chan": "fourchan_explorer",
    "5chan": "fivechan_explorer",
    "trendradar-zhihu": "trendradar-zhihu",
    "trendradar-juejin": "trendradar-juejin",
    "trendradar-ithome": "trendradar-ithome",
}


def _create_content_item(
    title: str, content: str, source: str, url: str | None = None
) -> ContentItem:
    """ContentItemã‚’ä½œæˆã™ã‚‹ãƒ˜ãƒ«ãƒ‘ãƒ¼é–¢æ•°"""
    return ContentItem(
        title=title,
        content=content,
        url=url,
        source=source,
    )


def _process_trendradar_articles(
    articles_data: list[dict], source: str
) -> list[ContentItem]:
    """TrendRadarç³»è¨˜äº‹ã‚’ContentItemãƒªã‚¹ãƒˆã«å¤‰æ›ã™ã‚‹å…±é€šé–¢æ•°.

    Parameters
    ----------
    articles_data : list[dict]
        TrendRadarç³»ã‚µãƒ¼ãƒ“ã‚¹ã‹ã‚‰å–å¾—ã—ãŸè¨˜äº‹ãƒ‡ãƒ¼ã‚¿ã®ãƒªã‚¹ãƒˆã€‚
    source : str
        ã‚½ãƒ¼ã‚¹åï¼ˆä¾‹: "trendradar-zhihu", "trendradar-juejin"ï¼‰ã€‚

    Returns
    -------
    list[ContentItem]
        å¤‰æ›ã•ã‚ŒãŸContentItemã®ãƒªã‚¹ãƒˆã€‚
    """
    items = []
    # äººæ°—åº¦ï¼ˆpopularity_scoreï¼‰ã®é™é †ã§ã‚½ãƒ¼ãƒˆ
    # å¤‰æ›ä¸å¯èƒ½ãªå€¤ï¼ˆNone, "N/A"ç­‰ï¼‰ã¯0ã¨ã—ã¦æ‰±ã†
    # Note: sorted()ã‚’ä½¿ç”¨ã—ã¦å…ƒã®ãƒªã‚¹ãƒˆã‚’å¤‰æ›´ã—ãªã„ï¼ˆå‰¯ä½œç”¨é˜²æ­¢ï¼‰
    sorted_articles = sorted(
        articles_data,
        key=lambda x: parse_popularity_score(x.get("popularity_score")),
        reverse=True,
    )

    for article in sorted_articles:
        content = ""
        if article.get("summary"):
            # è¦ç´„ã¯æ—¢ã«Markdownå½¢å¼ã§æ§‹é€ åŒ–ã•ã‚Œã¦ã„ã‚‹ãŸã‚ã€ãã®ã¾ã¾ä½¿ç”¨
            content = f"{article['summary']}\n\n"
        if article.get("category"):
            content += f"ã‚«ãƒ†ã‚´ãƒª: {article['category']}"

        items.append(
            _create_content_item(
                title=article.get("title", ""),
                content=content,
                url=article.get("url"),
                source=source,
            )
        )
    return items


@router.get("/content/{source}", response_model=ContentResponse)
async def get_content(
    source: str, date: str | None = None, response: Response = None
) -> ContentResponse:
    """
    ç‰¹å®šã®ã‚½ãƒ¼ã‚¹ã®ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’å–å¾—ã—ã¾ã™ã€‚

    Parameters
    ----------
    source : str
        ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹ï¼ˆreddit, hackernews, github, techfeed, paperï¼‰ã€‚
    date : str, optional
        è¡¨ç¤ºã™ã‚‹æ—¥ä»˜ï¼ˆYYYY-MM-DDå½¢å¼ï¼‰ã€‚

    Returns
    -------
    ContentResponse
        ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã€‚

    Raises
    ------
    HTTPException
        ã‚½ãƒ¼ã‚¹ãŒç„¡åŠ¹ãªå ´åˆã‚„ã€ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ãŒè¦‹ã¤ã‹ã‚‰ãªã„å ´åˆã€‚
    """
    if source not in SOURCE_MAPPING and source != "all":
        raise HTTPException(status_code=404, detail=f"Source '{source}' not found")

    # ã‚­ãƒ£ãƒƒã‚·ãƒ¥åˆ¶å¾¡ãƒ˜ãƒƒãƒ€ãƒ¼ã‚’è¨­å®šï¼ˆã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’ç„¡åŠ¹åŒ–ï¼‰
    if response:
        response.headers["Cache-Control"] = (
            "no-store, no-cache, must-revalidate, max-age=0"
        )
        response.headers["Pragma"] = "no-cache"
        response.headers["Expires"] = "0"

    # æ—¥ä»˜ã®å‡¦ç†
    target_date = None
    explicit_date_requested = date is not None
    if date:
        try:
            target_date = datetime.strptime(date, "%Y-%m-%d")
        except ValueError:
            raise HTTPException(
                status_code=400, detail=f"Invalid date format: {date}"
            ) from None
    else:
        target_date = datetime.now()

    items = []

    # ç‰¹å®šã®ã‚½ãƒ¼ã‚¹ã‹ã‚‰ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’å–å¾—
    if source != "all":
        service_name = SOURCE_MAPPING[source]

        # Hacker Newsã®å ´åˆã¯JSONã‹ã‚‰å€‹åˆ¥è¨˜äº‹ã‚’å–å¾—
        if source == "hacker-news":
            stories_data = storage.load_json(service_name, target_date)
            if stories_data:
                # ã‚¹ã‚³ã‚¢ã§é™é †ã‚½ãƒ¼ãƒˆ
                sorted_stories = sorted(
                    stories_data, key=lambda x: x.get("score", 0), reverse=True
                )
                for story in sorted_stories:
                    # è¦ç´„ãŒã‚ã‚Œã°è¦ç´„ã‚’ã€ãªã‘ã‚Œã°æœ¬æ–‡ã‚’ä½¿ç”¨
                    content = ""
                    if story.get("summary"):
                        content = f"**è¦ç´„**:\n{story['summary']}\n\n"
                    elif story.get("text"):
                        text_preview = story["text"][:1000]
                        if len(story["text"]) > 1000:
                            text_preview += "..."
                        content = f"{text_preview}\n\n"

                    content += f"ã‚¹ã‚³ã‚¢: {story['score']}"

                    items.append(
                        _create_content_item(
                            title=story["title"],
                            content=content,
                            url=story.get("url"),
                            source=source,
                        )
                    )
        # TrendRadar (Zhihu/Juejin/ITHome) ã®å ´åˆã¯JSONã‹ã‚‰å€‹åˆ¥è¨˜äº‹ã‚’å–å¾—
        elif source in ("trendradar-zhihu", "trendradar-juejin", "trendradar-ithome"):
            articles_data = storage.load_json(service_name, target_date)
            if articles_data:
                items.extend(_process_trendradar_articles(articles_data, source))
        else:
            # ä»–ã®ã‚½ãƒ¼ã‚¹ã¯å¾“æ¥é€šã‚ŠMarkdownã‹ã‚‰å–å¾—
            content = storage.load_markdown(service_name, target_date)

            if content:
                # è«–æ–‡è¦ç´„ã®å ´åˆã¯ã‚¿ã‚¤ãƒˆãƒ«ã‚’å¤‰æ›
                if source == "arxiv":
                    content = convert_paper_summary_titles(content)

                # ãƒãƒ¼ã‚¯ãƒ€ã‚¦ãƒ³ã‹ã‚‰ContentItemã‚’ä½œæˆ
                items.append(
                    _create_content_item(
                        title=(
                            ""
                            if source == "github"
                            else f"{_get_source_display_name(source)} - "
                            f"{target_date.strftime('%Y-%m-%d')}"
                        ),
                        content=content,
                        source=source,
                    )
                )
    else:
        # ã™ã¹ã¦ã®ã‚½ãƒ¼ã‚¹ã‹ã‚‰ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’å–å¾—
        for src, service_name in SOURCE_MAPPING.items():
            if src == "hacker-news":
                # Hacker Newsã¯å€‹åˆ¥è¨˜äº‹ã¨ã—ã¦è¿½åŠ 
                stories_data = storage.load_json(service_name, target_date)
                if stories_data:
                    # ã‚¹ã‚³ã‚¢ã§é™é †ã‚½ãƒ¼ãƒˆ
                    sorted_stories = sorted(
                        stories_data, key=lambda x: x.get("score", 0), reverse=True
                    )
                    for story in sorted_stories:
                        # è¦ç´„ãŒã‚ã‚Œã°è¦ç´„ã‚’ã€ãªã‘ã‚Œã°æœ¬æ–‡ã‚’ä½¿ç”¨
                        content = ""
                        if story.get("summary"):
                            content = f"**è¦ç´„**:\n{story['summary']}\n\n"
                        elif story.get("text"):
                            text_preview = story["text"][:500]
                            if len(story["text"]) > 500:
                                text_preview += "..."
                            content = f"{text_preview}\n\n"

                        content += f"ã‚¹ã‚³ã‚¢: {story['score']}"

                        items.append(
                            _create_content_item(
                                title=story["title"],
                                content=content,
                                url=story.get("url"),
                                source=src,
                            )
                        )
            elif src in ("trendradar-zhihu", "trendradar-juejin", "trendradar-ithome"):
                # TrendRadarç³»ã¯JSONã‹ã‚‰å€‹åˆ¥è¨˜äº‹ã¨ã—ã¦è¿½åŠ 
                articles_data = storage.load_json(service_name, target_date)
                if articles_data:
                    items.extend(_process_trendradar_articles(articles_data, src))
            else:
                # ä»–ã®ã‚½ãƒ¼ã‚¹ã¯å¾“æ¥é€šã‚ŠMarkdownã‹ã‚‰å–å¾—
                content = storage.load_markdown(service_name, target_date)
                if content:
                    # è«–æ–‡è¦ç´„ã®å ´åˆã¯ã‚¿ã‚¤ãƒˆãƒ«ã‚’å¤‰æ›
                    if src == "arxiv":
                        content = convert_paper_summary_titles(content)

                    items.append(
                        _create_content_item(
                            title=(
                                ""
                                if src == "github"
                                else f"{_get_source_display_name(src)} - "
                                f"{target_date.strftime('%Y-%m-%d')}"
                            ),
                            content=content,
                            source=src,
                        )
                    )

    if not items:
        if explicit_date_requested:
            return ContentResponse(items=[])

        # åˆ©ç”¨å¯èƒ½ãªæ—¥ä»˜ã‚’ç¢ºèª
        available_dates = []
        if source != "all":
            service_name = SOURCE_MAPPING[source]
            available_dates = storage.list_dates(service_name)
        else:
            for service_name in SOURCE_MAPPING.values():
                dates = storage.list_dates(service_name)
                available_dates.extend(dates)

        if not available_dates:
            raise HTTPException(
                status_code=404,
                detail="No content available. Please run the services first.",
            )
        else:
            # æœ€æ–°ã®åˆ©ç”¨å¯èƒ½ãªæ—¥ä»˜ã®ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’å–å¾—
            latest_date = max(available_dates)
            return await get_content(source, latest_date.strftime("%Y-%m-%d"))

    return ContentResponse(items=items)


def _get_source_display_name(source: str) -> str:
    """
    ã‚½ãƒ¼ã‚¹ã®è¡¨ç¤ºåã‚’å–å¾—ã—ã¾ã™ã€‚

    Parameters
    ----------
    source : str
        ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹

    Returns
    -------
    str
        è¡¨ç¤ºå
    """
    source_names = {
        "reddit": "Reddit",
        "hacker-news": "Hacker News",
        "github": "GitHub Trending",
        "tech-news": "Tech News",
        "business-news": "Business News",
        "paper": "ArXiv",
        "zenn": "Zenn",
        "qiita": "Qiita",
        "note": "Note",
        "4chan": "4chan",
        "5chan": "5ã¡ã‚ƒã‚“ã­ã‚‹",
        "trendradar-zhihu": "çŸ¥ä¹ (Zhihu)",
        "trendradar-juejin": "æ˜é‡‘ (Juejin)",
        "trendradar-ithome": "ITä¹‹å®¶ (ITHome)",
    }
    return source_names.get(source, source)
