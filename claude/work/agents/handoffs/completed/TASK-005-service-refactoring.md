# TASK-005: ã‚µãƒ¼ãƒ“ã‚¹ã‚¯ãƒ©ã‚¹ã®ãƒªãƒ•ã‚¡ã‚¯ã‚¿ãƒªãƒ³ã‚°

## å‰²ã‚Šå½“ã¦: backend

## ç›®çš„
ã™ã¹ã¦ã®æ—¢å­˜ã‚µãƒ¼ãƒ“ã‚¹ã‚¯ãƒ©ã‚¹ã‚’æ–°ã—ã„åŸºåº•ã‚¯ãƒ©ã‚¹ï¼ˆBaseServiceï¼‰ã‚’ç¶™æ‰¿ã™ã‚‹ã‚ˆã†ã«ä¿®æ­£ã—ã€å…±é€šå‡¦ç†ã‚’çµ±ä¸€åŒ–ã™ã‚‹ã€‚

## èƒŒæ™¯
TASK-001ã§ä½œæˆã•ã‚ŒãŸåŸºåº•ã‚¯ãƒ©ã‚¹ã¨TASK-003ã§å®Ÿè£…ã•ã‚ŒãŸéåŒæœŸå‡¦ç†ã‚’æ´»ç”¨ã—ã€æ—¢å­˜ã®ã‚µãƒ¼ãƒ“ã‚¹ã‚¯ãƒ©ã‚¹ã‚’ãƒªãƒ•ã‚¡ã‚¯ã‚¿ãƒªãƒ³ã‚°ã™ã‚‹å¿…è¦ãŒã‚ã‚‹ã€‚

## å‰ææ¡ä»¶
- TASK-001ï¼ˆåŸºåº•ã‚¯ãƒ©ã‚¹ï¼‰ãŒå®Œäº†ã—ã¦ã„ã‚‹ã“ã¨
- TASK-003ï¼ˆéåŒæœŸå‡¦ç†ï¼‰ãŒå®Œäº†ã—ã¦ã„ã‚‹ã“ã¨

## å®Ÿè£…å†…å®¹

### 1. GitHubTrendingServiceã®ãƒªãƒ•ã‚¡ã‚¯ã‚¿ãƒªãƒ³ã‚°
**ãƒ•ã‚¡ã‚¤ãƒ«**: `nook/services/github_trending.py`

```python
from typing import List, Dict, Any, Optional
from datetime import datetime
import toml

from nook.common.base_service import BaseService
from nook.common.decorators import handle_errors, log_execution_time
from nook.common.exceptions import ServiceException, DataException
from nook.common.http_client import get_http_client
from nook.common.async_utils import gather_with_errors, run_with_semaphore


class GitHubTrendingService(BaseService):
    """GitHub Trending ãƒªãƒã‚¸ãƒˆãƒªåé›†ã‚µãƒ¼ãƒ“ã‚¹"""
    
    def __init__(self):
        super().__init__("github_trending")
        self.api_base_url = "https://api.github.com"
        self.languages = self._load_languages()
    
    def _load_languages(self) -> List[str]:
        """è¨€èªè¨­å®šã‚’èª­ã¿è¾¼ã¿"""
        config_path = self.get_config_path("languages.toml")
        
        try:
            with open(config_path, "r", encoding="utf-8") as f:
                config = toml.load(f)
                languages = config.get("languages", [])
                self.logger.info(f"Loaded {len(languages)} languages")
                return languages
        except FileNotFoundError:
            self.logger.warning(f"Config file not found: {config_path}")
            return ["python", "javascript", "go"]
        except Exception as e:
            raise DataException(f"Failed to load languages config: {e}") from e
    
    @log_execution_time
    @handle_errors(retries=3)
    async def collect(self) -> None:
        """ãƒˆãƒ¬ãƒ³ãƒ‰ãƒªãƒã‚¸ãƒˆãƒªã‚’åé›†"""
        self.logger.info("Starting GitHub trending collection")
        
        async with await get_http_client() as client:
            # å„è¨€èªã®ãƒˆãƒ¬ãƒ³ãƒ‰ã‚’ä¸¦è¡Œå–å¾—
            tasks = [
                self._collect_language_repos(client, lang)
                for lang in self.languages
            ]
            
            results = await gather_with_errors(
                *tasks,
                task_names=self.languages
            )
            
            # æˆåŠŸã—ãŸçµæœã®ã¿ã‚’çµ±åˆ
            all_repos = []
            for result in results:
                if result.success and result.result:
                    all_repos.extend(result.result)
                elif not result.success:
                    self.logger.error(
                        f"Failed to collect {result.name}: {result.error}"
                    )
            
            if not all_repos:
                raise ServiceException("No repositories collected")
            
            # ãƒªãƒã‚¸ãƒˆãƒªè©³ç´°ã‚’ä¸¦è¡Œå–å¾—
            enriched_repos = await self._enrich_repositories(client, all_repos)
            
            # ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆã¨ä¿å­˜
            report = await self._generate_report(enriched_repos)
            filename = f"github_trending_{datetime.now():%Y-%m-%d}.md"
            await self.save_markdown(report, filename)
            
            # ã‚µãƒãƒªãƒ¼ãƒ‡ãƒ¼ã‚¿ã‚‚ä¿å­˜
            summary = self._create_summary(enriched_repos)
            await self.save_json(summary, f"summary_{datetime.now():%Y-%m-%d}.json")
            
            self.logger.info(
                f"Successfully collected {len(enriched_repos)} repositories"
            )
    
    async def _collect_language_repos(
        self,
        client,
        language: str
    ) -> List[Dict[str, Any]]:
        """ç‰¹å®šè¨€èªã®ãƒªãƒã‚¸ãƒˆãƒªã‚’åé›†"""
        params = {
            "q": f"language:{language} created:>={datetime.now():%Y-%m-%d}",
            "sort": "stars",
            "order": "desc",
            "per_page": 10
        }
        
        url = f"{self.api_base_url}/search/repositories"
        response = await client.get_json(url, params=params)
        
        repos = response.get("items", [])
        self.logger.debug(f"Found {len(repos)} repos for {language}")
        
        return repos
    
    async def _enrich_repositories(
        self,
        client,
        repos: List[Dict[str, Any]]
    ) -> List[Dict[str, Any]]:
        """ãƒªãƒã‚¸ãƒˆãƒªã«è©³ç´°æƒ…å ±ã‚’è¿½åŠ """
        async def enrich_single_repo(repo: Dict[str, Any]) -> Dict[str, Any]:
            try:
                # READMEã‚’å–å¾—
                readme = await self._fetch_readme(client, repo)
                if readme:
                    # è¦ç´„ã‚’ç”Ÿæˆ
                    summary = await self._summarize_content(readme, repo["name"])
                    repo["ai_summary"] = summary
                else:
                    repo["ai_summary"] = repo.get("description", "")
                
                # è¿½åŠ ã®çµ±è¨ˆæƒ…å ±
                repo["quality_score"] = self._calculate_quality_score(repo)
                
                return repo
                
            except Exception as e:
                self.logger.warning(
                    f"Failed to enrich {repo['name']}: {e}",
                    extra={"repo": repo["full_name"]}
                )
                repo["ai_summary"] = repo.get("description", "")
                return repo
        
        # ä¸¦è¡Œå‡¦ç†ï¼ˆãƒ¬ãƒ¼ãƒˆåˆ¶é™ã‚’è€ƒæ…®ï¼‰
        tasks = [enrich_single_repo(repo) for repo in repos]
        enriched = await run_with_semaphore(tasks, max_concurrent=5)
        
        return enriched
    
    async def _fetch_readme(
        self,
        client,
        repo: Dict[str, Any]
    ) -> Optional[str]:
        """READMEã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’å–å¾—"""
        try:
            url = f"{repo['url']}/readme"
            headers = {"Accept": "application/vnd.github.v3.raw"}
            
            response = await client.get(url, headers=headers)
            if response.status_code == 200:
                return response.text[:5000]  # æœ€å¤§5000æ–‡å­—
                
        except Exception as e:
            self.logger.debug(f"README not found for {repo['name']}: {e}")
        
        return None
    
    async def _summarize_content(self, content: str, repo_name: str) -> str:
        """ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’AIã§è¦ç´„"""
        prompt = f"""
        ä»¥ä¸‹ã®GitHubãƒªãƒã‚¸ãƒˆãƒªã€Œ{repo_name}ã€ã®READMEã‚’ã€
        æ—¥æœ¬èªã§100æ–‡å­—ä»¥å†…ã§è¦ç´„ã—ã¦ãã ã•ã„ã€‚
        æŠ€è¡“çš„ãªç‰¹å¾´ã¨ä¸»ãªç”¨é€”ã‚’å«ã‚ã¦ãã ã•ã„ã€‚
        
        README:
        {content}
        """
        
        return await self.gpt_client.generate_async(
            prompt,
            max_tokens=200,
            temperature=0.5
        )
    
    def _calculate_quality_score(self, repo: Dict[str, Any]) -> float:
        """ãƒªãƒã‚¸ãƒˆãƒªã®å“è³ªã‚¹ã‚³ã‚¢ã‚’è¨ˆç®—"""
        score = 0.0
        
        # ã‚¹ã‚¿ãƒ¼æ•°ï¼ˆå¯¾æ•°ã‚¹ã‚±ãƒ¼ãƒ«ï¼‰
        stars = repo.get("stargazers_count", 0)
        if stars > 0:
            import math
            score += min(math.log10(stars) * 10, 40)
        
        # ãƒ•ã‚©ãƒ¼ã‚¯æ•°
        forks = repo.get("forks_count", 0)
        if forks > 0:
            score += min(forks / 10, 20)
        
        # æ›´æ–°é »åº¦
        updated = repo.get("updated_at", "")
        if updated:
            from datetime import datetime, timezone
            updated_date = datetime.fromisoformat(updated.replace("Z", "+00:00"))
            days_ago = (datetime.now(timezone.utc) - updated_date).days
            if days_ago < 30:
                score += 20
            elif days_ago < 90:
                score += 10
        
        # ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ
        if repo.get("description"):
            score += 10
        if repo.get("homepage"):
            score += 10
        
        return min(score, 100.0)
    
    async def _generate_report(self, repos: List[Dict[str, Any]]) -> str:
        """Markdownãƒ¬ãƒãƒ¼ãƒˆã‚’ç”Ÿæˆ"""
        lines = [
            f"# GitHub Trending - {datetime.now():%Yå¹´%mæœˆ%dæ—¥}",
            "",
            "æœ¬æ—¥ã®ãƒˆãƒ¬ãƒ³ãƒ‰ãƒªãƒã‚¸ãƒˆãƒªã‚’AIãŒåˆ†æã—ã¾ã—ãŸã€‚",
            "",
            "## ã‚µãƒãƒªãƒ¼",
            f"- åé›†ãƒªãƒã‚¸ãƒˆãƒªæ•°: {len(repos)}",
            f"- å¯¾è±¡è¨€èª: {', '.join(self.languages)}",
            ""
        ]
        
        # è¨€èªåˆ¥ã«ã‚°ãƒ«ãƒ¼ãƒ—åŒ–
        by_language = {}
        for repo in repos:
            lang = repo.get("language", "Other")
            if lang not in by_language:
                by_language[lang] = []
            by_language[lang].append(repo)
        
        # å“è³ªã‚¹ã‚³ã‚¢ã§ã‚½ãƒ¼ãƒˆ
        for lang, lang_repos in sorted(by_language.items()):
            lines.extend([
                f"## {lang}",
                ""
            ])
            
            sorted_repos = sorted(
                lang_repos,
                key=lambda x: x.get("quality_score", 0),
                reverse=True
            )
            
            for repo in sorted_repos[:5]:  # å„è¨€èªä¸Šä½5ä»¶
                lines.extend([
                    f"### [{repo['name']}]({repo['html_url']})",
                    "",
                    f"- â­ **{repo['stargazers_count']:,}** stars"
                    f" | ğŸ´ **{repo['forks_count']:,}** forks",
                    f"- ğŸ“Š å“è³ªã‚¹ã‚³ã‚¢: {repo['quality_score']:.1f}/100",
                    f"- ğŸ·ï¸ ãƒˆãƒ”ãƒƒã‚¯: {', '.join(repo.get('topics', [])[:5]) or 'ãªã—'}",
                    "",
                    "**æ¦‚è¦**:",
                    repo.get("ai_summary", repo.get("description", "")),
                    "",
                    "---",
                    ""
                ])
        
        return "\n".join(lines)
    
    def _create_summary(self, repos: List[Dict[str, Any]]) -> Dict[str, Any]:
        """ã‚µãƒãƒªãƒ¼ãƒ‡ãƒ¼ã‚¿ã‚’ä½œæˆ"""
        return {
            "date": datetime.now().isoformat(),
            "total_repos": len(repos),
            "languages": list({r.get("language") for r in repos if r.get("language")}),
            "top_repos": [
                {
                    "name": r["name"],
                    "url": r["html_url"],
                    "stars": r["stargazers_count"],
                    "language": r.get("language"),
                    "score": r.get("quality_score", 0)
                }
                for r in sorted(repos, key=lambda x: x.get("quality_score", 0), reverse=True)[:10]
            ],
            "stats": {
                "avg_stars": sum(r["stargazers_count"] for r in repos) / len(repos) if repos else 0,
                "avg_forks": sum(r["forks_count"] for r in repos) / len(repos) if repos else 0,
                "total_stars": sum(r["stargazers_count"] for r in repos)
            }
        }
```

### 2. RedditExplorerServiceã®ãƒªãƒ•ã‚¡ã‚¯ã‚¿ãƒªãƒ³ã‚°
**ãƒ•ã‚¡ã‚¤ãƒ«**: `nook/services/reddit_explorer.py`

```python
from typing import List, Dict, Any, Optional
from datetime import datetime
import asyncio
import toml
import praw
from prawcore.exceptions import ResponseException, RequestException

from nook.common.base_service import BaseService
from nook.common.decorators import handle_errors, log_execution_time
from nook.common.exceptions import ServiceException, APIException, ConfigurationException
from nook.common.async_utils import gather_with_errors, run_sync_in_thread


class RedditExplorerService(BaseService):
    """RedditæŠ•ç¨¿åé›†ãƒ»åˆ†æã‚µãƒ¼ãƒ“ã‚¹"""
    
    def __init__(self):
        super().__init__("reddit_explorer")
        self.reddit_client = self._initialize_reddit()
        self.subreddits = self._load_subreddits()
    
    def _initialize_reddit(self) -> praw.Reddit:
        """Redditã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã‚’åˆæœŸåŒ–"""
        try:
            client_id = self.config.REDDIT_CLIENT_ID
            client_secret = self.config.REDDIT_CLIENT_SECRET
            
            if not client_id or not client_secret:
                raise ConfigurationException(
                    "Reddit credentials not found in environment"
                )
            
            return praw.Reddit(
                client_id=client_id.get_secret_value(),
                client_secret=client_secret.get_secret_value(),
                user_agent=self.config.REDDIT_USER_AGENT
            )
            
        except Exception as e:
            raise ConfigurationException(
                f"Failed to initialize Reddit client: {e}"
            ) from e
    
    def _load_subreddits(self) -> Dict[str, List[str]]:
        """ã‚µãƒ–ãƒ¬ãƒ‡ã‚£ãƒƒãƒˆè¨­å®šã‚’èª­ã¿è¾¼ã¿"""
        config_path = self.get_config_path("subreddits.toml")
        
        try:
            with open(config_path, "r", encoding="utf-8") as f:
                config = toml.load(f)
                self.logger.info(
                    f"Loaded {sum(len(v) for v in config.values())} subreddits"
                )
                return config
        except Exception as e:
            raise DataException(f"Failed to load subreddits: {e}") from e
    
    @log_execution_time
    @handle_errors(retries=3)
    async def collect(self) -> None:
        """RedditæŠ•ç¨¿ã‚’åé›†ãƒ»åˆ†æ"""
        self.logger.info("Starting Reddit collection")
        
        # ã‚«ãƒ†ã‚´ãƒªã”ã¨ã«ä¸¦è¡Œå‡¦ç†
        tasks = []
        for category, subreddit_list in self.subreddits.items():
            for subreddit in subreddit_list:
                tasks.append(self._collect_subreddit(subreddit, category))
        
        results = await gather_with_errors(
            *tasks,
            task_names=[f"{cat}/{sub}" for cat, subs in self.subreddits.items() for sub in subs]
        )
        
        # çµæœã‚’çµ±åˆ
        all_posts = []
        failed_subreddits = []
        
        for result in results:
            if result.success and result.result:
                all_posts.extend(result.result)
            else:
                failed_subreddits.append(result.name)
                self.logger.error(f"Failed to collect {result.name}: {result.error}")
        
        if not all_posts:
            raise ServiceException("No posts collected from Reddit")
        
        # æŠ•ç¨¿ã‚’åˆ†æ
        analyzed_posts = await self._analyze_posts(all_posts)
        
        # ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ
        report = await self._generate_report(analyzed_posts, failed_subreddits)
        filename = f"reddit_{datetime.now():%Y-%m-%d}.md"
        await self.save_markdown(report, filename)
        
        # çµ±è¨ˆæƒ…å ±ã‚’ä¿å­˜
        stats = self._generate_statistics(analyzed_posts)
        await self.save_json(stats, f"stats_{datetime.now():%Y-%m-%d}.json")
        
        self.logger.info(
            f"Collected {len(analyzed_posts)} posts from "
            f"{len(set(p['subreddit'] for p in analyzed_posts))} subreddits"
        )
    
    async def _collect_subreddit(
        self,
        subreddit_name: str,
        category: str
    ) -> List[Dict[str, Any]]:
        """ç‰¹å®šã®ã‚µãƒ–ãƒ¬ãƒ‡ã‚£ãƒƒãƒˆã‹ã‚‰æŠ•ç¨¿ã‚’åé›†"""
        try:
            # Reddit APIã¯åŒæœŸçš„ãªã®ã§ã€åˆ¥ã‚¹ãƒ¬ãƒƒãƒ‰ã§å®Ÿè¡Œ
            posts = await run_sync_in_thread(
                self._fetch_subreddit_posts,
                subreddit_name,
                limit=20
            )
            
            # ã‚«ãƒ†ã‚´ãƒªæƒ…å ±ã‚’è¿½åŠ 
            for post in posts:
                post["category"] = category
            
            self.logger.debug(
                f"Collected {len(posts)} posts from r/{subreddit_name}"
            )
            
            return posts
            
        except ResponseException as e:
            if e.response.status_code == 404:
                raise APIException(
                    f"Subreddit r/{subreddit_name} not found",
                    status_code=404
                ) from e
            else:
                raise APIException(
                    f"Reddit API error for r/{subreddit_name}: {e}",
                    status_code=e.response.status_code
                ) from e
        except Exception as e:
            raise ServiceException(
                f"Failed to collect from r/{subreddit_name}: {e}"
            ) from e
    
    def _fetch_subreddit_posts(self, subreddit_name: str, limit: int) -> List[Dict[str, Any]]:
        """ã‚µãƒ–ãƒ¬ãƒ‡ã‚£ãƒƒãƒˆã®æŠ•ç¨¿ã‚’å–å¾—ï¼ˆåŒæœŸï¼‰"""
        subreddit = self.reddit_client.subreddit(subreddit_name)
        posts = []
        
        for submission in subreddit.hot(limit=limit):
            post_data = {
                "id": submission.id,
                "title": submission.title,
                "author": str(submission.author) if submission.author else "[deleted]",
                "subreddit": subreddit_name,
                "score": submission.score,
                "upvote_ratio": submission.upvote_ratio,
                "num_comments": submission.num_comments,
                "created_utc": submission.created_utc,
                "url": f"https://reddit.com{submission.permalink}",
                "is_self": submission.is_self,
                "selftext": submission.selftext[:1000] if submission.is_self else "",
                "link_url": submission.url if not submission.is_self else None,
                "flair": submission.link_flair_text
            }
            posts.append(post_data)
        
        return posts
    
    async def _analyze_posts(
        self,
        posts: List[Dict[str, Any]]
    ) -> List[Dict[str, Any]]:
        """æŠ•ç¨¿ã‚’åˆ†æ"""
        # åˆ†æãŒå¿…è¦ãªæŠ•ç¨¿ã‚’ãƒ•ã‚£ãƒ«ã‚¿
        posts_to_analyze = [
            p for p in posts
            if p["score"] > 100 or p["num_comments"] > 50
        ]
        
        self.logger.info(f"Analyzing {len(posts_to_analyze)} high-quality posts")
        
        async def analyze_single_post(post: Dict[str, Any]) -> Dict[str, Any]:
            try:
                # ã‚¿ã‚¤ãƒˆãƒ«ã¨å†…å®¹ã‹ã‚‰è¦ç´„ã‚’ç”Ÿæˆ
                content = f"Title: {post['title']}\n"
                if post.get("selftext"):
                    content += f"Content: {post['selftext']}"
                
                analysis = await self._generate_post_analysis(content, post["subreddit"])
                post["ai_analysis"] = analysis
                
                # ã‚»ãƒ³ãƒãƒ¡ãƒ³ãƒˆåˆ†æ
                post["sentiment"] = await self._analyze_sentiment(content)
                
                # é‡è¦åº¦ã‚¹ã‚³ã‚¢
                post["importance_score"] = self._calculate_importance(post)
                
                return post
                
            except Exception as e:
                self.logger.warning(f"Failed to analyze post {post['id']}: {e}")
                post["ai_analysis"] = ""
                return post
        
        # ä¸¦è¡Œåˆ†æï¼ˆãƒ¬ãƒ¼ãƒˆåˆ¶é™ã‚’è€ƒæ…®ï¼‰
        tasks = [analyze_single_post(post) for post in posts_to_analyze]
        analyzed = await run_with_semaphore(tasks, max_concurrent=3)
        
        # åˆ†æã•ã‚Œãªã‹ã£ãŸæŠ•ç¨¿ã‚‚å«ã‚ã‚‹
        analyzed_ids = {p["id"] for p in analyzed}
        for post in posts:
            if post["id"] not in analyzed_ids:
                post["ai_analysis"] = ""
                post["importance_score"] = self._calculate_importance(post)
        
        return posts
    
    async def _generate_post_analysis(self, content: str, subreddit: str) -> str:
        """æŠ•ç¨¿ã®åˆ†æã‚’ç”Ÿæˆ"""
        prompt = f"""
        ä»¥ä¸‹ã®RedditæŠ•ç¨¿ï¼ˆr/{subreddit}ï¼‰ã‚’åˆ†æã—ã€
        æ—¥æœ¬èªã§è¦ç‚¹ã‚’50æ–‡å­—ä»¥å†…ã§ã¾ã¨ã‚ã¦ãã ã•ã„ï¼š
        
        {content}
        
        æŠ€è¡“çš„ãªå†…å®¹ã®å ´åˆã¯ã€ãã®æŠ€è¡“çš„ä¾¡å€¤ã‚‚å«ã‚ã¦ãã ã•ã„ã€‚
        """
        
        return await self.gpt_client.generate_async(
            prompt,
            max_tokens=150,
            temperature=0.3
        )
    
    async def _analyze_sentiment(self, content: str) -> str:
        """ã‚»ãƒ³ãƒãƒ¡ãƒ³ãƒˆåˆ†æ"""
        prompt = f"""
        ä»¥ä¸‹ã®ãƒ†ã‚­ã‚¹ãƒˆã®ã‚»ãƒ³ãƒãƒ¡ãƒ³ãƒˆã‚’åˆ†æã—ã€
        ã€Œãƒã‚¸ãƒ†ã‚£ãƒ–ã€ã€Œãƒã‚¬ãƒ†ã‚£ãƒ–ã€ã€Œä¸­ç«‹ã€ã®ã„ãšã‚Œã‹ã§ç­”ãˆã¦ãã ã•ã„ï¼š
        
        {content[:500]}
        """
        
        response = await self.gpt_client.generate_async(
            prompt,
            max_tokens=10,
            temperature=0
        )
        
        return response.strip()
    
    def _calculate_importance(self, post: Dict[str, Any]) -> float:
        """æŠ•ç¨¿ã®é‡è¦åº¦ã‚’è¨ˆç®—"""
        score = 0.0
        
        # ã‚¹ã‚³ã‚¢ã®é‡ã¿
        score += min(post["score"] / 100, 30)
        
        # ã‚³ãƒ¡ãƒ³ãƒˆæ•°ã®é‡ã¿
        score += min(post["num_comments"] / 50, 20)
        
        # ã‚¢ãƒƒãƒ—voteç‡
        score += post.get("upvote_ratio", 0.5) * 20
        
        # æ–°ã—ã•ï¼ˆ24æ™‚é–“ä»¥å†…ãªã‚‰åŠ ç‚¹ï¼‰
        age_hours = (datetime.now().timestamp() - post["created_utc"]) / 3600
        if age_hours < 24:
            score += (24 - age_hours) / 24 * 20
        
        # ãƒ•ãƒ¬ã‚¢ãŒã‚ã‚‹å ´åˆ
        if post.get("flair"):
            score += 10
        
        return min(score, 100.0)
    
    async def _generate_report(
        self,
        posts: List[Dict[str, Any]],
        failed_subreddits: List[str]
    ) -> str:
        """Markdownãƒ¬ãƒãƒ¼ãƒˆã‚’ç”Ÿæˆ"""
        lines = [
            f"# Reddit ãƒˆãƒ¬ãƒ³ãƒ‰åˆ†æ - {datetime.now():%Yå¹´%mæœˆ%dæ—¥}",
            "",
            "AIãŒé¸ã‚“ã ä»Šæ—¥ã®æ³¨ç›®æŠ•ç¨¿",
            ""
        ]
        
        if failed_subreddits:
            lines.extend([
                "## âš ï¸ åé›†ã«å¤±æ•—ã—ãŸã‚µãƒ–ãƒ¬ãƒ‡ã‚£ãƒƒãƒˆ",
                ", ".join(failed_subreddits),
                ""
            ])
        
        # ã‚«ãƒ†ã‚´ãƒªåˆ¥ã«æ•´ç†
        by_category = {}
        for post in posts:
            cat = post.get("category", "Other")
            if cat not in by_category:
                by_category[cat] = []
            by_category[cat].append(post)
        
        for category, cat_posts in sorted(by_category.items()):
            lines.extend([
                f"## {category}",
                ""
            ])
            
            # é‡è¦åº¦ã§ã‚½ãƒ¼ãƒˆ
            sorted_posts = sorted(
                cat_posts,
                key=lambda x: x.get("importance_score", 0),
                reverse=True
            )[:10]  # å„ã‚«ãƒ†ã‚´ãƒªä¸Šä½10ä»¶
            
            for post in sorted_posts:
                sentiment_emoji = {
                    "ãƒã‚¸ãƒ†ã‚£ãƒ–": "ğŸ˜Š",
                    "ãƒã‚¬ãƒ†ã‚£ãƒ–": "ğŸ˜",
                    "ä¸­ç«‹": "ğŸ˜"
                }.get(post.get("sentiment", ""), "")
                
                lines.extend([
                    f"### [{post['title']}]({post['url']})",
                    f"r/{post['subreddit']} | "
                    f"ğŸ‘¤ u/{post['author']} | "
                    f"â¬†ï¸ {post['score']:,} | "
                    f"ğŸ’¬ {post['num_comments']:,} | "
                    f"{sentiment_emoji}",
                    "",
                ])
                
                if post.get("ai_analysis"):
                    lines.extend([
                        "**AIåˆ†æ**:",
                        post["ai_analysis"],
                        ""
                    ])
                
                if post.get("link_url") and not post["is_self"]:
                    lines.extend([
                        f"ğŸ”— [å¤–éƒ¨ãƒªãƒ³ã‚¯]({post['link_url']})",
                        ""
                    ])
                
                lines.append("---")
                lines.append("")
        
        return "\n".join(lines)
    
    def _generate_statistics(self, posts: List[Dict[str, Any]]) -> Dict[str, Any]:
        """çµ±è¨ˆæƒ…å ±ã‚’ç”Ÿæˆ"""
        total_score = sum(p["score"] for p in posts)
        total_comments = sum(p["num_comments"] for p in posts)
        
        sentiment_counts = {}
        for post in posts:
            sentiment = post.get("sentiment", "ä¸æ˜")
            sentiment_counts[sentiment] = sentiment_counts.get(sentiment, 0) + 1
        
        return {
            "date": datetime.now().isoformat(),
            "total_posts": len(posts),
            "subreddits": list({p["subreddit"] for p in posts}),
            "categories": list({p.get("category", "Other") for p in posts}),
            "total_score": total_score,
            "total_comments": total_comments,
            "avg_score": total_score / len(posts) if posts else 0,
            "avg_comments": total_comments / len(posts) if posts else 0,
            "sentiment_distribution": sentiment_counts,
            "top_posts": [
                {
                    "title": p["title"],
                    "url": p["url"],
                    "score": p["score"],
                    "subreddit": p["subreddit"],
                    "importance": p.get("importance_score", 0)
                }
                for p in sorted(
                    posts,
                    key=lambda x: x.get("importance_score", 0),
                    reverse=True
                )[:20]
            ]
        }
```

### 3. å…±é€šãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£ã®è¿½åŠ 
**ãƒ•ã‚¡ã‚¤ãƒ«**: `nook/common/base_service.py` (è¿½åŠ ãƒ¡ã‚½ãƒƒãƒ‰)

```python
# BaseServiceã‚¯ãƒ©ã‚¹ã«ä»¥ä¸‹ã®ãƒ¡ã‚½ãƒƒãƒ‰ã‚’è¿½åŠ 

    def get_config_path(self, filename: str) -> Path:
        """ã‚µãƒ¼ãƒ“ã‚¹å›ºæœ‰ã®è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ã‚’å–å¾—"""
        return Path(f"nook/services/{self.service_name}/{filename}")
    
    async def save_json(self, data: Any, filename: str) -> None:
        """JSONãƒ‡ãƒ¼ã‚¿ã‚’ä¿å­˜"""
        import json
        json_str = json.dumps(data, ensure_ascii=False, indent=2)
        await self.save_data(json_str, filename)
    
    async def load_json(self, filename: str) -> Any:
        """JSONãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿"""
        import json
        content = await self.storage.load(filename)
        return json.loads(content) if content else None
    
    async def save_with_backup(self, data: Any, filename: str, keep_backups: int = 3):
        """ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ä»˜ãã§ãƒ‡ãƒ¼ã‚¿ã‚’ä¿å­˜"""
        # æ—¢å­˜ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—
        existing = await self.storage.exists(filename)
        if existing:
            for i in range(keep_backups - 1, 0, -1):
                old_backup = f"{filename}.{i}"
                new_backup = f"{filename}.{i + 1}"
                if await self.storage.exists(old_backup):
                    await self.storage.rename(old_backup, new_backup)
            
            await self.storage.rename(filename, f"{filename}.1")
        
        # æ–°ã—ã„ãƒ‡ãƒ¼ã‚¿ã‚’ä¿å­˜
        await self.save_data(data, filename)
```

### 4. æ®‹ã‚Šã®ã‚µãƒ¼ãƒ“ã‚¹ã®ãƒªãƒ•ã‚¡ã‚¯ã‚¿ãƒªãƒ³ã‚°æŒ‡é‡

ä»¥ä¸‹ã®ã‚µãƒ¼ãƒ“ã‚¹ã‚‚åŒæ§˜ã«ãƒªãƒ•ã‚¡ã‚¯ã‚¿ãƒªãƒ³ã‚°ã™ã‚‹ï¼š

1. **HackerNewsService**
   - éåŒæœŸHTTPé€šä¿¡ã¸ã®ç§»è¡Œ
   - ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°ã®çµ±ä¸€
   - AIã«ã‚ˆã‚‹ã‚³ãƒ¡ãƒ³ãƒˆåˆ†æã®è¿½åŠ 

2. **TechFeedService / BusinessFeedService**
   - RSSè§£æã®éåŒæœŸåŒ–
   - è¤‡æ•°ãƒ•ã‚£ãƒ¼ãƒ‰ã®ä¸¦è¡Œå–å¾—
   - è¨˜äº‹ã®é‡è¦åº¦ã‚¹ã‚³ã‚¢ãƒªãƒ³ã‚°

3. **PaperSummarizerService**
   - arXiv APIã®éåŒæœŸåŒ–
   - è«–æ–‡ã®ä¸¦è¡Œãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
   - è¦ç´„ç”Ÿæˆã®æœ€é©åŒ–

4. **FourchanExplorer / FivechanExplorer**
   - ã‚¹ãƒ¬ãƒƒãƒ‰åé›†ã®ä¸¦è¡ŒåŒ–
   - ãƒ¬ãƒ¼ãƒˆåˆ¶é™ã®å®Ÿè£…
   - ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°

å„ã‚µãƒ¼ãƒ“ã‚¹ã§å®Ÿè£…ã™ã¹ãå…±é€šè¦ç´ ï¼š
- `BaseService`ã®ç¶™æ‰¿
- éåŒæœŸãƒ¡ã‚½ãƒƒãƒ‰ï¼ˆ`async def`ï¼‰ã®ä½¿ç”¨
- çµ±ä¸€ã•ã‚ŒãŸã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°
- æ§‹é€ åŒ–ãƒ­ã‚°ã®å‡ºåŠ›
- è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã®å¤–éƒ¨åŒ–
- AIã«ã‚ˆã‚‹åˆ†æãƒ»è¦ç´„æ©Ÿèƒ½
- çµ±è¨ˆæƒ…å ±ã®ç”Ÿæˆ

## ãƒ†ã‚¹ãƒˆè¦ä»¶

å„ãƒªãƒ•ã‚¡ã‚¯ã‚¿ãƒªãƒ³ã‚°ã•ã‚ŒãŸã‚µãƒ¼ãƒ“ã‚¹ã«å¯¾ã—ã¦ï¼š

1. **ãƒ¦ãƒ‹ãƒƒãƒˆãƒ†ã‚¹ãƒˆ**ã‚’ä½œæˆ
2. **ãƒ¢ãƒƒã‚¯ã‚’ä½¿ç”¨**ã—ã¦å¤–éƒ¨ä¾å­˜ã‚’æ’é™¤
3. **éåŒæœŸå‡¦ç†**ã®ãƒ†ã‚¹ãƒˆ
4. **ã‚¨ãƒ©ãƒ¼ã‚±ãƒ¼ã‚¹**ã®ãƒ†ã‚¹ãƒˆ
5. **ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹**ã®è¨ˆæ¸¬

## å®Œäº†æ¡ä»¶

1. ã™ã¹ã¦ã®ã‚µãƒ¼ãƒ“ã‚¹ãŒ`BaseService`ã‚’ç¶™æ‰¿ã—ã¦ã„ã‚‹ã“ã¨
2. éåŒæœŸå‡¦ç†ã«ç§»è¡Œã—ã¦ã„ã‚‹ã“ã¨
3. ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°ãŒçµ±ä¸€ã•ã‚Œã¦ã„ã‚‹ã“ã¨
4. ãƒ­ã‚°å‡ºåŠ›ãŒæ§‹é€ åŒ–ã•ã‚Œã¦ã„ã‚‹ã“ã¨
5. ãƒ†ã‚¹ãƒˆãŒä½œæˆã•ã‚Œã€ãƒ‘ã‚¹ã—ã¦ã„ã‚‹ã“ã¨
6. ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆãŒæ›´æ–°ã•ã‚Œã¦ã„ã‚‹ã“ã¨

## æ³¨æ„äº‹é …

1. æ—¢å­˜ã®æ©Ÿèƒ½ã‚’å£Šã•ãªã„ã‚ˆã†æ®µéšçš„ã«ç§»è¡Œ
2. å¤–éƒ¨APIã®ãƒ¬ãƒ¼ãƒˆåˆ¶é™ã‚’è€ƒæ…®
3. ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã®å¢—åŠ ã«æ³¨æ„
4. ä¸¦è¡Œå‡¦ç†æ•°ã‚’é©åˆ‡ã«åˆ¶é™
5. ã‚¨ãƒ©ãƒ¼æ™‚ã®ãƒªãƒˆãƒ©ã‚¤æˆ¦ç•¥ã‚’å®Ÿè£…

## ä¾å­˜é–¢ä¿‚

- TASK-001ï¼ˆåŸºåº•ã‚¯ãƒ©ã‚¹ï¼‰ã®å®Œäº†
- TASK-002ï¼ˆã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°ï¼‰ã®å®Œäº†
- TASK-003ï¼ˆéåŒæœŸå‡¦ç†ï¼‰ã®å®Œäº†

## æœŸé™

3æ—¥é–“